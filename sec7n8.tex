
\definecolor{vidred}{HTML}{f3a9a6}
\definecolor{temptasks}{HTML}{EA6B66}
\definecolor{vislang}{HTML}{ffd3aa}
\definecolor{mm}{HTML}{FFB570}
\definecolor{eap}{HTML}{d1a8c2}
\definecolor{vfp}{HTML}{B5739D}
\definecolor{states}{HTML}{c0d3f0}
\definecolor{vad}{HTML}{7EA6E0}
\definecolor{aa}{HTML}{aed2cc}
\definecolor{gen}{HTML}{67AB9F}


\begin{figure*}[t]
    \centering
    \begin{minipage}[c]{\linewidth}
    \includegraphics[width=\linewidth]{figs/works_per_year.png}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c c c c c c c c c c}
         \textcolor{vidred}{VRe} : 27 &
         \textcolor{temptasks}{TS} : 181 &
         \textcolor{vislang}{V\&L} : 275 &
         \textcolor{mm}{MM} : 146 &
         \textcolor{eap}{EAP} : 23 &
         \textcolor{vfp}{VFP} : 18 &
         \textcolor{states}{ST} : 28 &
         \textcolor{vad}{VAD} : 138 &
         \textcolor{aa}{AA} : 36 &
         \textcolor{gen}{Gen} : 213
         \\
    \end{tabular}
    }
    \caption[Caption]{\textbf{Number of action understanding papers per year}. The research focus (bottom to top) includes video reduction approaches \textcolor{vidred}{VRe}, temporal tasks \textcolor{temptasks}{TS}, vision and language methods \textcolor{vislang}{V\&L}, multimodal models \textcolor{mm}{MM}, early action prediction \textcolor{eap}{EAP}, video frame prediction \textcolor{vfp}{VFP}, state-based tasks \textcolor{states}{ST}, video anomaly detection \textcolor{vad}{VAD}, action anticipation \textcolor{aa}{AA}, and video generation \textcolor{gen}{Gen}. The number of relevant papers is approximated from all works citing influential papers with $\geq 300$ citations per group\textcolor{red}{$^*$}. Small disparities are expected as recent works may not be included. The increasing research activity in action understanding is evident.}
    % legend: 
    %[color 1.1] Video reduction methods, 
    %[color 1.2] Temporal tasks, 
    %[color 1.3] Video-language, 
    %[color 1.4] Multimodal, 
    %[color 2.1] EAP, 
    %[color 2.2] VFP, 
    %[color 2.3] State changes, 
    %[color 2.4] Anomaly detection
    %[color 3.1] AA
    %[color 3.2] Gnerative models
    \label{fig:works_over_year}
    \end{minipage}
\end{figure*}


\section{Research directions to explore}
\label{sec:directions}

Progress in video understanding is fast-paced. As shown in \Cref{fig:works_over_year}, vision and language, video generation, and anomaly detection tasks have experienced significant interest in recent years. In tandem, well-established problems such as temporal tasks have remained relevant. We provide a look into the future and explore three main directions of progress beyond the continuation of current trends. We envision ways for future models to reason about abstractions in \Cref{sec:directions::reason}. We then consider the tasks and objectives that future action understanding models will address in \Cref{sec:directions::tasks}. Finally, we discuss efficiency improvements for training and deployment in \Cref{sec:directions::efficiency}.




\subsection{Reasoning semantics}
\label{sec:directions::reason}

With the shift from visual to semantic pattern extraction, the notion of abstraction levels will become more central. We explore future directions for interpreting actions, considering intentions and goals, and adapting to unseen scenarios. 

\subsubsection{From action to understanding}
\label{sec:directions::reason:::action2understanding}

Increasingly, action understanding is concerned not only with what is visually depicted but with reasoning about how the depiction is just one out of a multitude of possible perspectives. While action recognition tasks have driven a significant amount of progress on visual representations, future tasks will require more semantic interpretation. When moving from isolated clips of actions to longer episodes depicting behaviors, modeling long-range temporal dependencies becomes more important. Understanding behavior over time requires more than just aggregated interpretations of brief clips. Simultaneously, the distinction between visual observation and interpretation will become weaker, achieving a less deterministic view of action understanding with potentially multiple possible interpretations. Consequently, the automated analysis of videos will shift from objectively measuring or labeling, to a more subjective, context-dependent interpretation. In turn, this will require novel ways of training and evaluation, for example by including humans \pcite{kaufmann2023survey}.

One perspective on context is to include the intentions of those depicted. Despite VLMs' great progress in learning procedural steps in tasks through natural language-guided embeddings \pcite{li2025llama,li2024mini,wu2024longvideobench}, their reliance on visual information remains partial. \tcite{al2024unibench} showed that scaling models and data sizes do not offer substantial reasoning performance gains for vision tasks despite strong performance in skill-based tasks. Thus, a new avenue for future approaches is the design of open-world models from multi-level semantics that reflect human intentions and goals. Capturing information about the visual world in a scene may not necessarily require to be associated with language embeddings. Instead, relations could correspond to different pairs or groups of modalities adaptively. Longitudinal data specific to individuals may also be used to tailor the model's understanding of the world based on the user's goals, objectives, intentions, and interactions. Such treatment gives rise to a person- or case-specific perspective, bringing general action understanding to the consumer.

\blfootnote{\noindent \textcolor{red}{$^*$} From Google Scholar as of the 28th of October 2024.}


\subsubsection{Novel problem adaptation}
\label{sec:directions::reason:::novel}

Zero-shot performance has improved significantly over the past years. This is especially evident in language- and semantic-based video tasks aided by LLMs' large capacity and context. However, limitations remain in tasks orthogonal to pretext SSL \pcite{liu2024mmbench}. Recent approaches such as modality and probabilistic adapters \pcite{chen2024efficient,lin2023vision,sung2022vl,upadhyay2023probvlm,lu2024improving}, information gating \pcite{zhang2024llama}, visual prompt learning \pcite{khattak2023maple}, knowledge distillation \pcite{mistretta2024improving}, and model caching \pcite{zhang2021tip} have improved zero-shot downstream task performance by adjusting pre-trained models. However, only a few structural elements or objectives in models explicitly improve zero-shot performance for unseen distinct tasks. Unified models that can be used as mixture of experts controllers \pcite{bao2022vlmo,lin2024moe,wang2022image,yu2024boosting} are promising to bridge this gap. Sparsely trained models in mixture of experts settings allow for faster inference times with only task-relative sub-models using conditional computations \pcite{bengio2013estimating,jacobs1991adaptive}. When such mixtures can be linked to different levels of abstraction, re-use of models across levels also becomes feasible. The integration of experts can be done regardless of the backbone architecture.



\subsection{Better task definitions}
\label{sec:directions::tasks}

Training models does not necessarily guarantee that temporal dynamics of videos are learned at a foundational level. Future research may revisit and integrate beneficial properties for representation learning. Additionally, future works can explore performance measurements beyond simple metrics, instead focusing on explanation by observing embedding distributions and feature correspondences. 


\subsubsection{Objectives}
\label{sec:directions::tasks:::objectives}

Distributions learned from representation-based objectives can be effectively used as priors to downstream tasks \pcite{janocha2017loss,larochelle2009exploring}. Drawing inspiration from \pcite{bengio2013representation}, a number of widely-accepted target properties are discussed below.

\noindent
\textbf{Temporal and spatial coherence}. Temporally or spatially proximal instances should correspond to similar representations. This notion can be extended to maintain proportional distances across both the pixel and embedding spaces. This coherence prior has been explored in objectives relating to time such as cyclic consistency \pcite{dwibedi2018temporal,donahue2024learning,haresh2021learning}, video procedural learning \pcite{chen2022frame,sermanet2018time}, DTW \pcite{dvornik2021drop,hadji2021representation}, and cross-frame stochasticity \pcite{zhang2023modeling}. These priors can be explored in a more general context as pre-training tasks similar to SSL while being tailored to the nature of videos. 

\noindent
\textbf{Abstractions and hierarchies}. Beyond fine-grained categories and semantics, most current works do not explicitly learn levels of abstraction. They are primarily limited to implicit connections between specific types \pcite{li2024deal} that often lead to spurious correlations \pcite{chen2020counterfactual,kim2023exposing,tian2024argue} as well as task- and instance-based misalignment \pcite{zhang2024rethinking}. Objectives that enforce abstraction hierarchies can potentially mitigate such misalignments. Promising efforts include partial order relations \pcite{alper2024emergent}, prototype learning \pcite{ramesh2022hierarchical}, hyperbolic representations \pcite{mettes2024hyperbolic}, and scene graphs \pcite{li2024scene}. As models become more polysemantic, the use of natural hierarchies and abstractions is expected to become more prevalent.

\noindent
\textbf{Natural clustering and manifolds}. Local representations tend to preserve similar polysemantic characteristics. Several works have shown that real data are not represented within the totality of the feature space but instead form dense concentrations in specific regions \pcite{genovese2012minimax,jiang2018trust,liang2022mind}. Using the tangent space of these distributions as a prior has shown promise in vision tasks such as generation \pcite{he2023manifold}, model explanation \pcite{bordt2023manifold}, anomaly detection \pcite{shin2023anomaly}, and corruption robustness \pcite{chen2022vita}. However, using the tangent space from real data distributions as an objective-steering prior remains largely an open question for large-scale multimodal action understanding models. 


\subsubsection{Limitations in performance beyond metrics} \label{sec:directions:tasks:metrics}

Much of the progress in the domain of action understanding originates from comparing model outputs on benchmark data. While reported performance provides insights into the relative merits of models, it does not provide a good understanding of typical failure modes. Recent image-based \pcite{kowal2024understanding,kowal2024visual,park2023self,walmer2023teaching} and video-based \pcite{kowal2024understanding,stergiou2023leaping} visualization approaches provide human-interpretable insights into predictions at the instance level. However, understanding how semantic interpretations of actions are addressed, remains largely unexplored. This limits understanding the generalization ability to novel domains and tasks. Uptake of recent explainable AI trends \pcite{minh2022explainable} into computer vision model development can prompt the development of better measures for the capabilities and limitations of novel models.

Beyond benchmarking models on tasks and metrically evaluating performance, understanding the distributions and learned correlations provides new research opportunities. In-Context Learning (ICL) \pcite{brown2020language,hoffmann2022empirical} and Chain-of Thought (CoT) \pcite{wei2022chain} prompting are promising directions for LLMs and VLMs. \tcite{bansal2023rethinking} has shown that LLMs' capabilities are influenced by just a small number of attention or feed-forward layers, which are highly task-specific. Both \tcite{baldassini2024makes,chen2024understanding} showed that ICL in VLMs primarily relies on text information. Disparities between target and learned features can occur due to shortcuts learned by models. Common factors that can lead to shortcuts include contrastive loss' multiple local minima \pcite{robinson2021can}, suppression of visual information by language \pcite{li2023addressing}, and low mutual information between latent representations and real data \pcite{adnan2022monitoring}. Recently, \tcite{bleeker2024demonstrating} showed that introducing unique information distal to the overall training distribution favors VLMs' reliance on shortcuts for models trained on contrastive objectives. Such insights provide opportunities for exploring objectives and models with better multimodal and data-varying generalization capabilities.


\subsection{Efficiency}
\label{sec:directions::efficiency}

Model efficiency is essential for real-time application. Given the rapid deployment of models in a multitude of applications, we also highlight privacy risks alongside opportunities for domain specialization.  

\subsubsection{From research to deployment}
\label{sec:directions::efficiency:::deployment}


The increased variety of action understanding tasks also comes with the potential of improving actual deployment. Current and future models achieve performance and robustness levels that allow them to automate processes such as video data curation and surveillance. Novel applications based on behavior analysis can also benefit from these advances. Moving from benchmarks to the real world requires attention to computational efficiency. While the accuracy of current models is remarkable, performance comes at a cost. The trend of increasing model sizes, partly because of the focus on foundation models, largely prohibits the use of these models in computationally constrained operational settings. Attempts to reduce the computational complexity of trained models through pruning \pcite{iofinova2023bias}, knowledge distillation \pcite{mistretta2024improving}, or domain-specific adapters \pcite{hu2021lora} are not without limitations. The generalization performance gap between the currently best-performing models and those that can run on consumer hardware is significant. Several recent works \pcite{dao2022flashattention,gu2023mamba,poli2023hyena} propose novel processing paradigms that have the potential to scale better. Future work should address whether advances in multimodal training can transfer across both settings and models.


\subsubsection{Generalizable priors}
\label{sec:directions::efficiency:::priors}

Modern models are primarily trained on large-scale uncurated datasets aimed at multi-domain generalization. However, training distributions can include noise or be insufficiently rich for domain-specific datasets. Recent approaches have aimed to reduce training data requirements by including distribution priors at training. \tcite{kahana2022improving} aimed to improve zero-shot performance with a joint objective that matches label distributions while minimizing the divergence to original zero-shot predictions. \tcite{gao2022pyramidclip} utilized multiple levels of abstraction to contrast language and visual semantics and improve training efficiency. \tcite{nag2024safari} used a weakly-supervised approach to refine pseudo-object masks with cross-modal alignment in low-annotation settings. Approaches specifically utilizing priors in videos include concept distillation from normalized language embeddings \pcite{ranasinghe2023language}, and motion-specific alignment between video and textual descriptions of movements \pcite{zhang2024enhanced}. Distilling learned information to then be used as an optimization prior is a promising route for efficient training by reducing resource requirements. It can also impose a constraint based on the nature of expected motions with potential benefits in model convergence.


\subsubsection{Privacy and specialization}
\label{sec:directions::efficiency:::privacy}


Vision-based models are susceptible to attacks that either invert their gradients to reconstruct inputs \pcite{hatamizadeh2022gradvit} or discover intermediate representations \pcite{fang2023gifd}. Such attacks can compromise potentially proprietary training data, and reveal identifiable information. Inputs and features from VLMs can also be inferred through learnable vision-language triggers \pcite{bai2024badclip}, inference-time adversarial perturbations in frames \pcite{li2024fmm}, backdoor attacks through adversarial patches in training \pcite{carlini2022poisoning}, and injecting malicious prompts during instruction tuning \pcite{liang2024vl}. \tcite{kariyappa2023cocktail} showed that semantics from the original data can still be recovered even in distributed settings over large batches. Such vulnerabilities can be exploited across downstream tasks. Thus, evaluating model robustness is an important topic that the community should attend to.

The importance of privacy can also be understood through the current shift toward domain-expert sub-models integrated into general-purpose frameworks. \tcite{shen2024tag} showed that LLM specialization on domain-specific tasks significantly improves zero-shot generalization in related domains. Visual instructional tuning \pcite{bai2024generalist} and evolutionary instruction-based prompting \pcite{luo2024mmevol} have also shown promising results for vision-language models. Enhancing video-based models with domain specialization requires further exploration, for example through singular general models of high capacity, or multiple models in holistic frameworks.


\section{Conclusion}
\label{sec:conclusion}

Video action understanding includes a diverse set of tasks. These previously isolated tasks are increasingly overlapping in terms of the deployed models, utilized training data, and used evaluation protocols. To this end, we provide a comprehensive review of the broad domain of video action understanding. We discussed the main challenges, relevant datasets, and seminal works with an emphasis on recent (multimodal) advancements across tasks, and future research directions. We explicitly included multimodal advances. We focused on three temporal scopes from which tasks and approaches understand actions performed. We discussed recognition tasks that use complete observations of actions to infer fine- or coarse-grained labels. We then overviewed predictive tasks from partial observations of actions. Finally, we outlined forecasting tasks with anticipation models that infer general scene knowledge and forecast future actions not yet performed. Using time as a stepping stone, we outline current limitations and promising research directions to further advance the scope, robustness, and deployment of action understanding research.


\noindent
\textbf{Data availability}
We do not use or generate datasets. Dataset statistics used for comparisons are sourced from the respective papers referenced below. 