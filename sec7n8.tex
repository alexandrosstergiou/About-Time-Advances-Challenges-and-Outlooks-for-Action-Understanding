
\definecolor{vidred}{HTML}{f3a9a6}
\definecolor{temptasks}{HTML}{EA6B66}
\definecolor{vislang}{HTML}{ffd3aa}
\definecolor{mm}{HTML}{FFB570}
\definecolor{eap}{HTML}{d1a8c2}
\definecolor{vfp}{HTML}{B5739D}
\definecolor{states}{HTML}{c0d3f0}
\definecolor{vad}{HTML}{7EA6E0}
\definecolor{aa}{HTML}{aed2cc}
\definecolor{gen}{HTML}{67AB9F}


\begin{figure*}[t]
    \centering
    \begin{minipage}[c]{\linewidth}
    \includegraphics[width=\linewidth]{figs/works_per_year.png}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c c c c c c c c c c}
         \textcolor{vidred}{VRe} : 27 &
         \textcolor{temptasks}{TS} : 181 &
         \textcolor{vislang}{V\&L} : 275 &
         \textcolor{mm}{MM} : 146 &
         \textcolor{eap}{EAP} : 23 &
         \textcolor{vfp}{VFP} : 18 &
         \textcolor{states}{ST} : 28 &
         \textcolor{vad}{VAD} : 138 &
         \textcolor{aa}{AA} : 36 &
         \textcolor{gen}{Gen} : 213
         \\
    \end{tabular}
    }
    \caption[Caption]{\textbf{Number of action understanding works per year}. Research focus (bottom to top) of works include video reduction approaches \textcolor{vidred}{VRe}, temporal tasks \textcolor{temptasks}{TS}, vision and language methods \textcolor{vislang}{V\&L}, multimodal models \textcolor{mm}{MM}, early action prediction \textcolor{eap}{EAP}, video frame prediction \textcolor{vfp}{VFP}, state-based tasks \textcolor{states}{ST}, video anomaly detection \textcolor{vad}{VAD}, action anticipation \textcolor{aa}{AA}, and video generation \textcolor{gen}{Gen}. The number of relevant papers is approximated from all works citing influential papers with $\geq 300$ citations per group\textcolor{red}{$^*$}. Small disparities are expected as very recent works may not be considered. The increasing research activity in action understanding works can be seen from the total number of action understanding works shown for 2006, 2012, 2017, and 2023.}
    % legend: 
    %[color 1.1] Video reduction methods, 
    %[color 1.2] Temporal tasks, 
    %[color 1.3] Video-language, 
    %[color 1.4] Multi-modal, 
    %[color 2.1] EAP, 
    %[color 2.2] VFP, 
    %[color 2.3] State changes, 
    %[color 2.4] Anomaly detection
    %[color 3.1] AA
    %[color 3.2] Gnerative models
    \label{fig:works_over_year}
    \end{minipage}
\end{figure*}


\section{Research directions to explore}
\label{sec:outlook}

Progress in video understanding has been rapid over the last few years. As shown in \Cref{fig:works_over_year}, vision and language, video generation, and anomaly detection tasks have experienced significant interest recently while well-established problems such as temporal tasks have continued to be relevant. Considering current research directions, we provide a look into the future and explore three main pathways beyond the continuation of current trends toward scaling upwards. We envision ways for future models to reason abstractions in \Cref{sec:outlook::reason}. We then consider the tasks and objectives future action understanding models will aim to solve in \Cref{sec:outlook::tasks}. Finally, we discuss efficiency improvements for training and preserving robust representations in \Cref{sec:outlook::efficiency}.

%Overall, move to application, so focus on a diversity of tasks, in a production environment and with specific data.

% Adaptability to new tasks?



\subsection{Reasoning semantics}
\label{sec:outlook::reason}


Objectives and reasoning mechanisms are central to understanding actions. For this, a reliance on multiple levels of abstraction developed from both analogies and conceptualization is needed. We explore general future directions for interpreting intentions, adapting to unseen scenarios, and passing perceptual information across model generations. 


\subsubsection{Intentions in the visual world}

Desires and goals determine the performance of actions. Understanding intentions from visual cues is developed early in adolescence \citep{flavell1999cognitive} as it helps acquire foundational cognition abilities related to the causal relation between mental states \citep{flavell1998social}, roles in activities \citep{woodward2009infants}, and definitions of personality traits \citep{nelson1980factors}. These abilities are learned primarily observationally from interactions with the physical world and with limited use of language. Despite VLMs' great progress in learning procedural steps in tasks \citep{li2025llama,li2024mini,wu2024longvideobench} though natural language-guided embeddings, their reliance on visual information remains partial. \citet{al2024unibench} showed that scaling models and data sizes do not offer substantial reasoning performance gains for vision tasks despite strong performance in skill-based tasks. Thus, a new avenue for future video models to explore beyond scaling upward can be the design of open-world models from multi-level semantics describing human intentions and goals. Capturing information about the visual world in a scene may not necessarily require to be associated with language embeddings. Instead, they could correspond to different pairs of groups of modalities adaptively. Longitudinal data specific to individuals may also be used to tailor the model's understanding of the world based on the user's goals, objectives, intentions, and interactions.



\subsubsection{Novel problem adaptation}

Zero-shot performance has improved significantly over the past years. This is especially evident in language- and semantic-based video tasks aided by LLMs' large capacity and context. However, limitations still exist in tasks orthogonal to pre-text SSL \citep{liu2024mmbench}. Recent approaches such as modality/probabilistic adapters \citep{lin2023vision,sung2022vl,upadhyay2023probvlm}, information gating \citep{zhang2024llama}, visual prompt learning \citep{khattak2023maple}, knowledge distillation \citep{mistretta2024improving}, and model caching \citep{zhang2021tip} have been used to improve zero-shot downstream task performance by adjusting pre-trained models. However, very few structural elements or objectives in models explicitly improve zero-shot performance for unseen distinct tasks. Unified models that can be used as a mixture of experts controllers  \citep{bao2022vlmo,lin2024moe,wang2022image,yu2024boosting} are promising direction for bridging this. Sparsely trained models in mixture of experts settings allow for faster inference times with only task-relative sub-models using conditional computations \citep{bengio2013estimating,jacobs1991adaptive}. The integration of experts can be done regardless of the backbone architecture, with the approach being flexible to stand the test of time.

\blfootnote{\noindent \textcolor{red}{$^*$} From GoogleScholar as of the 28th of October 2024.}

\subsubsection{Linking visual understanding principles across model generations}

Current models rely on scale and data availability. Looking back, the field has shifted towards advancements that improve scalability; DT, to DPMs, CNNs, ViTs, and now VLMs. There are also several recently introduced candidates \citep{dao2022flashattention,gu2023mamba,poli2023hyena} that their scalability potential is being explored. Works to come will continue to push towards better-scaling architectures as they are still an active area of research with increasing innovation and interest. Future multi-modal models are expected to improve problem-solving capabilities for a greater number of tasks. However, traits learned for reasoning solutions to tasks are not passed across model generation. Designing models and architectures that can import principles learned about the visual world from the previous iteration of approaches, and refining them with new knowledge, is thus a promising path. Principles implicitly or explicitly imported in later models can potentially be inspired by globally accepted useful principles from human perception. Gestalt conditions from cognitive psychology \citep{koffka2013principles,kohler1967gestalt} describe essential properties, groupings, and identities in visual scenes. Convergence and improvements in resource use for each principle can be passed over model generations. This further provides a new avenue for evaluating models beyond skill-based benchmarks by migrating to benchmarks on cognitive principles.



\subsection{Better task definitions}
\label{sec:outlook::tasks}

Training and benchmarking models do not necessarily guarantee learning video temporal dynamics at a foundational level. Future research may revisit and integrate beneficial properties for representation learning. Additionally, future works can explore performance measurements beyond metrics, instead focusing on embedding distributions and feature correspondences. 


\subsubsection{Objectives}

Distributions learned from representation-based objectives can be effectively used as priors to downstream tasks \citep{bengio2013representation,janocha2017loss,larochelle2009exploring}. Drawing inspiration from \citep{bengio2013representation} a number of widely-accepted target beneficial properties are discussed below.

\noindent
\textbf{Temporal and spatial coherence}. Temporally or spatially proximal instances should correspond to similarities in their representations. This can be extended to maintaining proportional distances across both the pixel and embedding spaces. This coherence prior has been explored in objectives relating to time such as; cyclic consistency \citep{dwibedi2018temporal,donahue2024learning,haresh2021learning}, video procedural learning \citep{chen2022frame,sermanet2018time}, DTW \citep{dvornik2021drop,hadji2021representation}, and cross-frame stochasticity \citep{zhang2023modeling}. These priors can be explored in a more general context as pre-training tasks similar to SSL while tailoring to the nature of videos. 

\noindent
\textbf{Abstractions and hierarchies}. Beyond fine-grained categories and semantics, most current works do not explicitly learn levels of abstraction. They are primarily limited to implicit connections between specific types \citep{li2024deal} that often lead to spurious correlations \citep{chen2020counterfactual,kim2023exposing,tian2024argue} and task-/instance-based misalignments \citep{zhang2024rethinking}. Objectives that enforce abstraction hierarchies can potentially mitigate such misalignments. Promising efforts have included partial order relations \citep{alper2024emergent},  prototype learning \citep{ramesh2022hierarchical}, hyperbolic representations \citep{mettes2024hyperbolic}, and scene graphs \citep{li2024scene}. As models become more polysemantic, the utilization of natural hierarchies and abstractions is expected to be more important.

\noindent
\textbf{Natural clustering and manifolds}. Local representations tend to preserve similar polysemantic characteristics. Works have shown that real data are not represented within the totality of the feature space but instead form dense concentrations over specific regions \citep{genovese2012minimax,jiang2018trust,liang2022mind}. Using the tangent space of these distributions as a prior has shown great promise in vision tasks such as generation \citep{he2023manifold}, model explanations \citep{bordt2023manifold}, anomaly detection \citep{shin2023anomaly}, and corruption robustness \citep{chen2022vita}. However, using the tangent space from real data distributions as an objective-steering prior remains largely an open question for large-scale multi-model action understanding models. 



%\subsubsection{Learning schemes}
%- different learning schemes apart from pre-training/fine-tuning or VLM adoption --> online learning, self-supervision for specific (temporal) tasks

%\subsubsection{Deviations from memorized patterns}

%- Adaptations to deviations from memorized patterns (in-context learning, task familiarity vs task complexity)

\subsubsection{Limitations in performance beyond metrics}

Beyond benchmarking models on tasks and metrically evaluating performance, understanding the distributions and correlations learned provides new research opportunities. In-Context Learning (ICL) \citep{brown2020language,hoffmann2022empirical} and Chain-of Thought (CoT) \citep{wei2022chain} prompting are promising directions for LLMs and VLMs. \citet{bansal2023rethinking} has shown that LLMs' capabilities are only influenced by a small number of attention/feedforward layers which can perform highly for specific tasks. Both \citet{baldassini2024makes,chen2024understanding} also showed that ICL in VLMs primarily relies on text information and does not extend to the visual modality. Disparities between target and learned features can also happen due to shortcuts learned by models. Common factors that can lead to shortcuts include contrastive loss's multiple local minima \citep{robinson2021can}, vision information being suppressed by language \citep{li2023addressing}, and low mutual information between latent representations and real data \citep{adnan2022monitoring}. Recently, \citet{bleeker2024demonstrating} showed that introducing unique information distal to the overall training distribution favors VLMs' reliance on shortcuts for models trained on contrastive objectives. These insights provide opportunities for future research to explore objectives and models with better multi-modal and data-varying generalizability capabilities.


%\subsection{Understanding/explaining performance}
%- explainability
%- understanding limitations in task performance

\subsection{Efficiency}
\label{sec:outlook::efficiency}

Model efficiency has been regularly mentioned as a metric for real-time applicability. Looking beyond inference speeds bounded to specific computation environments, we overview general properties that future works may be inspired by. Given the rapid deployment of models in a multitude of applications we also highlight privacy risks that remain unaddressed alongside opportunities for domain specialization.  

\subsubsection{Generalizable priors}

Modern models are primarily trained on large-scale uncurated datasets aimed at multi-domain generalization. Training distributions can however include noise or be insufficient for domain-specific datasets. Recent approaches have aimed to reduce training data requirements by including distribution priors during training. \citet{kahana2022improving} aimed to improve zero-shot performance with a joint objective matching label distribution and minimizing the divergence to original zero-shot predictions. \citet{gao2022pyramidclip} used multiple levels of abstraction to contrast language and visual semantics and improve training efficiency. \citet{nag2024safari} used a weakly-supervised approach for refining pseudo-object-masks with cross-modal alignment in low-annotation settings. Approaches specifically utilizing priors in videos include concept distillation from normalized language embeddings \citep{ranasinghe2023language}, and motion-specific alignment between video and textual descriptions of movements \citep{zhang2024enhanced}. Distilling learned information to then be used as a prior within the optimization has been a promising route for reducing resource requirements for more efficient training. It can also impose a constraint on the nature of expected motions with potential benefits in model convergence.


\subsubsection{Privacy and specialization}


Vision-based models are susceptible to attacks that either invert their gradients to reconstruct inputs \citep{hatamizadeh2022gradvit} or discover intermediate representations \citep{fang2023gifd}. Inputs and features from VLMs can also be inferred through learnable vision-language triggers \citep{bai2024badclip}, inference-time adversarial perturbations in frames \citep{li2024fmm}, backdoor attacks through adversarial patches in training \citep{carlini2022poisoning}, and injecting malicious prompts during instruction tuning \citep{liang2024vl}. \citet{kariyappa2023cocktail} showed that semantics from the original data can still be recovered even in distributed settings over large batches. Such vulnerabilities can be exploited across multiple downstream tasks. Thus, evaluating model robustness is a broad topic that the community can attend to.

The importance of privacy can also be understood by the current shift toward domain-expert sub-models integrated into general-purpose frameworks. \citet{shen2024tag} showed that LLM specialization on domain-specific tasks significantly improves zero-shot generalizability for adjacent domains. Visual instructional tuning \citep{bai2024generalist} and evolutionary instruction-based prompting \citep{luo2024mmevol} have also shown promising results for vision-language models. Enhancing video-based models with domain specialization remains to be explored further in future works. This could be through either singular general models of high capacity or multiple models in holistic frameworks.  


%- in contrast to increasing scale
%- overcoming too strong priors
%- VLMs patch generalization?
% reinforcement learning?

% data privacy
% more human in the loop, interactive understanding

\section{Discussion}
\label{sec:discussion}

Action understanding includes a diverse set of tasks over multiple domains. As a now well-matured research field, we aimed to provide a comprehensive review of the main challenges, relevant datasets, seminal works with emphasis on recent advancements, and future research directions. The taxonomy of this survey is based on three temporal scopes from which tasks and approaches understand actions performed. We discussed recognition tasks that use full observations of actions and aim to infer fine or coarse-grained scene information. We then overviewed predictive tasks from partial observations of actions. Finally, we outlined forecasting tasks with anticipation models that infer general scene knowledge and forecast future actions not yet performed. The temporal nature of the three scopes is adjacent. We therefore expect future models to jointly address multiple aspects of action understanding. By capturing a holistic view of the current state of action understanding and a look forward, this survey offers a first step towards emerging and promising research directions.  