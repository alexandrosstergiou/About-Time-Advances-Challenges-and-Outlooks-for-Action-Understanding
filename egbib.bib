%% ---- SURVEYS PAPERS ----

@inproceedings{aggarwal1994articulated,
  title={Articulated and elastic non-rigid motion: A review},
  author={Aggarwal, Jake K and Cai, Qin and Liao, Wen and Sabata, Bikash},
  booktitle={Workshop on Motion of Non-rigid and Articulated Objects},
  year={1994}
}

@article{cedras1995motion,
  title={Motion-based recognition a survey},
  author={Cedras, Claudette and Shah, Mubarak},
  journal={IVC},
  year={1995},
}

@inproceedings{park2019relational,
  title={Relational knowledge distillation},
  author={Park, Wonpyo and Kim, Dongju and Lu, Yan and Cho, Minsu},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{rangrej2023glitr,
  title={GliTr: Glimpse Transformers with Spatiotemporal Consistency for Online Action Prediction},
  author={Rangrej, Samrudhdhi B and Liang, Kevin J and Hassner, Tal and Clark, James J},
  booktitle={WACV},
  year={2023}
}

@article{aggarwal1998nonrigid,
  title={Nonrigid motion analysis: Articulated and elastic motion},
  author={Aggarwal, Jake K and Cai, Quin and Liao, W and Sabata, Bikash},
  journal={CVIU},
  year={1998}
}

@article{aggarwal1999human,
  title={Human motion analysis: A review},
  author={Aggarwal, Jake K and Cai, Quin},
  journal={CVIU},
  year={1999}
}

@article{moeslund2001survey,
  title={A survey of computer vision-based human motion capture},
  author={Moeslund, Thomas B and Granum, Erik},
  journal={CVIU},
  year={2001}
}

@article{buxton2003learning,
  title={Learning and understanding dynamic scene activity: a review},
  author={Buxton, Hilary},
  journal={IVC},
  year={2003},
}

@article{moeslund2006survey,
  title={A survey of advances in vision-based human motion capture and analysis},
  author={Moeslund, Thomas B and Hilton, Adrian and Kr{\"u}ger, Volker},
  journal={CVIU},
  year={2006}
}

@article{yilmaz2006object,
  title={Object tracking: A survey},
  author={Yilmaz, Alper and Javed, Omar and Shah, Mubarak},
  journal={CSUR},
  year={2006}
}

@article{poppe2007vision,
  title={Vision-based human motion analysis: An overview},
  author={Poppe, Ronald},
  journal={CVIU},
  year={2007}
}

@article{turaga2008machine,
  title={Machine recognition of human activities: A survey},
  author={Turaga, Pavan and Chellappa, Rama and Subrahmanian, Venkatramana S and Udrea, Octavian},
  journal={IEEE TCSVT},
  year={2008}
}

@article{poppe2010survey,
  title={A survey on vision-based human action recognition},
  author={Poppe, Ronald},
  journal={IVC},
  year={2010}
}

@article{weinland2011survey,
  title={A survey of vision-based methods for action representation, segmentation and recognition},
  author={Weinland, Daniel and Ronfard, Remi and Boyer, Edmond},
  journal={CVIU},
  year={2011}
}

@article{chaaraoui2012review,
  title={A review on vision techniques applied to human behaviour analysis for ambient-assisted living},
  author={Chaaraoui, Alexandros Andr{\'e} and Climent-P{\'e}rez, Pau and Fl{\'o}rez-Revuelta, Francisco},
  journal={ESWA},
  year={2012}
}

@article{metaxas2013review,
  title={A review of motion analysis methods for human nonverbal communication computing},
  author={Metaxas, Dimitris and Zhang, Shaoting},
  journal={IVC},
  year={2013}
}

@article{vishwakarma2013survey,
  title={A survey on activity recognition and behavior understanding in video surveillance},
  author={Vishwakarma, Sarvesh and Agrawal, Anupam},
  journal={TVC},
  year={2013}
}

@article{herath2017going,
  title={Going deeper into action recognition: A survey},
  author={Herath, Samitha and Harandi, Mehrtash and Porikli, Fatih},
  journal={IVC},
  year={2017}
}

@article{wang2018rgb,
  title={RGB-D-based human motion recognition with deep learning: A survey},
  author={Wang, Pichao and Li, Wanqing and Ogunbona, Philip and Wan, Jun and Escalera, Sergio},
  journal={CVIU},
  year={2018}
}

@article{dhiman2019review,
  title={A review of state-of-the-art techniques for abnormal human activity recognition},
  author={Dhiman, Chhavi and Vishwakarma, Dinesh Kumar},
  journal={EAAI},
  year={2019}
}

@article{hussain2019different,
  title={Different approaches for human activity recognition: A survey},
  author={Hussain, Zawar and Sheng, Michael and Zhang, Wei Emma},
  journal={arxiv},
  year={2019}
}

@article{beddiar2020vision,
  title={Vision-based human activity recognition: a survey},
  author={Beddiar, Djamila Romaissa and Nini, Brahim and Sabokrou, Mohammad and Hadid, Abdenour},
  journal={MTA},
  year={2020}
}

@article{zheng2020deep,
  title={Deep learning-based human pose estimation: A survey},
  author={Zheng, Ce and Wu, Wenhan and Chen, Chen and Yang, Taojiannan and Zhu, Sijie and Shen, Ju and Kehtarnavaz, Nasser and Shah, Mubarak},
  journal={CSUR},
  year={2020}
}


@article{pareek2021survey,
  title={A survey on video-based human action recognition: recent updates, datasets, challenges, and applications},
  author={Pareek, Preksha and Thakkar, Ankit},
  journal={UMT-AIR},
  year={2021}
}

@article{stergiou2019analyzing,
  title={Analyzing human--human interactions: A survey},
  author={Stergiou, Alexandros and Poppe, Ronald},
  journal={CVIU},
  year={2019}
}

@article{rasouli2020deep,
  title={Deep learning for vision-based prediction: A survey},
  author={Rasouli, Amir},
  journal={arxiv},
  year={2020}
}

@article{yao2019review,
  title={A review of convolutional-neural-network-based action recognition},
  author={Yao, Guangle and Lei, Tao and Zhong, Jiandan},
  journal={PRL},
  year={2019}
}

@article{zhang2019comprehensive,
  title={A comprehensive survey of vision-based human action recognition methods},
  author={Zhang, Hong-Bo and Zhang, Yi-Xiang and Zhong, Bineng and Lei, Qing and Yang, Lijie and Du, Ji-Xiang and Chen, Duan-Sheng},
  journal={Sensors},
  year={2019}
}

@article{ramachandra2020survey,
  title={A survey of single-scene video anomaly detection},
  author={Ramachandra, Bharathkumar and Jones, Michael J and Vatsavai, Ranga Raju},
  journal={IEEE TPAMI},
  year={2020}
}

@article{rodin2021predicting,
  title={Predicting the future from first person (egocentric) vision: A survey},
  author={Rodin, Ivan and Furnari, Antonino and Mavroeidis, Dimitrios and Farinella, Giovanni Maria},
  journal={CVIU},
  year={2021},
}

@article{song2021human,
  title={Human pose estimation and its application to action recognition: A survey},
  author={Song, Liangchen and Yu, Gang and Yuan, Junsong and Liu, Zicheng},
  journal={JVCIR},
  year={2021}
}

@article{sun2022human,
  title={Human action recognition from various data modalities: A review},
  author={Sun, Zehua and Ke, Qiuhong and Rahmani, Hossein and Bennamoun, Mohammed and Wang, Gang and Liu, Jun},
  journal={IEEE TPAMI},
  year={2022}
}

@article{kong2022human,
  title={Human action recognition and prediction: A survey},
  author={Kong, Yu and Fu, Yun},
  journal={IJCV},
  year={2022}
}

@article{oprea2022review,
  title={A review on deep learning techniques for video prediction},
  author={Oprea, Sergiu and Martinez-Gonzalez, Pablo and Garcia-Garcia, Alberto and Castro-Vargas, John Alejandro and Orts-Escolano, Sergio and Garcia-Rodriguez, Jose and Argyros, Antonis},
  journal={IEEE TPAMI},
  year={2022}
}

@article{hu2022online,
  title={Online human action detection and anticipation in videos: A survey},
  author={Hu, Xuejiao and Dai, Jingzhao and Li, Ming and Peng, Chenglei and Li, Yang and Du, Sidan},
  journal={Neurocomputing},
  year={2022},
}

@article{schiappa2023self,
  title={Self-supervised learning for videos: A survey},
  author={Schiappa, Madeline C and Rawat, Yogesh S and Shah, Mubarak},
  journal={CSUR},
  year={2023}
}

@article{selva2023video,
  title={Video transformers: A survey},
  author={Selva, Javier and Johansen, Anders S and Escalera, Sergio and Nasrollahi, Kamal and Moeslund, Thomas B and Clap{\'e}s, Albert},
  journal={IEEE TPAMI},
  year={2023}
}

@article{ding2023temporal,
  title={Temporal action segmentation: An analysis of modern techniques},
  author={Ding, Guodong and Sener, Fadime and Yao, Angela},
  journal={IEEE TPAMI},
  year={2023}
}

@article{zhong2023survey,
  title={A survey on deep learning techniques for action anticipation},
  author={Zhong, Zeyun and Martin, Manuel and Voit, Michael and Gall, Juergen and Beyerer, J{\"u}rgen},
  journal={arxiv},
  year={2023}
}

@article{plizzari2024outlook,
  title={An Outlook into the Future of Egocentric Vision},
  author={Plizzari, Chiara and Goletto, Gabriele and Furnari, Antonino and Bansal, Siddhant and Ragusa, Francesco and Farinella, Giovanni Maria and Damen, Dima and Tommasi, Tatiana},
  journal={IJCV},
  year={2024}
}



%% ---- MOTION-BASED PAPERS ----
@article{johansson1975visual,
  title={Visual motion perception},
  author={Johansson, Gunnar},
  journal={Scientific American},
  year={1975}
}

@article{marr1982representation,
  title={Representation and recognition of the movements of shapes},
  author={Marr, David and Vaina, Lucia},
  journal={Series B. Biological Sciences},
  year={1982}
}

@article{hogg1983model,
  title={Model-based vision: a program to see a walking person},
  author={Hogg, David},
  journal={IVC},
  year={1983}
}

@article{flinchbaugh1981theory,
  title={A theory of spatio-temporal aggregation for vision},
  author={Flinchbaugh, Bruce E and Chandrasekaran, B},
  journal={Artificial Intelligence},
  year={1981}
}

@article{potter1977scene,
  title={Scene segmentation using motion information},
  author={Potter, Jerry L},
  journal={CGIP},
  year={1977}
}

@article{orourke1980model,
  title={Model-based image analysis of human motion using constraint propagation},
  author={O'rourke, Joseph and Badler, Norman I},
  journal={IEEE TPAMI},
  year={1980}
}

@article{roach1979computer,
  title={Computer tracking of objects moving in space},
  author={Roach, John W and Aggarwal, JK},
  journal={IEEE TPAMI},
  year={1979}
}

@book{ullman1979interpretation,
  title={The interpretation of visual motion},
  author={Ullman, Shimon},
  year={1979},
  publisher={MIT Press}
}

@inproceedings{ni2014multiple,
  title={Multiple granularity analysis for fine-grained action detection},
  author={Ni, Bingbing and Paramathayalan, Vignesh R and Moulin, Pierre},
  booktitle={CVPR},
  year={2014}
}


@article{rohr1994towards,
  title={Towards model-based recognition of human movements in image sequences},
  author={Rohr, Karl},
  journal={CVGIP},
  year={1994}
}

@article{isard1998condensation,
  title={Condensation—conditional density propagation for visual tracking},
  author={Isard, Michael and Blake, Andrew},
  journal={IJCV},
  year={1998}
}

@inproceedings{cipolla1990dynamic,
  title={The dynamic analysis of apparent contours},
  author={Cipolla, Roberto and Blake, Andrew},
  booktitle={ICCV},
  year={1990}
}




%% ----- END OF MOTION-BASED PAPERS -----

@inproceedings{forstner1987fast,
  title={A fast operator for detection and precise location of distinct points, corners and centres of circular features},
  author={F{\"o}rstner, Wolfgang and G{\"u}lch, Eberhard},
  booktitle={ICFPPD},
  year={1987}
}


@inproceedings{harris1988combined,
  title={A combined corner and edge detector},
  author={Harris, Chris and Stephens, Mike and others},
  booktitle={AVC},
  year={1988}
}



@inproceedings{schuldt2004recognizing,
  title={Recognizing human actions: a local SVM approach},
  author={Schuldt, Christian and Laptev, Ivan and Caputo, Barbara},
  booktitle={ICPR},
  year={2004}
}

@article{gorelick2007actions,
  title={Actions as space-time shapes},
  author={Gorelick, Lena and Blank, Moshe and Shechtman, Eli and Irani, Michal and Basri, Ronen},
  journal={IEEE TPAMI},
  year={2007}
}

@inproceedings{laptev2007retrieving,
  title={Retrieving actions in movies},
  author={Laptev, Ivan and P{\'e}rez, Patrick},
  booktitle={ICCV},
  year={2007}
}

@inproceedings{wang2007human,
  title={Human activity recognition based on r transform},
  author={Wang, Ying and Huang, Kaiqi and Tan, Tieniu},
  booktitle={CVPR},
  year={2007}
}

@inproceedings{nowozin2007discriminative,
  title={Discriminative subsequence mining for action classification},
  author={Nowozin, Sebastian and Bakir, Gokhan and Tsuda, Koji},
  booktitle={ICCV},
  year={2007}
}



@inproceedings{efros2003recognizing,
  title={Recognizing action at a distance},
  author={Efros, Alexei and Berg, Alexander and Mori, Greg and Malik, Jitendra},
  booktitle={ICCV},
  year={2003}
}

@inproceedings{liu2008learning,
  title={Learning human actions via information maximization},
  author={Liu, Jingen and Shah, Mubarak},
  booktitle={CVPR},
  year={2008}
}


@inproceedings{zelnik2001event,
  title={Event-based analysis of video},
  author={Zelnik-Manor, Lihi and Irani, Michal},
  booktitle={CVPR},
  year={2001}
}

@article{gorelick2006shape,
  title={Shape representation and classification using the poisson equation},
  author={Gorelick, Lena and Galun, Meirav and Sharon, Eitan and Basri, Ronen and Brandt, Achi},
  journal={IEEE TPAMI},
  year={2006}
}

@inproceedings{stergiou2021multi,
  title={Multi-temporal convolutions for human action recognition in videos},
  author={Stergiou, Alexandros and Poppe, Ronald},
  booktitle={IJCNN},
  year={2021}
}

@inproceedings{chen2019drop,
  title={Drop an octave: Reducing spatial redundancy in convolutional neural networks with octave convolution},
  author={Chen, Yunpeng and Fan, Haoqi and Xu, Bing and Yan, Zhicheng and Kalantidis, Yannis and Rohrbach, Marcus and Yan, Shuicheng and Feng, Jiashi},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{jia2008human,
  title={Human action recognition using local spatio-temporal discriminant embedding},
  author={Jia, Kui and Yeung, Dit-Yan},
  booktitle={CVPR},
  year={2008}
}



@inproceedings{rodriguez2008action,
  title={Action mach a spatio-temporal maximum average correlation height filter for action recognition},
  author={Rodriguez, Mikel D and Ahmed, Javed and Shah, Mubarak},
  booktitle={CVPR},
  year={2008}
}

@inproceedings{laptev2008learning,
  title={Learning realistic human actions from movies},
  author={Laptev, Ivan and Marszalek, Marcin and Schmid, Cordelia and Rozenfeld, Benjamin},
  booktitle={CVPR},
  year={2008}
}

@inproceedings{ryoo2009spatio,
  title={Spatio-temporal relationship match: Video structure comparison for recognition of complex human activities},
  author={Ryoo, Michael S and Aggarwal, Jake K},
  booktitle={ICCV},
  year={2009}
}

@inproceedings{liu2009recognizing,
  title={Recognizing realistic actions from videos ``in the wild'''},
  author={Liu, Jingen and Luo, Jiebo and Shah, Mubarak},
  booktitle={CVPR},
  year={2009}
}

@inproceedings{wong2007extracting,
  title={Extracting spatiotemporal interest points using global information},
  author={Wong, Shu-Fai and Cipolla, Roberto},
  booktitle={ICCV},
  year={2007},
}


@inproceedings{marszalek2009actions,
  title={Actions in context},
  author={Marszalek, Marcin and Laptev, Ivan and Schmid, Cordelia},
  booktitle={CVPR},
  year={2009}
}

@inproceedings{patron2010high,
  title={High Five: Recognising human interactions in TV shows.},
  author={Patron-Perez, Alonso and Marszalek, Marcin and Zisserman, Andrew and Reid, Ian},
  booktitle={BMVC},
  year={2010}
}

@article{reddy2013recognizing,
  title={Recognizing 50 human action categories of web videos},
  author={Reddy, Kishore K and Shah, Mubarak},
  journal={MVA},
  year={2013}
}

@inproceedings{niebles2010modeling,
  title={Modeling temporal structure of decomposable motion segments for activity classification},
  author={Niebles, Juan Carlos and Chen, Chih-Wei and Fei-Fei, Li},
  booktitle={ECCV},
  year={2010}
}

@inproceedings{kuehne2011hmdb,
  title={HMDB: a large video database for human motion recognition},
  author={Kuehne, Hildegard and Jhuang, Hueihan and Garrote, Est{\'\i}baliz and Poggio, Tomaso and Serre, Thomas},
  booktitle={ICCV},
  year={2011}
}

@inproceedings{jiang2011consumer,
  title={Consumer video understanding: A benchmark database and an evaluation of human and machine performance},
  author={Jiang, Yu-Gang and Ye, Guangnan and Chang, Shih-Fu and Ellis, Daniel and Loui, Alexander C},
  booktitle={ICMR},
  year={2011}
}

@article{soomro2012ucf101,
  title={UCF101: A dataset of 101 human actions classes from videos in the wild},
  author={Soomro, Khurram and Zamir, Amir Roshan and Shah, Mubarak},
  journal={arxiv},
  year={2012}
}

@inproceedings{sung2012unstructured,
  title={Unstructured human activity detection from rgbd images},
  author={Sung, Jaeyong and Ponce, Colin and Selman, Bart and Saxena, Ashutosh},
  booktitle={ICRA},
  year={2012}
}

@inproceedings{rohrbach2012database,
  title={A database for fine grained activity detection of cooking activities},
  author={Rohrbach, Marcus and Amin, Sikandar and Andriluka, Mykhaylo and Schiele, Bernt},
  booktitle={CVPR},
  year={2012}
}

@inproceedings{stein2013combining,
  title={Combining embedded accelerometers with computer vision for recognizing food preparation activities},
  author={Stein, Sebastian and McKenna, Stephen J},
  booktitle={UbiComp},
  year={2013}
}

@article{koppula2013learning,
  title={Learning human activities and object affordances from rgb-d videos},
  author={Koppula, Hema Swetha and Gupta, Rudhir and Saxena, Ashutosh},
  journal={IJRR},
  year={2013},
}

@inproceedings{karpathy2014large,
  title={Large-scale video classification with convolutional neural networks},
  author={Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and Fei-Fei, Li},
  booktitle={CVPR},
  year={2014}
}

@inproceedings{li2015delving,
  title={Delving into egocentric actions},
  author={Li, Yin and Ye, Zhefan and Rehg, James M},
  booktitle={CVPR},
  year={2015}
}

@inproceedings{caba2015activitynet,
  title={Activitynet: A large-scale video benchmark for human activity understanding},
  author={Caba Heilbron, Fabian and Escorcia, Victor and Ghanem, Bernard and Carlos Niebles, Juan},
  booktitle={CVPR},
  year={2015}
}

@inproceedings{wu2015watch,
  title={Watch-n-patch: Unsupervised understanding of actions and relations},
  author={Wu, Chenxia and Zhang, Jiemi and Savarese, Silvio and Saxena, Ashutosh},
  booktitle={CVPR},
  year={2015}
}

@inproceedings{kahatapitiya2024victr,
  title={Victr: Video-conditioned text representations for activity recognition},
  author={Kahatapitiya, Kumara and Arnab, Anurag and Nagrani, Arsha and Ryoo, Michael S},
  booktitle={CVPR},
  year={2024}
}


@inproceedings{srivastava2024omnivec2,
  title={OmniVec2-A Novel Transformer based Network for Large Scale Multimodal and Multitask Learning},
  author={Srivastava, Siddharth and Sharma, Gaurav},
  booktitle={CVPR},
  year={2024}
}

@article{dai2022one,
  title={One model, multiple modalities: A sparsely activated approach for text, sound, image, video and code},
  author={Dai, Yong and Tang, Duyu and Liu, Liangxin and Tan, Minghuan and Zhou, Cong and Wang, Jingquan and Feng, Zhangyin and Zhang, Fan and Hu, Xueyu and Shi, Shuming},
  journal={arxiv},
  year={2022}
}

@inproceedings{xue2023dynamic,
  title={Dynamic multimodal fusion},
  author={Xue, Zihui and Marculescu, Radu},
  booktitle={CVPR},
  year={2023}
}

@article{abu2016youtube,
  title={Youtube-8m: A large-scale video classification benchmark},
  author={Abu-El-Haija, Sami and Kothari, Nisarg and Lee, Joonseok and Natsev, Paul and Toderici, George and Varadarajan, Balakrishnan and Vijayanarasimhan, Sudheendra},
  journal={arxiv},
  year={2016}
}

@inproceedings{sigurdsson2016hollywood,
  title={Hollywood in homes: Crowdsourcing data collection for activity understanding},
  author={Sigurdsson, Gunnar A and Varol, G{\"u}l and Wang, Xiaolong and Farhadi, Ali and Laptev, Ivan and Gupta, Abhinav},
  booktitle={ECCV},
  year={2016}
}

@inproceedings{van2016spatio,
  title={Spatio-temporal detection of fine-grained dyadic human interactions},
  author={Van Gemeren, Coert and Poppe, Ronald and Veltkamp, Remco C},
  booktitle={HBU},
  year={2016}
}

@inproceedings{li2016recognition,
  title={Recognition of ongoing complex activities by sequence prediction over a hierarchical label space},
  author={Li, Wenbin and Fritz, Mario},
  booktitle={WACV},
  year={2016}
}

@article{edwards2016pose,
  title={From pose to activity: Surveying datasets and introducing CONVERSE},
  author={Edwards, Michael and Deng, Jingjing and Xie, Xianghua},
  journal={CVIU},
  year={2016}
}

@inproceedings{de2016online,
  title={Online action detection},
  author={De Geest, Roeland and Gavves, Efstratios and Ghodrati, Amir and Li, Zhenyang and Snoek, Cees and Tuytelaars, Tinne},
  booktitle={ECCV},
  year={2016}
}

@inproceedings{barekatain2017okutama,
  title={Okutama-action: An aerial view video dataset for concurrent human action detection},
  author={Barekatain, Mohammadamin and Mart{\'\i}, Miquel and Shih, Hsueh-Fu and Murray, Samuel and Nakayama, Kotaro and Matsuo, Yutaka and Prendinger, Helmut},
  booktitle={ICCVw},
  year={2017}
}

@article{kay2017kinetics,
  title={The kinetics human action video dataset},
  author={Kay, Will and Carreira, Joao and Simonyan, Karen and Zhang, Brian and Hillier, Chloe and Vijayanarasimhan, Sudheendra and Viola, Fabio and Green, Tim and Back, Trevor and Natsev, Paul and others},
  journal={arxiv},
  year={2017}
}

@inproceedings{goyal2017something,
  title={The" something something" video database for learning and evaluating visual common sense},
  author={Goyal, Raghav and Ebrahimi Kahou, Samira and Michalski, Vincent and Materzynska, Joanna and Westphal, Susanne and Kim, Heuna and Haenel, Valentin and Fruend, Ingo and Yianilos, Peter and Mueller-Freitag, Moritz and others},
  booktitle={ICCV},
  year={2017}
}

@article{monfort2019moments,
  title={Moments in time dataset: one million videos for event understanding},
  author={Monfort, Mathew and Andonian, Alex and Zhou, Bolei and Ramakrishnan, Kandan and Bargal, Sarah Adel and Yan, Tom and Brown, Lisa and Fan, Quanfu and Gutfreund, Dan and Vondrick, Carl and others},
  journal={IEEE TPAMI},
  year={2019}
}

@article{yeung2018every,
  title={Every moment counts: Dense detailed labeling of actions in complex videos},
  author={Yeung, Serena and Russakovsky, Olga and Jin, Ning and Andriluka, Mykhaylo and Mori, Greg and Fei-Fei, Li},
  journal={IJCV},
  year={2018}
}

@article{weinzaepfel2016towards,
  title={Towards weakly-supervised action localization},
  author={Weinzaepfel, Philippe and Martin, Xavier and Schmid, Cordelia},
  journal={arxiv},
  year={2016}
}

@inproceedings{li2018resound,
  title={Resound: Towards action recognition without representation bias},
  author={Li, Yingwei and Li, Yi and Vasconcelos, Nuno},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{damen2018scaling,
  title={Scaling egocentric vision: The epic-kitchens dataset},
  author={Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria and Fidler, Sanja and Furnari, Antonino and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan and Perrett, Toby and Price, Will and others},
  booktitle={ECCV},
  year={2018}
}

@article{carreira2018short,
  title={A short note about kinetics-600},
  author={Carreira, Joao and Noland, Eric and Banki-Horvath, Andras and Hillier, Chloe and Zisserman, Andrew},
  journal={arxiv},
  year={2018}
}

@inproceedings{fouhey2018lifestyle,
  title={From lifestyle vlogs to everyday interactions},
  author={Fouhey, David F and Kuo, Wei-cheng and Efros, Alexei A and Malik, Jitendra},
  booktitle={CVPR},
  year={2018}
}

@article{carreira2019short,
  title={A short note on the kinetics-700 human action dataset},
  author={Carreira, Joao and Noland, Eric and Hillier, Chloe and Zisserman, Andrew},
  journal={arxiv},
  year={2019}
}

@inproceedings{zhao2019hacs,
  title={Hacs: Human action clips and segments dataset for recognition and temporal localization},
  author={Zhao, Hang and Torralba, Antonio and Torresani, Lorenzo and Yan, Zhicheng},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{ghadiyaram2019large,
  title={Large-scale weakly-supervised pre-training for video action recognition},
  author={Ghadiyaram, Deepti and Tran, Du and Mahajan, Dhruv},
  booktitle={CVPR},
  year={2019}
}

@article{piergiovanni2020avid,
  title={Avid dataset: Anonymized videos from diverse countries},
  author={Piergiovanni, AJ and Ryoo, Michael},
  journal={NeurIPS},
  year={2020}
}

@inproceedings{diba2020large,
  title={Large scale holistic video understanding},
  author={Diba, Ali and Fayyaz, Mohsen and Sharma, Vivek and Paluri, Manohar and Gall, J{\"u}rgen and Stiefelhagen, Rainer and Van Gool, Luc},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{chung2021haa500,
  title={Haa500: Human-centric atomic action dataset with curated videos},
  author={Chung, Jihoon and Wuu, Cheng-hsin and Yang, Hsuan-ru and Tai, Yu-Wing and Tang, Chi-Keung},
  booktitle={ICCV},
  year={2021}
}

@article{smaira2020short,
  title={A short note on the kinetics-700-2020 human action dataset},
  author={Smaira, Lucas and Carreira, Jo{\~a}o and Noland, Eric and Clancy, Ellen and Wu, Amy and Zisserman, Andrew},
  journal={arxiv},
  year={2020}
}

@inproceedings{shao2020finegym,
  title={Finegym: A hierarchical video dataset for fine-grained action understanding},
  author={Shao, Dian and Zhao, Yue and Dai, Bo and Lin, Dahua},
  booktitle={CVPR},
  year={2020}
}

@article{damen2022rescaling,
  title={Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100},
  author={Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria and Furnari, Antonino and Kazakos, Evangelos and Ma, Jian and Moltisanti, Davide and Munro, Jonathan and Perrett, Toby and Price, Will and others},
  journal={IJCV},
  year={2022}
}

%% ---- ACTION RECOGNITION DATASETS ----


%% ----- END OF ACTION REC DATASETS -----


%% ---- ACTION RECOGNITION PAPERS ----

@inproceedings{dollar2005behavior,
  title={Behavior recognition via sparse spatio-temporal features},
  author={Doll{\'a}r, Piotr and Rabaud, Vincent and Cottrell, Garrison and Belongie, Serge},
  booktitle={VS-PETS},
  year={2005}
}

@inproceedings{blank2005actions,
  title={Actions as space-time shapes},
  author={Blank, Moshe and Gorelick, Lena and Shechtman, Eli and Irani, Michal and Basri, Ronen},
  booktitle={ICCV},
  year={2005}
}

@inproceedings{yilmaz2005recognizing,
  title={Recognizing human actions in videos acquired by uncalibrated moving cameras},
  author={Yilmaz, Alper and Shah, Mubarak},
  booktitle={ICCV},
  year={2005},
}

@inproceedings{oikonomopoulos2005spatiotemporal,
  title={Spatiotemporal saliency for human action recognition},
  author={Oikonomopoulos, Antonios and Patras, Ioannis and Pantic, Maja},
  booktitle={ICME},
  year={2005}
}

@inproceedings{weinland2007action,
  title={Action recognition from arbitrary views using 3d exemplars},
  author={Weinland, Daniel and Boyer, Edmond and Ronfard, Remi},
  booktitle={ICCV},
  year={2007}
}

@inproceedings{niebles2007hierarchical,
  title={A hierarchical model of shape and appearance for human action classification},
  author={Niebles, Juan Carlos and Fei-Fei, Li},
  booktitle={CVPR},
  year={2007}
}

@article{gupta2009observing,
  title={Observing human-object interactions: Using spatial and functional compatibility for recognition},
  author={Gupta, Abhinav and Kembhavi, Aniruddha and Davis, Larry S},
  journal={IEEE TPAMI},
  year={2009}
}

@inproceedings{ikizler2010object,
  title={Object, scene and actions: Combining multiple features for human action recognition},
  author={Ikizler-Cinbis, Nazli and Sclaroff, Stan},
  booktitle={ECCV},
  year={2010}
}


@inproceedings{pishchulin2013strong,
  title={Strong appearance and expressive spatial models for human pose estimation},
  author={Pishchulin, Leonid and Andriluka, Mykhaylo and Gehler, Peter and Schiele, Bernt},
  booktitle={CVPR},
  year={2013}
}

@article{kviatkovsky2014online,
  title={Online action recognition using covariance of shape and motion},
  author={Kviatkovsky, Igor and Rivlin, Ehud and Shimshoni, Ilan},
  journal={CVIU},
  year={2014}
}


@inproceedings{rahmani2014real,
  title={Real time action recognition using histograms of depth gradients and random decision forests},
  author={Rahmani, Hossein and Mahmood, Arif and Huynh, Du Q and Mian, Ajmal},
  booktitle={WACV},
  year={2014}
}



@inproceedings{yao2010modeling,
  title={Modeling mutual context of object and human pose in human-object interaction activities},
  author={Yao, Bangpeng and Fei-Fei, Li},
  booktitle={CVPR},
  year={2010}
}


@inproceedings{ke2007spatio,
  title={Spatio-temporal shape and flow correlation for action recognition},
  author={Ke, Yan and Sukthankar, Rahul and Hebert, Martial},
  booktitle={CVPR},
  year={2007}
}

@inproceedings{wong2007learning,
  title={Learning motion categories using both semantic and structural information},
  author={Wong, Shu-Fai and Kim, Tae-Kyun and Cipolla, Roberto},
  booktitle={CVPR},
  year={2007}
}

@article{wang2007learning,
  title={Learning and matching of dynamic shape manifolds for human action recognition},
  author={Wang, Liang and Suter, David},
  journal={IEEE T-IP},
  year={2007}
}

@inproceedings{ikizler2007human,
  title={Human action recognition using distribution of oriented rectangular patches},
  author={Ikizler, Nazl{\i} and Duygulu, P{\i}nar},
  booktitle={Workshop on Human Motion},
  year={2007},
}

@inproceedings{ikizler2007searching,
  title={Searching video for complex activities with finite state models},
  author={Ikizler, Nazli and Forsyth, David},
  booktitle={CVPR},
  year={2007}
}

@inproceedings{husz2007human,
  title={Human activity recognition with action primitives},
  author={Husz, Zsolt L and Wallace, Andrew M and Green, Patrick R},
  booktitle={AVSS},
  year={2007}
}

@inproceedings{schindler2008action,
  title={Action snippets: How many frames does human action recognition require?},
  author={Schindler, Konrad and Van Gool, Luc},
  booktitle={CVPR},
  year={2008}
}

@article{ali2008human,
  title={Human action recognition in videos using kinematic features and multiple instance learning},
  author={Ali, Saad and Shah, Mubarak},
  journal={IEEE TPAMI},
  year={2008}
}

@inproceedings{fathi2008action,
  title={Action recognition by learning mid-level motion features},
  author={Fathi, Alireza and Mori, Greg},
  booktitle={CVPR},
  year={2008}
}

@inproceedings{tran2008human,
  title={Human activity recognition with metric learning},
  author={Tran, Du and Sorokin, Alexander},
  booktitle={ECCV},
  year={2008}
}

@inproceedings{liu2008recognizing,
  title={Recognizing human actions using multiple features},
  author={Liu, Jingen and Ali, Saad and Shah, Mubarak},
  booktitle={CVPR},
  year={2008}
}

@inproceedings{weinland2008action,
  title={Action recognition using exemplar-based embedding},
  author={Weinland, Daniel and Boyer, Edmond},
  booktitle={CVPR},
  year={2008}
}

@inproceedings{zhang2008motion,
  title={Motion context: A new representation for human action recognition},
  author={Zhang, Ziming and Hu, Yiqun and Chan, Syin and Chia, Liang-Tien},
  booktitle={ECCV},
  year={2008}
}

@inproceedings{yan2008learning,
  title={Learning 4D action feature models for arbitrary view action recognition},
  author={Yan, Pingkun and Khan, Saad M and Shah, Mubarak},
  booktitle={CVPR},
  year={2008}
}

@inproceedings{natarajan2008online,
  title={Online, real-time tracking and recognition of human actions},
  author={Natarajan, Pradeep and Nevatia, Ramakant},
  booktitle={WMVC},
  year={2008}
}

@inproceedings{batra2008space,
  title={Space-time shapelets for action recognition},
  author={Batra, Dhruv and Chen, Tsuhan and Sukthankar, Rahul},
  booktitle={WMVC},
  year={2008}
}

@article{tran2012part,
  title={Part-based motion descriptor image for human action recognition},
  author={Tran, Khai N and Kakadiaris, Ioannis A and Shah, Shishir K},
  journal={PR},
  year={2012}
}


@article{liang2008learning,
  title={Learning atomic human actions using variable-length Markov models},
  author={Liang, Yu-Ming and Shih, Sheng-Wen and Shih, Arthur Chun-Chieh and Liao, Hong-Yuan Mark and Lin, Cheng-Chung},
  journal={IEEE TSMC},
  year={2008}
}

@inproceedings{wang2009evaluation,
  title={Evaluation of local spatio-temporal features for action recognition},
  author={Wang, Heng and Ullah, Muhammad Muneeb and Klaser, Alexander and Laptev, Ivan and Schmid, Cordelia},
  booktitle={BMVC},
  year={2009}
}

@inproceedings{yeffet2009local,
  title={Local trinary patterns for human action recognition},
  author={Yeffet, Lahav and Wolf, Lior},
  booktitle={ICCV},
  year={2009}
}


@inproceedings{duchenne2009automatic,
  title={Automatic annotation of human actions in video},
  author={Duchenne, Olivier and Laptev, Ivan and Sivic, Josef and Bach, Francis and Ponce, Jean},
  booktitle={ICCV},
  year={2009}
}


@article{iosifidis2012view,
  title={View-invariant action recognition based on artificial neural networks},
  author={Iosifidis, Alexandros and Tefas, Anastasios and Pitas, Ioannis},
  journal={IEEE TNNLS},
  year={2012}
}

@article{wang2009human,
  title={Human action recognition by semilatent topic models},
  author={Wang, Yang and Mori, Greg},
  journal={IEEE TPAMI},
  year={2009}
}

@article{escobar2009action,
  title={Action recognition using a bio-inspired feedforward spiking network},
  author={Escobar, Maria-Jose and Masson, Guillaume S and Vieville, Thierry and Kornprobst, Pierre},
  journal={IJCV},
  year={2009}
}

@inproceedings{sheikh2005exploring,
  title={Exploring the space of a human action},
  author={Sheikh, Yaser and Sheikh, Mumtaz and Shah, Mubarak},
  booktitle={ICCV},
  year={2005}
}

@article{niebles2008unsupervised,
  title={Unsupervised learning of human action categories using spatial-temporal words},
  author={Niebles, Juan Carlos and Wang, Hongcheng and Fei-Fei, Li},
  journal={IJCV},
  year={2008}
}

@inproceedings{sun2009action,
  title={Action recognition via local descriptors and holistic features},
  author={Sun, Xinghua and Chen, Mingyu and Hauptmann, Alexander},
  booktitle={CVPRw},
  year={2009}
}

@inproceedings{reddy2009incremental,
  title={Incremental action recognition using feature-tree},
  author={Reddy, Kishore K and Liu, Jingen and Shah, Mubarak},
  booktitle={ICCV},
  year={2009}
}

@inproceedings{willems2009exemplar,
  title={Exemplar-based Action Recognition in Video.},
  author={Willems, Geert and Becker, Jan Hendrik and Tuytelaars, Tinne and Van Gool, Luc},
  booktitle={BMVC},
  year={2009}
}

@article{gilbert2010action,
  title={Action recognition using mined hierarchical compound features},
  author={Gilbert, Andrew and Illingworth, John and Bowden, Richard},
  journal={IEEE TPAMI},
  year={2010}
}

@inproceedings{taylor2010convolutional,
  title={Convolutional learning of spatio-temporal features},
  author={Taylor, Graham W and Fergus, Rob and LeCun, Yann and Bregler, Christoph},
  booktitle={ECCV},
  year={2010}
}

@inproceedings{bilen2016dynamic,
  title={Dynamic image networks for action recognition},
  author={Bilen, Hakan and Fernando, Basura and Gavves, Efstratios and Vedaldi, Andrea and Gould, Stephen},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{yue2015beyond,
  title={Beyond short snippets: Deep networks for video classification},
  author={Yue-Hei Ng, Joe and Hausknecht, Matthew and Vijayanarasimhan, Sudheendra and Vinyals, Oriol and Monga, Rajat and Toderici, George},
  booktitle={CVPR},
  year={2015}
}

@article{simonyan2014two,
  title={Two-stream convolutional networks for action recognition in videos},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={NeurIPS},
  year={2014}
}

@article{ji20123d,
  title={3D convolutional neural networks for human action recognition},
  author={Ji, Shuiwang and Xu, Wei and Yang, Ming and Yu, Kai},
  journal={IEEE TPAMI},
  year={2012}
}

@inproceedings{tran2015learning,
  title={Learning spatiotemporal features with 3d convolutional networks},
  author={Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
  booktitle={ICCV},
  year={2015}
}

@inproceedings{sadanand2012action,
  title={Action bank: A high-level representation of activity in video},
  author={Sadanand, Sreemanananth and Corso, Jason J},
  booktitle={CVPR},
  year={2012}
}

@inproceedings{laptev2003space,
  title={Space-time Interest Points},
  author={Laptev, Ivan and Lindeberg, Tony},
  booktitle={ICCV},
  year={2003}
}

@inproceedings{amer2012sum,
  title={Sum-product networks for modeling activities with stochastic structure},
  author={Amer, Mohamed R and Todorovic, Sinisa},
  booktitle={CVPR},
  year={2012}
}


@article{bobick2001recognition,
  title={The recognition of human movement using temporal templates},
  author={Bobick, Aaron F. and Davis, James W.},
  journal={IEEE TPAMI},
  year={2001}
}

@article{yilmaz2006matching,
  title={Matching actions in presence of camera motion},
  author={Yilmaz, Alper and Shah, Mubarak},
  journal={CVIU},
  year={2006}
}

@inproceedings{shechtman2005space,
  title={Space-time behavior based correlation},
  author={Shechtman, Eli and Irani, Michal},
  booktitle={CVPR},
  year={2005}
}



@inproceedings{jain2015modeep,
  title={Modeep: A deep learning framework using motion features for human pose estimation},
  author={Jain, Arjun and Tompson, Jonathan and LeCun, Yann and Bregler, Christoph},
  booktitle={ACCV},
  year={2015}
}

@inproceedings{le2011learning,
  title={Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis},
  author={Le, Quoc V and Zou, Will Y and Yeung, Serena Y and Ng, Andrew Y},
  booktitle={CVPR},
  year={2011}
}

@inproceedings{baccouche2011sequential,
  title={Sequential deep learning for human action recognition},
  author={Baccouche, Moez and Mamalet, Franck and Wolf, Christian and Garcia, Christophe and Baskurt, Atilla},
  booktitle={HBU},
  year={2011}
}

@inproceedings{sharma2015action,
  title={Action recognition using visual attention},
  author={Sharma, Shikhar and Kiros, Ryan and Salakhutdinov, Ruslan},
  booktitle={ICLR},
  year={2015}
}

@inproceedings{ballas2015delving,
  title={Delving deeper into convolutional networks for learning video representations},
  author={Ballas, Nicolas and Yao, Li and Pal, Chris and Courville, Aaron},
  booktitle={ICLR},
  year={2015}
}

@inproceedings{sun2015human,
  title={Human action recognition using factorized spatio-temporal convolutional networks},
  author={Sun, Lin and Jia, Kui and Yeung, Dit-Yan and Shi, Bertram E},
  booktitle={ICCV},
  year={2015}
}

@inproceedings{fernando2015modeling,
  title={Modeling video evolution for action recognition},
  author={Fernando, Basura and Gavves, Efstratios and Oramas, Jose M and Ghodrati, Amir and Tuytelaars, Tinne},
  booktitle={CVPR},
  year={2015}
}

@article{fernando2016rank,
  title={Rank pooling for action recognition},
  author={Fernando, Basura and Gavves, Efstratios and Oramas, Jos{\'e} and Ghodrati, Amir and Tuytelaars, Tinne},
  journal={IEEE TPAMI},
  year={2016}
}

@inproceedings{feichtenhofer2017spatiotemporal,
  title={Spatiotemporal multiplier networks for video action recognition},
  author={Feichtenhofer, Christoph and Pinz, Axel and Wildes, Richard P},
  booktitle={CVPR},
  year={2017}
}

@article{ullah2017action,
  title={Action recognition in video sequences using deep bi-directional LSTM with CNN features},
  author={Ullah, Amin and Ahmad, Jamil and Muhammad, Khan and Sajjad, Muhammad and Baik, Sung Wook},
  journal={IEEE access},
  year={2017}
}

@article{varol2017long,
  title={Long-term temporal convolutions for action recognition},
  author={Varol, G{\"u}l and Laptev, Ivan and Schmid, Cordelia},
  journal={IEEE TPAMI},
  year={2017}
}

@article{du2017recurrent,
  title={Recurrent spatial-temporal attention network for action recognition in videos},
  author={Du, Wenbin and Wang, Yali and Qiao, Yu},
  journal={IEEE T-IP},
  year={2017}
}

@inproceedings{zhang2016real,
  title={Real-time action recognition with enhanced motion vector CNNs},
  author={Zhang, Bowen and Wang, Limin and Wang, Zhe and Qiao, Yu and Wang, Hanli},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{zhu2016key,
  title={A key volume mining deep framework for action recognition},
  author={Zhu, Wangjiang and Hu, Jie and Sun, Gang and Cao, Xudong and Qiao, Yu},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{zhou2018temporal,
  title={Temporal relational reasoning in videos},
  author={Zhou, Bolei and Andonian, Alex and Oliva, Aude and Torralba, Antonio},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{chen2021watch,
  title={Watch only once: An end-to-end video action detection framework},
  author={Chen, Shoufa and Sun, Peize and Xie, Enze and Ge, Chongjian and Wu, Jiannan and Ma, Lan and Shen, Jiajun and Luo, Ping},
  booktitle={ICCV},
  year={2021}
}


@inproceedings{liu2016ssd,
  title={Ssd: Single shot multibox detector},
  author={Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C},
  booktitle={ECCV},
  year={2016}
}

@article{gulati2020conformer,
  title={Conformer: Convolution-augmented Transformer for Speech Recognition},
  author={Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and others},
  journal={Interspeech},
  year={2020},
}

@inproceedings{liu2022learning_the,
  title={Learning the spectrogram temporal resolution for audio classification},
  author={Liu, Haohe and Liu, Xubo and Kong, Qiuqiang and Wang, Wenwu and Plumbley, Mark D},
  booktitle={AAAI},
  year={2022}
}

@article{huang2022masked,
  title={Masked autoencoders that listen},
  author={Huang, Po-Yao and Xu, Hu and Li, Juncheng and Baevski, Alexei and Auli, Michael and Galuba, Wojciech and Metze, Florian and Feichtenhofer, Christoph},
  journal={NeurIPS},
  year={2022}
}

@article{morrongiello1998developmental,
  title={Developmental changes in associations between auditory-visual events},
  author={Morrongiello, Barbara A and Fenwick, Kimberley D and Nutley, Tanya},
  journal={Infant Behavior and Development},
  year={1998}
}

@article{chen1998audio,
  title={Audio-visual integration in multimodal communication},
  author={Chen, Tsuhan and Rao, Ram R},
  journal={Proceedings of the IEEE},
  year={1998}
}

@article{matthews2002extraction,
  title={Extraction of visual features for lipreading},
  author={Matthews, Iain and Cootes, Timothy F and Bangham, J Andrew and Cox, Stephen and Harvey, Richard},
  journal={IEEE TPAMI},
  year={2002}
}

@article{aleksic2006audio,
  title={Audio-visual biometrics},
  author={Aleksic, Petar S and Katsaggelos, Aggelos K},
  journal={Proceedings of the IEEE},
  year={2006}
}

@inproceedings{feng2024coarse,
  title={From Coarse to Fine: Efficient Training for Audio Spectrogram Transformers},
  author={Feng, Jiu and Erol, Mehmet Hamza and Chung, Joon Son and Senocak, Arda},
  booktitle={ICASSP},
  year={2024}
}

@inproceedings{redmon2016you,
  title={You only look once: Unified, real-time object detection},
  author={Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  booktitle={CVPR},
  year={2016}
}


@inproceedings{sevilla2019integration,
  title={On the integration of optical flow and action recognition},
  author={Sevilla-Lara, Laura and Liao, Yiyi and G{\"u}ney, Fatma and Jampani, Varun and Geiger, Andreas and Black, Michael J},
  booktitle={GCPR},
  year={2019}
}

@inproceedings{hoai2015improving,
  title={Improving human action recognition using score distribution and ranking},
  author={Hoai, Minh and Zisserman, Andrew},
  booktitle={ACCV},
  year={2015}
}

@article{girdhar2017attentional,
  title={Attentional pooling for action recognition},
  author={Girdhar, Rohit and Ramanan, Deva},
  journal={NeurIPS},
  year={2017}
}

@article{zong2021motion,
  title={Motion saliency based multi-stream multiplier ResNets for action recognition},
  author={Zong, Ming and Wang, Ruili and Chen, Xiubo and Chen, Zhe and Gong, Yuanhao},
  journal={IVC},
  year={2021}
}

@inproceedings{chung2016signs,
  title={Signs in time: Encoding human motion as a temporal image},
  author={Chung, J and Zisserman, A},
  booktitle={ECCVw},
  year={2016}
}

@inproceedings{tran2018closer,
  title={A closer look at spatiotemporal convolutions for action recognition},
  author={Tran, Du and Wang, Heng and Torresani, Lorenzo and Ray, Jamie and LeCun, Yann and Paluri, Manohar},
  booktitle={CVPR},
  year={2018}
}

@article{chen20182,
  title={A\^{} 2-nets: Double attention networks},
  author={Chen, Yunpeng and Kalantidis, Yannis and Li, Jianshu and Yan, Shuicheng and Feng, Jiashi},
  journal={NeurIPS},
  year={2018}
}

@inproceedings{chen2018multi,
  title={Multi-fiber networks for video recognition},
  author={Chen, Yunpeng and Kalantidis, Yannis and Li, Jianshu and Yan, Shuicheng and Feng, Jiashi},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{hegde2018morph,
  title={Morph: Flexible acceleration for 3d cnn-based video understanding},
  author={Hegde, Kartik and Agrawal, Rohit and Yao, Yulun and Fletcher, Christopher W},
  booktitle={MICRO},
  year={2018}
}

@inproceedings{dwibedi2018temporal,
  title={Temporal reasoning in videos using convolutional gated recurrent units},
  author={Dwibedi, Debidatta and Sermanet, Pierre and Tompson, Jonathan},
  booktitle={CVPRw},
  year={2018}
}

@inproceedings{jiang2019stm,
  title={Stm: Spatiotemporal and motion encoding for action recognition},
  author={Jiang, Boyuan and Wang, MengMeng and Gan, Weihao and Wu, Wei and Yan, Junjie},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{tran2019video,
  title={Video classification with channel-separated convolutional networks},
  author={Tran, Du and Wang, Heng and Torresani, Lorenzo and Feiszli, Matt},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{xu2019self,
  title={Self-supervised spatiotemporal learning via video clip order prediction},
  author={Xu, Dejing and Xiao, Jun and Zhao, Zhou and Shao, Jian and Xie, Di and Zhuang, Yueting},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{qiu2017learning,
  title={Learning spatio-temporal representation with pseudo-3d residual networks},
  author={Qiu, Zhaofan and Yao, Ting and Mei, Tao},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{wang2018appearance,
  title={Appearance-and-relation networks for video classification},
  author={Wang, Limin and Li, Wei and Li, Wen and Van Gool, Luc},
  booktitle={CVPR},
  year={2018}
}

@techreport{deguide2008guide,
  author = {Fernando De la Torre Frade and Jessica K. Hodgins and Adam W. Bargteil and Xavier Martin Artal and Justin C. Macey and Alexandre Collado I Castells and Josep Beltran},
  title = {Guide to the Carnegie Mellon University Multimodal Activity (CMU-MMAC) Database},
  year = {2008},
  institution={CMU}
}

@inproceedings{ye2023unified,
  title={A unified model for continuous conditional video prediction},
  author={Ye, Xi and Bilodeau, Guillaume-Alexandre},
  booktitle={CVPRw},
  year={2023}
}


@inproceedings{jhuang2013towards,
  title = {Towards understanding action recognition},
  author = {H. Jhuang and J. Gall and S. Zuffi and C. Schmid and M. J. Black},
  booktitle = {ICCV},
  year = {2013}
}

@inproceedings{ibrahim2016hierarchical,
  title={A hierarchical deep temporal model for group activity recognition},
  author={Ibrahim, Mostafa S and Muralidharan, Srikanth and Deng, Zhiwei and Vahdat, Arash and Mori, Greg},
  booktitle={CVPR},
  year={2016}
}

@article{sigurdsson2018charades,
  title={Charades-ego: A large-scale dataset of paired third and first person videos},
  author={Sigurdsson, Gunnar A and Gupta, Abhinav and Schmid, Cordelia and Farhadi, Ali and Alahari, Karteek},
  journal={arxiv},
  year={2018}
}

@article{dai2022toyota,
  title={Toyota smarthome untrimmed: Real-world untrimmed videos for activity detection},
  author={Dai, Rui and Das, Srijan and Sharma, Saurav and Minciullo, Luca and Garattoni, Lorenzo and Bremond, Francois and Francesca, Gianpiero},
  journal={IEEE TPAMI},
  year={2022},
}

@article{miech2020rareact,
  title={Rareact: A video dataset of unusual interactions},
  author={Miech, Antoine and Alayrac, Jean-Baptiste and Laptev, Ivan and Sivic, Josef and Zisserman, Andrew},
  journal={arxiv},
  year={2020}
}

@inproceedings{rai2021home,
  title={Home action genome: Cooperative compositional action understanding},
  author={Rai, Nishant and Chen, Haofeng and Ji, Jingwei and Desai, Rishi and Kozuka, Kazuki and Ishizaka, Shun and Adeli, Ehsan and Niebles, Juan Carlos},
  booktitle={CVPR},
  year={2021}
}

@article{liu2022fineaction,
  title={Fineaction: A fine-grained video dataset for temporal action localization},
  author={Liu, Yi and Wang, Limin and Wang, Yali and Ma, Xiao and Qiao, Yu},
  journal={IEEE T-IP},
  year={2022}
}

@inproceedings{doughty2022you,
  title={How do you do it? fine-grained action understanding with pseudo-adverbs},
  author={Doughty, Hazel and Snoek, Cees GM},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{pirsiavash2012detecting,
  title={Detecting activities of daily living in first-person camera views},
  author={Pirsiavash, Hamed and Ramanan, Deva},
  booktitle={CVPR},
  year={2012},
}

@inproceedings{mueller2017real,
  title={Real-time hand tracking under occlusion from an egocentric rgb-d sensor},
  author={Mueller, Franziska and Mehta, Dushyant and Sotnychenko, Oleksandr and Sridhar, Srinath and Casas, Dan and Theobalt, Christian},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{garcia2018first,
  title={First-person hand action benchmark with rgb-d videos and 3d hand pose annotations},
  author={Garcia-Hernando, Guillermo and Yuan, Shanxin and Baek, Seungryul and Kim, Tae-Kyun},
  booktitle={CVPR},
  year={2018}
}
@inproceedings{chao2021dexycb,
  title={DexYCB: A benchmark for capturing hand grasping of objects},
  author={Chao, Yu-Wei and Yang, Wei and Xiang, Yu and Molchanov, Pavlo and Handa, Ankur and Tremblay, Jonathan and Narang, Yashraj S and Van Wyk, Karl and Iqbal, Umar and Birchfield, Stan and others},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{hampali2020honnotate,
  title={Honnotate: A method for 3d annotation of hand and object poses},
  author={Hampali, Shreyas and Rad, Mahdi and Oberweger, Markus and Lepetit, Vincent},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{moon2020interhand2,
  title={Interhand2. 6m: A dataset and baseline for 3d interacting hand pose estimation from a single rgb image},
  author={Moon, Gyeongsik and Yu, Shoou-I and Wen, He and Shiratori, Takaaki and Lee, Kyoung Mu},
  booktitle={ECCV},
  year={2020},
}

@inproceedings{ohkawa2023assemblyhands,
    title     = {{AssemblyHands:} Towards Egocentric Activity Understanding via 3D Hand Pose Estimation},
    author    = {Takehiko Ohkawa and Kun He and Fadime Sener and Tomas Hodan and Luan Tran and Cem Keskin},
    booktitle = {CVPR},
    year      = {2023},
}

@inproceedings{grauman2024ego,
  title={Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives},
  author={Grauman, Kristen and Westbury, Andrew and Torresani, Lorenzo and Kitani, Kris and Malik, Jitendra and Afouras, Triantafyllos and Ashutosh, Kumar and Baiyya, Vijay and Bansal, Siddhant and Boote, Bikram and others},
  booktitle={CVPR},
  year={2024}
}
    

@inproceedings{hara2018can,
  title={Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?},
  author={Hara, Kensho and Kataoka, Hirokatsu and Satoh, Yutaka},
  booktitle={CVPR},
  year={2018}
}

@article{luo2021moma,
  title={Moma: Multi-object multi-actor activity parsing},
  author={Luo, Zelun and Xie, Wanze and Kapoor, Siddharth and Liang, Yiyun and Cooper, Michael and Niebles, Juan Carlos and Adeli, Ehsan and Li, Fei-Fei},
  journal={NeurIPS},
  year={2021}
}

@inproceedings{luo2019grouped,
  title={Grouped spatial-temporal aggregation for efficient action recognition},
  author={Luo, Chenxu and Yuille, Alan L},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{qiu2019learning,
  title={Learning spatio-temporal representation with local and global diffusion},
  author={Qiu, Zhaofan and Yao, Ting and Ngo, Chong-Wah and Tian, Xinmei and Mei, Tao},
  booktitle={CVPR},
  year={2019}
}

@article{choi2019can,
  title={Why can't I dance in the mall? learning to mitigate scene bias in action recognition},
  author={Choi, Jinwoo and Gao, Chen and Messou, Joseph CE and Huang, Jia-Bin},
  journal={NeurIPS},
  year={2019}
}

@inproceedings{suris2021learning,
  title={Learning the predictability of the future},
  author={Sur{\'\i}s, D{\'\i}dac and Liu, Ruoshi and Vondrick, Carl},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{liang2022visual,
  title={Visual abductive reasoning},
  author={Liang, Chen and Wang, Wenguan and Zhou, Tianfei and Yang, Yi},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{li2023intentqa,
  title={Intentqa: Context-aware video intent reasoning},
  author={Li, Jiapeng and Wei, Ping and Han, Wenjuan and Fan, Lifeng},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{pei2011parsing,
  title={Parsing video events with goal inference and intent prediction},
  author={Pei, Mingtao and Jia, Yunde and Zhu, Song-Chun},
  booktitle={ICCV},
  year={2011}
}

@inproceedings{wang2021enhancing,
  title={Enhancing unsupervised video representation learning by decoupling the scene and the motion},
  author={Wang, Jinpeng and Gao, Yuting and Li, Ke and Hu, Jianguo and Jiang, Xinyang and Guo, Xiaowei and Ji, Rongrong and Sun, Xing},
  booktitle={AAAI},
  year={2021}
}

@inproceedings{lin2019tsm,
  title={Tsm: Temporal shift module for efficient video understanding},
  author={Lin, Ji and Gan, Chuang and Han, Song},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{feichtenhofer2019slowfast,
  title={Slowfast networks for video recognition},
  author={Feichtenhofer, Christoph and Fan, Haoqi and Malik, Jitendra and He, Kaiming},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{feichtenhofer2020x3d,
  title={X3d: Expanding architectures for efficient video recognition},
  author={Feichtenhofer, Christoph},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{kondratyuk2021movinets,
  title={Movinets: Mobile video networks for efficient video recognition},
  author={Kondratyuk, Dan and Yuan, Liangzhe and Li, Yandong and Zhang, Li and Tan, Mingxing and Brown, Matthew and Gong, Boqing},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{liu2022convnet,
  title={A convnet for the 2020s},
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{hussein2019timeception,
  title={Timeception for complex action recognition},
  author={Hussein, Noureldien and Gavves, Efstratios and Smeulders, Arnold WM},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{wang2020self,
  title={Self-supervised video representation learning by pace prediction},
  author={Wang, Jiangliu and Jiao, Jianbo and Liu, Yun-Hui},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{ji2020action,
  title={Action genome: Actions as compositions of spatio-temporal scene graphs},
  author={Ji, Jingwei and Krishna, Ranjay and Fei-Fei, Li and Niebles, Juan Carlos},
  booktitle={CVPR},
  year={2020}
}

@article{stergiou2021learn,
  title={Learn to cycle: Time-consistent feature discovery for action recognition},
  author={Stergiou, Alexandros and Poppe, Ronald},
  journal={PRL},
  year={2021}
}

@inproceedings{wang2018non,
  title={Non-local neural networks},
  author={Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{carreira2017quo,
  title={Quo vadis, action recognition? a new model and the kinetics dataset},
  author={Carreira, Joao and Zisserman, Andrew},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{zhou2018mict,
  title={Mict: Mixed 3d/2d convolutional tube for human action recognition},
  author={Zhou, Yizhou and Sun, Xiaoyan and Zha, Zheng-Jun and Zeng, Wenjun},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{feichtenhofer2016convolutional,
  title={Convolutional two-stream network fusion for video action recognition},
  author={Feichtenhofer, Christoph and Pinz, Axel and Zisserman, Andrew},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{wang2016temporal,
  title={Temporal segment networks: Towards good practices for deep action recognition},
  author={Wang, Limin and Xiong, Yuanjun and Wang, Zhe and Qiao, Yu and Lin, Dahua and Tang, Xiaoou and Van Gool, Luc},
  booktitle={ECCV},
  year={2016}
}

@inproceedings{wang2017spatiotemporal,
  title={Spatiotemporal pyramid network for video action recognition},
  author={Wang, Yunbo and Long, Mingsheng and Wang, Jianmin and Yu, Philip S},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{yang2020temporal,
  title={Temporal pyramid network for action recognition},
  author={Yang, Ceyuan and Xu, Yinghao and Shi, Jianping and Dai, Bo and Zhou, Bolei},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{srivastava2015unsupervised,
  title={Unsupervised learning of video representations using lstms},
  author={Srivastava, Nitish and Mansimov, Elman and Salakhudinov, Ruslan},
  booktitle={ICML},
  year={2015}
}

@inproceedings{donahue2015long,
  title={Long-term recurrent convolutional networks for visual recognition and description},
  author={Donahue, Jeffrey and Anne Hendricks, Lisa and Guadarrama, Sergio and Rohrbach, Marcus and Venugopalan, Subhashini and Saenko, Kate and Darrell, Trevor},
  booktitle={CVPR},
  year={2015}
}

@inproceedings{kwon2021h2o,
  title={H2o: Two hands manipulating objects for first person interaction recognition},
  author={Kwon, Taein and Tekin, Bugra and St{\"u}hmer, Jan and Bogo, Federica and Pollefeys, Marc},
  booktitle={ICCV},
  year={2021}
}


@inproceedings{singh2016multi,
  title={A multi-stream bi-directional recurrent neural network for fine-grained action detection},
  author={Singh, Bharat and Marks, Tim K and Jones, Michael and Tuzel, Oncel and Shao, Ming},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{wang2017recurrent,
  title={Recurrent modeling of interaction context for collective activity recognition},
  author={Wang, Minsi and Ni, Bingbing and Yang, Xiaokang},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{girdhar2019video,
  title={Video action transformer network},
  author={Girdhar, Rohit and Carreira, Joao and Doersch, Carl and Zisserman, Andrew},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{kalogeiton2017action,
  title={Action tubelet detector for spatio-temporal action localization},
  author={Kalogeiton, Vicky and Weinzaepfel, Philippe and Ferrari, Vittorio and Schmid, Cordelia},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{faure2023holistic,
  title={Holistic interaction transformer network for action detection},
  author={Faure, Gueter Josmy and Chen, Min-Hung and Lai, Shang-Hong},
  booktitle={WACV},
  year={2023}
}

@inproceedings{nguyen2024hig,
  title={Hig: Hierarchical interlacement graph approach to scene graph generation in video understanding},
  author={Nguyen, Trong-Thuan and Nguyen, Pha and Luu, Khoa},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{chen2023efficient,
  title={Efficient video action detection with token dropout and context refinement},
  author={Chen, Lei and Tong, Zhan and Song, Yibing and Wu, Gangshan and Wang, Limin},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{li2021multisports,
  title={Multisports: A multi-person video dataset of spatio-temporally localized sports actions},
  author={Li, Yixuan and Chen, Lei and He, Runyu and Wang, Zhenzhi and Wu, Gangshan and Wang, Limin},
  booktitle={ICCV},
  year={2021}
}


@inproceedings{bacharidis2023repetition,
  title={{Repetition-aware Image Sequence Sampling for Recognizing Repetitive Human Actions}},
  author={Bacharidis, Konstantinos and Argyros, Antonis},
  booktitle={ICCVw},
  year={2023}
}

@inproceedings{zhang2021repetitive,
  title={{Repetitive Activity Counting by Sight and Sound}},
  author={Zhang, Yunhua and Shao, Ling and Snoek, Cees GM},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{destro2024cyclecl,
  title={{CycleCL: Self-supervised Learning for Periodic Videos}},
  author={Destro, Matteo and Gygli, Michael},
  booktitle={WACV},
  year={2024}
}

@inproceedings{li2024repetitive,
  title={{Repetitive Action Counting With Motion Feature Learning}},
  author={Li, Xinjie and Xu, Huijuan},
  booktitle={WACV},
  year={2024}
}


@inproceedings{ousman2008segmentation,
  title={{Segmentation of Periodically Moving Objects}},
  author={Azy, Ousman and Ahuja, Narendra},
  booktitle={ICPR},
  year={2008}
}
@article{briassouli2007extraction,
  title={{Extraction and Analysis of Multiple Periodic Motions in Video Sequences}},
  author={Briassouli, Alexia and Ahuja, Narendra},
  journal={IEEE TPAMI)},
  year={2007},
}
@article{ross2000robust,
  title={{Robust Real-Time Periodic Motion Detection, Analysis, and Applications}},
  author={Cutler, Ross and Davis, Larry S.},
  journal={IEEE TPAMI},
  year={2000}
}
@inproceedings{pogalin2008visual,
  title={{Visual Quasi-Periodicity}},
  author={Pogalin, Erik and Smeulders, Arnold WM and Thean, Andrew HC},
  booktitle={CVPR},
  year={2008}
}
@article{branzan2008generic,
  title={{Generic Temporal Segmentation of Cyclic Human Motion}},
  author={Albu, A Branzan and Bergevin, Robert and Quirion, S{\'e}bastien},
  journal={PR   },
  year={2008},
}
@inproceedings{fourier6_periodic,
  title={{Periodic Motion Detection and Segmentation via Approximate Sequence Alignment}},
  author={Laptev, Ivan and Belongie, Serge J and P{\'e}rez, Patrick and Wills, Josh},
  booktitle={ICCV},
  year={2005}
}


@inproceedings{kapidis2019multitask,
  title={Multitask learning to improve egocentric action recognition},
  author={Kapidis, Georgios and Poppe, Ronald and Van Dam, Elsbeth and Noldus, Lucas and Veltkamp, Remco},
  booktitle={CVPRw},
  year={2019}
}

@inproceedings{wu2019long,
  title={Long-term feature banks for detailed video understanding},
  author={Wu, Chao-Yuan and Feichtenhofer, Christoph and Fan, Haoqi and He, Kaiming and Krahenbuhl, Philipp and Girshick, Ross},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{chao2018rethinking,
  title={Rethinking the faster r-cnn architecture for temporal action localization},
  author={Chao, Yu-Wei and Vijayanarasimhan, Sudheendra and Seybold, Bryan and Ross, David A and Deng, Jia and Sukthankar, Rahul},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{huang2019decoupling,
  title={Decoupling localization and classification in single shot temporal action detection},
  author={Huang, Yupan and Dai, Qi and Lu, Yutong},
  booktitle={ICME},
  year={2019},
}

@inproceedings{alwassel2021tsp,
  title={Tsp: Temporally-sensitive pretraining of video encoders for localization tasks},
  author={Alwassel, Humam and Giancola, Silvio and Ghanem, Bernard},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{zhang2022actionformer,
  title={Actionformer: Localizing moments of actions with transformers},
  author={Zhang, Chen-Lin and Wu, Jianxin and Li, Yin},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{shou2016temporal,
  title={Temporal action localization in untrimmed videos via multi-stage cnns},
  author={Shou, Zheng and Wang, Dongang and Chang, Shih-Fu},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{wang2013action,
  title={Action recognition with improved trajectories},
  author={Wang, Heng and Schmid, Cordelia},
  booktitle={ICCV},
  year={2013}
}

@inproceedings{oneata2013action,
  title={Action and event recognition with fisher vectors on a compact feature set},
  author={Oneata, Dan and Verbeek, Jakob and Schmid, Cordelia},
  booktitle={ICCV},
  year={2013}
}

@inproceedings{wang2017untrimmednets,
  title={Untrimmednets for weakly supervised action recognition and detection},
  author={Wang, Limin and Xiong, Yuanjun and Lin, Dahua and Van Gool, Luc},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{zeng2019graph,
  title={Graph convolutional networks for temporal action localization},
  author={Zeng, Runhao and Huang, Wenbing and Tan, Mingkui and Rong, Yu and Zhao, Peilin and Huang, Junzhou and Gan, Chuang},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{paul2018w,
  title={W-talc: Weakly-supervised temporal activity localization and classification},
  author={Paul, Sujoy and Roy, Sourya and Roy-Chowdhury, Amit K},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{luo2020weakly,
  title={Weakly-supervised action localization with expectation-maximization multi-instance learning},
  author={Luo, Zhekun and Guillory, Devin and Shi, Baifeng and Ke, Wei and Wan, Fang and Darrell, Trevor and Xu, Huijuan},
  booktitle={ECCV},
  year={2020}
}


@inproceedings{rizve2023pivotal,
  title={Pivotal: Prior-driven supervision for weakly-supervised temporal action localization},
  author={Rizve, Mamshad Nayeem and Mittal, Gaurav and Yu, Ye and Hall, Matthew and Sajeev, Sandra and Shah, Mubarak and Chen, Mei},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{shou2017cdc,
  title={Cdc: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos},
  author={Shou, Zheng and Chan, Jonathan and Zareian, Alireza and Miyazawa, Kazuyuki and Chang, Shih-Fu},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{xu2017r,
  title={R-c3d: Region convolutional 3d network for temporal activity detection},
  author={Xu, Huijuan and Das, Abir and Saenko, Kate},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{lin2018bsn,
  title={Bsn: Boundary sensitive network for temporal action proposal generation},
  author={Lin, Tianwei and Zhao, Xu and Su, Haisheng and Wang, Chongjing and Yang, Ming},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{lin2021learning,
  title={Learning salient boundary feature for anchor-free temporal action localization},
  author={Lin, Chuming and Xu, Chengming and Luo, Donghao and Wang, Yabiao and Tai, Ying and Wang, Chengjie and Li, Jilin and Huang, Feiyue and Fu, Yanwei},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{cheng2022tallformer,
  title={Tallformer: Temporal action localization with a long-memory transformer},
  author={Cheng, Feng and Bertasius, Gedas},
  booktitle={ECCV},
  year={2022},
}

@inproceedings{liu2020progressive,
  title={Progressive boundary refinement network for temporal action detection},
  author={Liu, Qinying and Wang, Zilei},
  booktitle={AAAI},
  year={2020}
}

@inproceedings{liu2021weakly,
  title={Weakly supervised temporal action localization through learning explicit subspaces for action and context},
  author={Liu, Ziyi and Wang, Le and Tang, Wei and Yuan, Junsong and Zheng, Nanning and Hua, Gang},
  booktitle={AAAI},
  year={2021}
}

@inproceedings{shi2023tridet,
  title={Tridet: Temporal action detection with relative boundary modeling},
  author={Shi, Dingfeng and Zhong, Yujie and Cao, Qiong and Ma, Lin and Li, Jia and Tao, Dacheng},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{shou2018autoloc,
  title={Autoloc: Weakly-supervised temporal action localization in untrimmed videos},
  author={Shou, Zheng and Gao, Hang and Zhang, Lei and Miyazawa, Kazuyuki and Chang, Shih-Fu},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{mettes2016spot,
  title={Spot on: Action localization from pointly-supervised proposals},
  author={Mettes, Pascal and Van Gemert, Jan C and Snoek, Cees GM},
  booktitle={ECCV},
  year={2016},
}

@inproceedings{liu2024end,
  title={End-to-end temporal action detection with 1b parameters across 1000 frames},
  author={Liu, Shuming and Zhang, Chen-Lin and Zhao, Chen and Ghanem, Bernard},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{zhao2023re2tal,
  title={Re2TAL: Rewiring pretrained video backbones for reversible temporal action localization},
  author={Zhao, Chen and Liu, Shuming and Mangalam, Karttikeya and Ghanem, Bernard},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{shao2023action,
  title={Action sensitivity learning for temporal action localization},
  author={Shao, Jiayi and Wang, Xiaohan and Quan, Ruijie and Zheng, Junjun and Yang, Jiang and Yang, Yi},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{zhu2024dual,
  title={Dual DETRs for Multi-Label Temporal Action Detection},
  author={Zhu, Yuhan and Zhang, Guozhen and Tan, Jing and Wu, Gangshan and Wang, Limin},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{zhang2021cola,
  title={Cola: Weakly-supervised temporal action localization with snippet contrastive learning},
  author={Zhang, Can and Cao, Meng and Yang, Dongming and Chen, Jie and Zou, Yuexian},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{chen2022dcan,
  title={Dcan: improving temporal action detection via dual context aggregation},
  author={Chen, Guo and Zheng, Yin-Dong and Wang, Limin and Lu, Tong},
  booktitle={AAAI},
  year={2022}
}

@inproceedings{zhao2021video,
  title={Video self-stitching graph network for temporal action localization},
  author={Zhao, Chen and Thabet, Ali K and Ghanem, Bernard},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{zhai2020two,
  title={Two-stream consensus network for weakly-supervised temporal action localization},
  author={Zhai, Yuanhao and Wang, Le and Tang, Wei and Zhang, Qilin and Yuan, Junsong and Hua, Gang},
  booktitle={ECCV},
  year={2020},
}

@inproceedings{he2022asm,
  title={Asm-loc: Action-aware segment modeling for weakly-supervised temporal action localization},
  author={He, Bo and Yang, Xitong and Kang, Le and Cheng, Zhiyu and Zhou, Xin and Shrivastava, Abhinav},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{nag2023difftad,
  title={Difftad: Temporal action detection with proposal denoising diffusion},
  author={Nag, Sauradip and Zhu, Xiatian and Deng, Jiankang and Song, Yi-Zhe and Xiang, Tao},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{qing2021temporal,
  title={Temporal context aggregation network for temporal action proposal refinement},
  author={Qing, Zhiwu and Su, Haisheng and Gan, Weihao and Wang, Dongliang and Wu, Wei and Wang, Xiang and Qiao, Yu and Yan, Junjie and Gao, Changxin and Sang, Nong},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{zhang2022unsupervised,
  title={Unsupervised pre-training for temporal action localization tasks},
  author={Zhang, Can and Yang, Tianyu and Weng, Junwu and Cao, Meng and Wang, Jue and Zou, Yuexian},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{hong2022spotting,
  title={Spotting temporally precise, fine-grained events in video},
  author={Hong, James and Zhang, Haotian and Gharbi, Micha{\"e}l and Fisher, Matthew and Fatahalian, Kayvon},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{ju2023distilling,
  title={Distilling vision-language pre-training to collaborate with weakly-supervised temporal action localization},
  author={Ju, Chen and Zheng, Kunhao and Liu, Jinxiang and Zhao, Peisen and Zhang, Ya and Chang, Jianlong and Tian, Qi and Wang, Yanfeng},
  booktitle={CVPR},
  year={2023}
}


@inproceedings{carion2020end,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={ECCV},
  year={2020}
}

@article{wang2023temporal,
  title={Temporal action localization in the deep learning era: A survey},
  author={Wang, Binglu and Zhao, Yongqiang and Yang, Le and Long, Teng and Li, Xuelong},
  journal={IEEE TPAMI},
  year={2023}
}


@inproceedings{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{arnab2021vivit,
  title={Vivit: A video vision transformer},
  author={Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu{\v{c}}i{\'c}, Mario and Schmid, Cordelia},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{bertasius2021space,
  title={Is space-time attention all you need for video understanding?},
  author={Bertasius, Gedas and Wang, Heng and Torresani, Lorenzo},
  booktitle={ICML},
  year={2021}
}

@article{patrick2021keeping,
  title={Keeping your eye on the ball: Trajectory attention in video transformers},
  author={Patrick, Mandela and Campbell, Dylan and Asano, Yuki and Misra, Ishan and Metze, Florian and Feichtenhofer, Christoph and Vedaldi, Andrea and Henriques, Joao F},
  journal={NeurIPS},
  year={2021}
}

@inproceedings{fan2021multiscale,
  title={Multiscale vision transformers},
  author={Fan, Haoqi and Xiong, Bo and Mangalam, Karttikeya and Li, Yanghao and Yan, Zhicheng and Malik, Jitendra and Feichtenhofer, Christoph},
  booktitle={ICCV},
  year={2021}
}

@article{ryoo2021tokenlearner,
  title={Tokenlearner: Adaptive space-time tokenization for videos},
  author={Ryoo, Michael and Piergiovanni, AJ and Arnab, Anurag and Dehghani, Mostafa and Angelova, Anelia},
  journal={NeurIPS},
  year={2021}
}

@article{bulat2021space,
  title={Space-time mixing attention for video transformer},
  author={Bulat, Adrian and Perez Rua, Juan Manuel and Sudhakaran, Swathikiran and Martinez, Brais and Tzimiropoulos, Georgios},
  journal={NeurIPS},
  year={2021}
}

@inproceedings{li2021groupformer,
  title={Groupformer: Group activity recognition with clustered spatial-temporal transformer},
  author={Li, Shuaicheng and Cao, Qianggang and Liu, Lingbo and Yang, Kunlin and Liu, Shinan and Hou, Jun and Yi, Shuai},
  booktitle={ICCV},
  year={2021}
}

@article{kim2021relational,
  title={Relational self-attention: What's missing in attention for video understanding},
  author={Kim, Manjin and Kwon, Heeseung and Wang, Chunyu and Kwak, Suha and Cho, Minsu},
  journal={NeurIPS},
  year={2021}
}

@inproceedings{liu2022video,
  title={Video swin transformer},
  author={Liu, Ze and Ning, Jia and Cao, Yue and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Hu, Han},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{li2022mvitv2,
  title={Mvitv2: Improved multiscale vision transformers for classification and detection},
  author={Li, Yanghao and Wu, Chao-Yuan and Fan, Haoqi and Mangalam, Karttikeya and Xiong, Bo and Malik, Jitendra and Feichtenhofer, Christoph},
  booktitle={CVPR},
  year={2022}
}

@article{zha2021shifted,
  title={Shifted chunk transformer for spatio-temporal representational learning},
  author={Zha, Xuefan and Zhu, Wentao and Xun, Lv and Yang, Sen and Liu, Ji},
  journal={NeurIPS},
  year={2021}
}

@inproceedings{wu2022memvit,
  title={Memvit: Memory-augmented multiscale vision transformer for efficient long-term video recognition},
  author={Wu, Chao-Yuan and Li, Yanghao and Mangalam, Karttikeya and Fan, Haoqi and Xiong, Bo and Malik, Jitendra and Feichtenhofer, Christoph},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{yan2022multiview,
  title={Multiview transformers for video recognition},
  author={Yan, Shen and Xiong, Xuehan and Arnab, Anurag and Lu, Zhichao and Zhang, Mi and Sun, Chen and Schmid, Cordelia},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{ryali2023hiera,
  title={Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles},
  author={Ryali, Chaitanya and Hu, Yuan-Ting and Bolya, Daniel and Wei, Chen and Fan, Haoqi and Huang, Po-Yao and Aggarwal, Vaibhav and Chowdhury, Arkabandhu and Poursaeed, Omid and Hoffman, Judy and others},
  booktitle={ICML},
  year={2023}
}

@inproceedings{li2022uniformer,
  title={UniFormer: Unified Transformer for Efficient Spatial-Temporal Representation Learning},
  author={Li, Kunchang and Wang, Yali and Peng, Gao and Song, Guanglu and Liu, Yu and Li, Hongsheng and Qiao, Yu},
  booktitle={ICLR},
  year={2022}
}

@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={ICML},
  year={2020}
}

@inproceedings{sudhakaran2020gate,
  title={Gate-shift networks for video action recognition},
  author={Sudhakaran, Swathikiran and Escalera, Sergio and Lanz, Oswald},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={CVPR},
  year={2022}
}

@article{feichtenhofer2022masked,
  title={Masked autoencoders as spatiotemporal learners},
  author={Feichtenhofer, Christoph and Li, Yanghao and He, Kaiming and others},
  journal={NeurIPS},
  year={2022}
}

@inproceedings{bandara2023adamae,
  title={AdaMAE: Adaptive Masking for Efficient Spatiotemporal Learning with Masked Autoencoders},
  author={Bandara, Wele Gedara Chaminda and Patel, Naman and Gholami, Ali and Nikkhah, Mehdi and Agrawal, Motilal and Patel, Vishal M},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{huang2023mgmae,
  title={Mgmae: Motion guided masking for video masked autoencoding},
  author={Huang, Bingkun and Zhao, Zhiyu and Zhang, Guozhen and Qiao, Yu and Wang, Limin},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{kim2024token,
  title={Token Fusion: Bridging the Gap between Token Pruning and Token Merging},
  author={Kim, Minchul and Gao, Shangqian and Hsu, Yen-Chang and Shen, Yilin and Jin, Hongxia},
  booktitle={WACV},
  year={2024}
}

@inproceedings{wang2023videomae,
  title={Videomae v2: Scaling video masked autoencoders with dual masking},
  author={Wang, Limin and Huang, Bingkun and Zhao, Zhiyu and Tong, Zhan and He, Yinan and Wang, Yi and Wang, Yali and Qiao, Yu},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{girshick2015fast,
  title={Fast r-cnn},
  author={Girshick, Ross},
  booktitle={ICCV},
  year={2015}
}

@inproceedings{girshick2014rich,
  title={Rich feature hierarchies for accurate object detection and semantic segmentation},
  author={Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  booktitle={CVPR},
  year={2014}
}

@inproceedings{soomro2015action,
  title={Action localization in videos through context walk},
  author={Soomro, Khurram and Idrees, Haroon and Shah, Mubarak},
  booktitle={ICCV},
  year={2015}
}

@inproceedings{arnab2021unified,
  title={Unified graph structured models for video understanding},
  author={Arnab, Anurag and Sun, Chen and Schmid, Cordelia},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{gritsenko2024end,
  title={End-to-end spatio-temporal action localisation with video transformers},
  author={Gritsenko, Alexey A and Xiong, Xuehan and Djolonga, Josip and Dehghani, Mostafa and Sun, Chen and Lucic, Mario and Schmid, Cordelia and Arnab, Anurag},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{xing2023svformer,
  title={Svformer: Semi-supervised video transformer for action recognition},
  author={Xing, Zhen and Dai, Qi and Hu, Han and Chen, Jingjing and Wu, Zuxuan and Jiang, Yu-Gang},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{wei2022masked,
  title={Masked feature prediction for self-supervised visual pre-training},
  author={Wei, Chen and Fan, Haoqi and Xie, Saining and Wu, Chao-Yuan and Yuille, Alan and Feichtenhofer, Christoph},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{wang2023masked,
  title={Masked video distillation: Rethinking masked feature modeling for self-supervised video representation learning},
  author={Wang, Rui and Chen, Dongdong and Wu, Zuxuan and Chen, Yinpeng and Dai, Xiyang and Liu, Mengchen and Yuan, Lu and Jiang, Yu-Gang},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{thurau2008pose,
  title={Pose primitive based human action recognition in videos or still images},
  author={Thurau, Christian and Hlav{\'a}c, V{\'a}clav},
  booktitle={CVPR},
  year={2008}
}


@inproceedings{curto2021dyadformer,
  title={Dyadformer: A multi-modal transformer for long-range modeling of dyadic interactions},
  author={Curto, David and Clap{\'e}s, Albert and Selva, Javier and Smeureanu, Sorina and Junior, Julio and Jacques, CS and Gallardo-Pujol, David and Guilera, Georgina and Leiva, David and Moeslund, Thomas B and others},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{mikolajczyk2008action,
  title={Action recognition with motion-appearance vocabulary forest},
  author={Mikolajczyk, Krystian and Uemura, Hirofumi},
  booktitle={CVPR},
  year={2008}
}


@article{wang2024internvideo2,
  title={Internvideo2: Scaling video foundation models for multimodal video understanding},
  author={Wang, Yi and Li, Kunchang and Li, Xinhao and Yu, Jiashuo and He, Yinan and Chen, Guo and Pei, Baoqi and Zheng, Rongkun and Xu, Jilan and Wang, Zun and others},
  journal={arxiv},
  year={2024}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={NeurIPS},
  year={2022}
}


@article{maaz2023video,
  title={Video-chatgpt: Towards detailed video understanding via large vision and language models},
  author={Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Khan, Fahad Shahbaz},
  journal={arxiv},
  year={2023}
}


@inproceedings{piergiovanni2024mirasol3b,
  title={Mirasol3b: A multimodal autoregressive model for time-aligned and contextual modalities},
  author={Piergiovanni, AJ and Noble, Isaac and Kim, Dahun and Ryoo, Michael S and Gomes, Victor and Angelova, Anelia},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{xu2023mplug,
  title={mplug-2: A modularized multi-modal foundation model across text, image and video},
  author={Xu, Haiyang and Ye, Qinghao and Yan, Ming and Shi, Yaya and Ye, Jiabo and Xu, Yuanhong and Li, Chenliang and Bi, Bin and Qian, Qi and Wang, Wei and others},
  booktitle={ICML},
  year={2023}
}

@article{zellers2021merlot,
  title={Merlot: Multimodal neural script knowledge models},
  author={Zellers, Rowan and Lu, Ximing and Hessel, Jack and Yu, Youngjae and Park, Jae Sung and Cao, Jize and Farhadi, Ali and Choi, Yejin},
  journal={NeurIPS},
  year={2021}
}

@article{zhao2024videoprism,
  title={VideoPrism: A Foundational Visual Encoder for Video Understanding},
  author={Zhao, Long and Gundavarapu, Nitesh B and Yuan, Liangzhe and Zhou, Hao and Yan, Shen and Sun, Jennifer J and Friedman, Luke and Qian, Rui and Weyand, Tobias and Zhao, Yue and others},
  journal={ICML},
  year={2024}
}

@inproceedings{zellers2022merlot,
  title={Merlot reserve: Neural script knowledge through vision and language and sound},
  author={Zellers, Rowan and Lu, Jiasen and Lu, Ximing and Yu, Youngjae and Zhao, Yanpeng and Salehi, Mohammadreza and Kusupati, Aditya and Hessel, Jack and Farhadi, Ali and Choi, Yejin},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{cao2022locvtp,
  title={Locvtp: Video-text pre-training for temporal localization},
  author={Cao, Meng and Yang, Tianyu and Weng, Junwu and Zhang, Can and Wang, Jue and Zou, Yuexian},
  booktitle={ECCV},
  year={2022},
}

@article{dwibedi2024ovr,
  title={OVR: A Dataset for Open Vocabulary Temporal Repetition Counting in Videos},
  author={Dwibedi, Debidatta and Aytar, Yusuf and Tompson, Jonathan and Zisserman, Andrew},
  journal={arXiv},
  year={2024}
}

@article{lei2021detecting,
  title={Detecting moments and highlights in videos via natural language queries},
  author={Lei, Jie and Berg, Tamara L and Bansal, Mohit},
  journal={NeurIPS},
  year={2021}
}

@article{escorcia2019temporal,
  title={Temporal localization of moments in video collections with natural language},
  author={Escorcia, Victor and Soldan, Mattia and Sivic, Josef and Ghanem, Bernard and Russell, Bryan},
  year={2019},
  journal={arxiv}
}

@inproceedings{buch2017sst,
  title={Sst: Single-stream temporal action proposals},
  author={Buch, Shyamal and Escorcia, Victor and Shen, Chuanqi and Ghanem, Bernard and Carlos Niebles, Juan},
  booktitle={CVPR},
  year={2017}
}

@article{lin2022egocentric,
  title={Egocentric video-language pretraining},
  author={Lin, Kevin Qinghong and Wang, Jinpeng and Soldan, Mattia and Wray, Michael and Yan, Rui and Xu, Eric Z and Gao, Difei and Tu, Rong-Cheng and Zhao, Wenzhe and Kong, Weijie and others},
  journal={NeurIPS},
  year={2022}
}

@article{wang2022contrastive,
  title={Contrastive video-language learning with fine-grained frame sampling},
  author={Wang, Zixu and Zhong, Yujie and Miao, Yishu and Ma, Lin and Specia, Lucia},
  journal={arXiv preprint arXiv:2210.05039},
  year={2022}
}


@article{yu2022coca,
  title={Coca: Contrastive captioners are image-text foundation models},
  author={Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
  journal={arxiv},
  year={2022}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={NeurIPS},
  year={2020}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arxiv},
  year={2023}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={ICML},
  year={2023},
}

@article{liu2024visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={NeurIPS},
  year={2024}
}


@inproceedings{grauman2022ego4d,
  title={Ego4d: Around the world in 3,000 hours of egocentric video},
  author={Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and others},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{fathi2012social,
  title={Social interactions: A first-person perspective},
  author={Fathi, Alircza and Hodgins, Jessica K and Rehg, James M},
  booktitle={CVPR},
  year={2012}
}


@inproceedings{xu2016msr,
  title={Msr-vtt: A large video description dataset for bridging video and language},
  author={Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{xiao2021next,
  title={Next-qa: Next phase of question-answering to explaining temporal actions},
  author={Xiao, Junbin and Shang, Xindi and Yao, Angela and Chua, Tat-Seng},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{miech2019howto100m,
  title={Howto100m: Learning a text-video embedding by watching hundred million narrated video clips},
  author={Miech, Antoine and Zhukov, Dimitri and Alayrac, Jean-Baptiste and Tapaswi, Makarand and Laptev, Ivan and Sivic, Josef},
  booktitle={CVPR},
  year={2019}
}

@article{lei2018tvqa,
  title={Tvqa: Localized, compositional video question answering},
  author={Lei, Jie and Yu, Licheng and Bansal, Mohit and Berg, Tamara L},
  journal={arxiv},
  year={2018}
}

@inproceedings{li2024mvbench,
  title={Mvbench: A comprehensive multi-modal video understanding benchmark},
  author={Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and others},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{cherian20222,
  title={(2.5+ 1) d spatio-temporal scene graphs for video question answering},
  author={Cherian, Anoop and Hori, Chiori and Marks, Tim K and Le Roux, Jonathan},
  booktitle={AAAI},
  year={2022}
}

@inproceedings{geng2021dynamic,
  title={Dynamic graph representation learning for video dialog via multi-modal shuffled transformers},
  author={Geng, Shijie and Gao, Peng and Chatterjee, Moitreya and Hori, Chiori and Le Roux, Jonathan and Zhang, Yongfeng and Li, Hongsheng and Cherian, Anoop},
  booktitle={AAAI},
  year={2021}
}

@inproceedings{chen2011collecting,
  title={Collecting highly parallel data for paraphrase evaluation},
  author={Chen, David and Dolan, William B},
  booktitle={ACL},
  year={2011}
}

 @inproceedings{zhou2018towards,
    author={Zhou, Luowei and Xu, Chenliang and Corso, Jason J},
    title = {Towards Automatic Learning of Procedures From Web Instructional Videos},
    booktitle = {AAAI},
    year = {2018},
  }
      
@inproceedings{wang2019vatex,
  title={Vatex: A large-scale, high-quality multilingual dataset for video-and-language research},
  author={Wang, Xin and Wu, Jiawei and Chen, Junkun and Li, Lei and Wang, Yuan-Fang and Wang, William Yang},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{gemmeke2017audio,
  title={Audio set: An ontology and human-labeled dataset for audio events},
  author={Gemmeke, Jort F and Ellis, Daniel PW and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R Channing and Plakal, Manoj and Ritter, Marvin},
  booktitle={ICASSP},
  year={2017}
}

@inproceedings{chen2020vggsound,
  title={Vggsound: A large-scale audio-visual dataset},
  author={Chen, Honglie and Xie, Weidi and Vedaldi, Andrea and Zisserman, Andrew},
  booktitle={ICASSP},
  year={2020}
}

@inproceedings{owens2016visually,
  title={Visually indicated sounds},
  author={Owens, Andrew and Isola, Phillip and McDermott, Josh and Torralba, Antonio and Adelson, Edward H and Freeman, William T},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{huh2023epic,
  title={Epic-sounds: A large-scale dataset of actions that sound},
  author={Huh, Jaesung and Chalk, Jacob and Kazakos, Evangelos and Damen, Dima and Zisserman, Andrew},
  booktitle={ICASSP},
  year={2023}
}

@inproceedings{zhou2022audio,
  title={Audio--visual segmentation},
  author={Zhou, Jinxing and Wang, Jianyuan and Zhang, Jiayi and Sun, Weixuan and Zhang, Jing and Birchfield, Stan and Guo, Dan and Kong, Lingpeng and Wang, Meng and Zhong, Yiran},
  booktitle={ECCV},
  year={2022},
}

@inproceedings{tian2020unified,
  title={Unified multisensory perception: Weakly-supervised audio-visual video parsing},
  author={Tian, Yapeng and Li, Dingzeyu and Xu, Chenliang},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{van2022exploring,
  title={Exploring fine-grained audiovisual categorization with the ssw60 dataset},
  author={Van Horn, Grant and Qian, Rui and Wilber, Kimberly and Adam, Hartwig and Mac Aodha, Oisin and Belongie, Serge},
  booktitle={ECCV},
  year={2022}
}

@article{sigal2010humaneva,
  title={Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion},
  author={Sigal, Leonid and Balan, Alexandru O and Black, Michael J},
  journal={IJCV},
  year={2010}
}

@article{joo2017panoptic,
  title={Panoptic Studio: A Massively Multiview System for Social Interaction Capture},
  author={Joo, Hanbyul and Simon, Tomas and Li, Xulong and Liu, Hao and Tan, Lei and Gui, Lin and Banerjee, Sean and Godisart, Timothy Scott and Nabbe, Bart and Matthews, Iain and Kanade, Takeo and Nobuhara, Shohei and Sheikh, Yaser},
  journal={IEEE TPAMI},
  year={2017}
}


@article{ionescu2013human3,
  title={Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments},
  author={Ionescu, Catalin and Papava, Dragos and Olaru, Vlad and Sminchisescu, Cristian},
  journal={IEEE TPAMI},
  year={2013}
}

@inproceedings{tsuchida2019aist,
  title={AIST Dance Video Database: Multi-Genre, Multi-Dancer, and Multi-Camera Database for Dance Information Processing.},
  author={Tsuchida, Shuhei and Fukayama, Satoru and Hamasaki, Masahiro and Goto, Masataka},
  booktitle={ISMIR},
  year={2019}
}

@inproceedings{peng2021neural,
  title={Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans},
  author={Peng, Sida and Zhang, Yuanqing and Xu, Yinghao and Wang, Qianqian and Shuai, Qing and Bao, Hujun and Zhou, Xiaowei},
  booktitle={CVPR},
  year={2021}
}

@article{broxton2020immersive,
  title={Immersive light field video with a layered mesh representation},
  author={Broxton, Michael and Flynn, John and Overbeck, Ryan and Erickson, Daniel and Hedman, Peter and Duvall, Matthew and Dourgarian, Jason and Busch, Jay and Whalen, Matt and Debevec, Paul},
  journal={ACM TOG},
  year={2020}
}

@inproceedings{yoon2020novel,
  title={Novel view synthesis of dynamic scenes with globally coherent depths from a monocular camera},
  author={Yoon, Jae Shin and Kim, Kihwan and Gallo, Orazio and Park, Hyun Soo and Kautz, Jan},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{lin2021deep,
  title={Deep 3d mask volume for view synthesis of dynamic scenes},
  author={Lin, Kai-En and Xiao, Lei and Liu, Feng and Yang, Guowei and Ramamoorthi, Ravi},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{li2022neural,
  title={Neural 3d video synthesis from multi-view video},
  author={Li, Tianye and Slavcheva, Mira and Zollhoefer, Michael and Green, Simon and Lassner, Christoph and Kim, Changil and Schmidt, Tanner and Lovegrove, Steven and Goesele, Michael and Newcombe, Richard and others},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{alayrac2017joint,
  title={Joint discovery of object states and manipulation actions},
  author={Alayrac, Jean-Baptiste and Laptev, Ivan and Sivic, Josef and Lacoste-Julien, Simon},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{liu2017jointly,
  title={Jointly recognizing object fluents and tasks in egocentric videos},
  author={Liu, Yang and Wei, Ping and Zhu, Song-Chun},
  booktitle={ICCV},
  year={2017}
}

@article{tschernezki2024epic,
  title={Epic fields: Marrying 3d geometry and video understanding},
  author={Tschernezki, Vadim and Darkhalil, Ahmad and Zhu, Zhifan and Fouhey, David and Laina, Iro and Larlus, Diane and Damen, Dima and Vedaldi, Andrea},
  journal={NeurIPS},
  year={2024}
}


@inproceedings{yeung2016end,
  title={End-to-end learning of action detection from frame glimpses in videos},
  author={Yeung, Serena and Russakovsky, Olga and Mori, Greg and Fei-Fei, Li},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{stergiou2023play,
  title={Play it back: Iterative attention for audio recognition},
  author={Stergiou, Alexandros and Damen, Dima},
  booktitle={ICASSP},
  year={2023},
}

@inproceedings{kazakos2021slow,
  title={Slow-fast auditory streams for audio recognition},
  author={Kazakos, Evangelos and Nagrani, Arsha and Zisserman, Andrew and Damen, Dima},
  booktitle={ICASSP},
  year={2021},
}

@article{gong2021psla,
  title={Psla: Improving audio tagging with pretraining, sampling, labeling, and aggregation},
  author={Gong, Yuan and Chung, Yu-An and Glass, James},
  journal={IEEE/ACM TASLP},
  year={2021},
}

@inproceedings{baade2022mae,
  title={Mae-ast: Masked autoencoding audio spectrogram transformer},
  author={Baade, Alan and Peng, Puyuan and Harwath, David},
  booktitle={Interspeech},
  year={2022}
}

@inproceedings{koutini2021efficient,
  title={Efficient training of audio transformers with patchout},
  author={Koutini, Khaled and Schl{\"u}ter, Jan and Eghbal-Zadeh, Hamid and Widmer, Gerhard},
  booktitle={Interspeech},
  year={2022}
}

@article{kong2020panns,
  title={Panns: Large-scale pretrained audio neural networks for audio pattern recognition},
  author={Kong, Qiuqiang and Cao, Yin and Iqbal, Turab and Wang, Yuxuan and Wang, Wenwu and Plumbley, Mark D},
  journal={IEEE/ACM TASLP},
  year={2020},
}

@article{wu2020dynamic,
  title={A dynamic frame selection framework for fast video recognition},
  author={Wu, Zuxuan and Li, Hengduo and Xiong, Caiming and Jiang, Yu-Gang and Davis, Larry S},
  journal={IEEE TPAMI},
  year={2020},
}

@inproceedings{kim2021efficient,
  title={Efficient action recognition via dynamic knowledge propagation},
  author={Kim, Hanul and Jain, Mihir and Lee, Jun-Tae and Yun, Sungrack and Porikli, Fatih},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{sun2021dynamic,
  title={Dynamic network quantization for efficient video inference},
  author={Sun, Ximeng and Panda, Rameswar and Chen, Chun-Fu Richard and Oliva, Aude and Feris, Rogerio and Saenko, Kate},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{abati2023resq,
  title={ResQ: Residual Quantization for Video Perception},
  author={Abati, Davide and Ben Yahia, Haitam and Nagel, Markus and Habibian, Amirhossein},
  booktitle={ICCV},
  year={2023}
}

@article{ma2022rethinking,
  title={Rethinking resolution in the context of efficient video recognition},
  author={Ma, Chuofan and Guo, Qiushan and Jiang, Yi and Luo, Ping and Yuan, Zehuan and Qi, Xiaojuan},
  journal={NeurIPS},
  year={2022}
}

@article{zhang2022look,
  title={Look more but care less in video recognition},
  author={Zhang, Yitian and Bai, Yue and Wang, Huan and Xu, Yi and Fu, Yun},
  journal={NeurIPS},
  year={2022}
}

@inproceedings{wu2024haltingvt,
  title={HaltingVT: Adaptive Token Halting Transformer for Efficient Video Recognition},
  author={Wu, Qian and Cui, Ruoxuan and Li, Yuke and Zhu, Haoqi},
  booktitle={ICASSP},
  year={2024},
}

@article{sandvine2024global,
  title={Global internet phenomena report},
  author={Sandvine, I},
  journal={North America and Latin America},
  year={2024}
}

@inproceedings{zhao2018sound,
  title={The sound of pixels},
  author={Zhao, Hang and Gan, Chuang and Rouditchenko, Andrew and Vondrick, Carl and McDermott, Josh and Torralba, Antonio},
  booktitle={ECCV},
  year={2018}
}

@article{gong2022uavm,
  title={Uavm: Towards unifying audio and visual models},
  author={Gong, Yuan and Liu, Alexander H and Rouditchenko, Andrew and Glass, James},
  journal={IEEE SPL},
  year={2022},
}

@inproceedings{hu2019deep,
  title={Deep multimodal clustering for unsupervised audiovisual learning},
  author={Hu, Di and Nie, Feiping and Li, Xuelong},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{hu2022mix,
  title={Mix and localize: Localizing sound sources in mixtures},
  author={Hu, Xixi and Chen, Ziyang and Owens, Andrew},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{mo2023unified,
  title={A unified audio-visual learning framework for localization, separation, and recognition},
  author={Mo, Shentong and Morgado, Pedro},
  booktitle={ICML},
  year={2023},
}

@inproceedings{wu2021exploring,
  title={Exploring heterogeneous clues for weakly-supervised audio-visual video parsing},
  author={Wu, Yu and Yang, Yi},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{ma2021active,
  title={Active Contrastive Learning of Audio-Visual Video Representations},
  author={Ma, Shuang and Zeng, Zhaoyang and McDuff, Daniel and Song, Yale},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{morgado2021audio,
  title={Audio-visual instance discrimination with cross-modal agreement},
  author={Morgado, Pedro and Vasconcelos, Nuno and Misra, Ishan},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{singh2024looking,
  title={Looking similar sounding different: Leveraging counterfactual cross-modal pairs for audiovisual representation learning},
  author={Singh, Nikhil and Wu, Chih-Wei and Orife, Iroro and Kalayeh, Mahdi},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{gong2023contrastive,
  title={Contrastive audio-visual masked autoencoder},
  author={Gong, Yuan and Rouditchenko, Andrew and Liu, Alexander H and Harwath, David and Karlinsky, Leonid and Kuehne, Hilde and Glass, James},
  booktitle={ICLR},
  year={2023}
}

@inproceedings{huang2023mavil,
  title={Mavil: Masked audio-video learners},
  author={Huang, Po-Yao and Sharma, Vasu and Xu, Hu and Ryali, Chaitanya and Li, Yanghao and Li, Shang-Wen and Ghosh, Gargi and Malik, Jitendra and Feichtenhofer, Christoph and others},
  booktitle={NeurIPS},
  year={2023}
}

@inproceedings{pian2023audio,
  title={Audio-visual class-incremental learning},
  author={Pian, Weiguo and Mo, Shentong and Guo, Yunhui and Tian, Yapeng},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{georgescu2023audiovisual,
  title={Audiovisual masked autoencoders},
  author={Georgescu, Mariana-Iuliana and Fonseca, Eduardo and Ionescu, Radu Tudor and Lucic, Mario and Schmid, Cordelia and Arnab, Anurag},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{zhang2024multimodal,
  title={Multimodal representation learning by alternating unimodal adaptation},
  author={Zhang, Xiaohui and Yoon, Jaehong and Bansal, Mohit and Yao, Huaxiu},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{guo2024crossmae,
  title={CrossMAE: Cross-Modality Masked Autoencoders for Region-Aware Audio-Visual Pre-Training},
  author={Guo, Yuxin and Sun, Siyang and Ma, Shuailei and Zheng, Kecheng and Bao, Xiaoyi and Ma, Shijie and Zou, Wei and Zheng, Yun},
  booktitle={CVPR},
  year={2024}
}

@article{lin2024siamese,
  title={Siamese vision transformers are scalable audio-visual learners},
  author={Lin, Yan-Bo and Bertasius, Gedas},
  journal={arxiv},
  year={2024}
}

@inproceedings{nagrani2021attention,
  title={Attention bottlenecks for multimodal fusion},
  author={Nagrani, Arsha and Yang, Shan and Arnab, Anurag and Jansen, Aren and Schmid, Cordelia and Sun, Chen},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{fayek2020large,
  title={Large scale audiovisual learning of sounds with weakly labeled data},
  author={Fayek, Haytham M and Kumar, Anurag},
  booktitle={IJCAI},
  year={2020}
}

@inproceedings{wang2020makes,
  title={What makes training multi-modal classification networks hard?},
  author={Wang, Weiyao and Tran, Du and Feiszli, Matt},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{lin2023vision,
  title={Vision transformers are parameter-efficient audio-visual learners},
  author={Lin, Yan-Bo and Sung, Yi-Lin and Lei, Jie and Bansal, Mohit and Bertasius, Gedas},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{jaegle2021perceiver,
  title={Perceiver: General perception with iterative attention},
  author={Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
  booktitle={ICML},
  year={2021},
}

@inproceedings{shahroudy2016ntu,
  title={Ntu rgb+ d: A large scale dataset for 3d human activity analysis},
  author={Shahroudy, Amir and Liu, Jun and Ng, Tian-Tsong and Wang, Gang},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{zhang2013actemes,
  title={From actemes to action: A strongly-supervised representation for detailed action understanding},
  author={Zhang, Weiyu and Zhu, Menglong and Derpanis, Konstantinos G},
  booktitle={ICCV},
  year={2013}
}

@inproceedings{tang2019coin,
  title={Coin: A large-scale dataset for comprehensive instructional video analysis},
  author={Tang, Yansong and Ding, Dajun and Rao, Yongming and Zheng, Yu and Zhang, Danyang and Zhao, Lili and Lu, Jiwen and Zhou, Jie},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{gu2018ava,
  title={Ava: A video dataset of spatio-temporally localized atomic visual actions},
  author={Gu, Chunhui and Sun, Chen and Ross, David A and Vondrick, Carl and Pantofaru, Caroline and Li, Yeqing and Vijayanarasimhan, Sudheendra and Toderici, George and Ricco, Susanna and Sukthankar, Rahul and others},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{ben2021ikea,
  title={The ikea asm dataset: Understanding people assembling furniture through actions, objects and pose},
  author={Ben-Shabat, Yizhak and Yu, Xin and Saleh, Fatemeh and Campbell, Dylan and Rodriguez-Opazo, Cristian and Li, Hongdong and Gould, Stephen},
  booktitle={WACV},
  year={2021}
}

@inproceedings{stergiou2023leaping,
  title={Leaping Into Memories: Space-Time Deep Feature Synthesis},
  author={Stergiou, Alexandros and Deligiannis, Nikos},
  booktitle={ICCV},
  year={2023}
}

@article{kuo2023mammut,
  title={Mammut: A simple architecture for joint learning for multimodal tasks},
  author={Kuo, Weicheng and Piergiovanni, AJ and Kim, Dahun and Luo, Xiyang and Caine, Ben and Li, Wei and Ogale, Abhijit and Zhou, Luowei and Dai, Andrew and Chen, Zhifeng and others},
  journal={TMLR},
  year={2023}
}

@inproceedings{islam2024video,
  title={Video ReCap: Recursive Captioning of Hour-Long Videos},
  author={Islam, Md Mohaiminul and Ho, Ngan and Yang, Xitong and Nagarajan, Tushar and Torresani, Lorenzo and Bertasius, Gedas},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{min2024morevqa,
  title={MoReVQA: Exploring Modular Reasoning Models for Video Question Answering},
  author={Min, Juhong and Buch, Shyamal and Nagrani, Arsha and Cho, Minsu and Schmid, Cordelia},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{xu2017video,
  title={Video question answering via gradually refined attention over appearance and motion},
  author={Xu, Dejing and Zhao, Zhou and Xiao, Jun and Wu, Fei and Zhang, Hanwang and He, Xiangnan and Zhuang, Yueting},
  booktitle={MM},
  year={2017}
}

@article{yu2023self,
  title={Self-chained image-language model for video localization and question answering},
  author={Yu, Shoubin and Cho, Jaemin and Yadav, Prateek and Bansal, Mohit},
  journal={NeurIPS},
  year={2023}
}

@inproceedings{xiao2024can,
  title={Can i trust your answer? visually grounded video question answering},
  author={Xiao, Junbin and Yao, Angela and Li, Yicong and Chua, Tat-Seng},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{gao2023mist,
  title={Mist: Multi-modal iterative spatial-temporal transformer for long-form video question answering},
  author={Gao, Difei and Zhou, Luowei and Ji, Lei and Zhu, Linchao and Yang, Yi and Shou, Mike Zheng},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{xiao2022video,
  title={Video graph transformer for video question answering},
  author={Xiao, Junbin and Zhou, Pan and Chua, Tat-Seng and Yan, Shuicheng},
  booktitle={ECCV},
  year={2022}
}

@article{xiao2023contrastive,
  title={Contrastive video question answering via video graph transformer},
  author={Xiao, Junbin and Zhou, Pan and Yao, Angela and Li, Yicong and Hong, Richang and Yan, Shuicheng and Chua, Tat-Seng},
  journal={IEEE TPAMI},
  year={2023},
}


@inproceedings{jang2017tgif,
  title={Tgif-qa: Toward spatio-temporal reasoning in visual question answering},
  author={Jang, Yunseok and Song, Yale and Yu, Youngjae and Kim, Youngjin and Kim, Gunhee},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{fan2019heterogeneous,
  title={Heterogeneous memory enhanced multimodal attention model for video question answering},
  author={Fan, Chenyou and Zhang, Xiaofan and Zhang, Shu and Wang, Wensheng and Zhang, Chi and Huang, Heng},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{ye2017video,
  title={Video question answering via attribute-augmented attention network learning},
  author={Ye, Yunan and Zhao, Zhou and Li, Yimeng and Chen, Long and Xiao, Jun and Zhuang, Yueting},
  booktitle={SIGIR},
  year={2017}
}

@inproceedings{yu2017end,
  title={End-to-end concept word detection for video captioning, retrieval, and question answering},
  author={Yu, Youngjae and Ko, Hyungjin and Choi, Jongwook and Kim, Gunhee},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{zeng2017leveraging,
  title={Leveraging video descriptions to learn video question answering},
  author={Zeng, Kuo-Hao and Chen, Tseng-Hung and Chuang, Ching-Yao and Liao, Yuan-Hong and Niebles, Juan Carlos and Sun, Min},
  booktitle={AAAI},
  year={2017}
}

@inproceedings{gao2018motion,
  title={Motion-appearance co-memory networks for video question answering},
  author={Gao, Jiyang and Ge, Runzhou and Chen, Kan and Nevatia, Ram},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{huang2020location,
  title={Location-aware graph convolutional networks for video question answering},
  author={Huang, Deng and Chen, Peihao and Zeng, Runhao and Du, Qing and Tan, Mingkui and Gan, Chuang},
  booktitle={AAAI},
  year={2020}
}

@inproceedings{li2019beyond,
  title={Beyond rnns: Positional self-attention with co-attention for video question answering},
  author={Li, Xiangpeng and Song, Jingkuan and Gao, Lianli and Liu, Xianglong and Huang, Wenbing and He, Xiangnan and Gan, Chuang},
  booktitle={AAAI},
  year={2019}
}

@inproceedings{mavroudi2023learning,
  title={Learning to ground instructional articles in videos through narrations},
  author={Mavroudi, Effrosyni and Afouras, Triantafyllos and Torresani, Lorenzo},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{li2022equivariant,
  title={Equivariant and invariant grounding for video question answering},
  author={Li, Yicong and Wang, Xiang and Xiao, Junbin and Chua, Tat-Seng},
  booktitle={MM},
  year={2022}
}

@inproceedings{li2023discovering,
  title={Discovering spatio-temporal rationales for video question answering},
  author={Li, Yicong and Xiao, Junbin and Feng, Chun and Wang, Xiang and Chua, Tat-Seng},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{jiang2020reasoning,
  title={Reasoning with heterogeneous graph alignment for video question answering},
  author={Jiang, Pin and Han, Yahong},
  booktitle={AAAI},
  year={2020}
}

@inproceedings{park2021bridge,
  title={Bridge to answer: Structure-aware graph interaction network for video question answering},
  author={Park, Jungin and Lee, Jiyoung and Sohn, Kwanghoon},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{guo2021multi,
  title={Multi-scale progressive attention network for video question answering},
  author={Guo, Zhicheng and Zhao, Jiaxuan and Jiao, Licheng and Liu, Xu and Li, Lingling},
  booktitle={ACL},
  year={2021}
}

@inproceedings{flanagan2023learning,
  title={Learning temporal sentence grounding from narrated egovideos},
  author={Flanagan, Kevin and Damen, Dima and Wray, Michael},
  booktitle={BMVC},
  year={2023}
}

@inproceedings{anne2017localizing,
  title={Localizing moments in video with natural language},
  author={Anne Hendricks, Lisa and Wang, Oliver and Shechtman, Eli and Sivic, Josef and Darrell, Trevor and Russell, Bryan},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{gao2017tall,
  title={Tall: Temporal activity localization via language query},
  author={Gao, Jiyang and Sun, Chen and Yang, Zhenheng and Nevatia, Ram},
  booktitle={ICCV},
  year={2017}
}

@article{regneri2013grounding,
  title={Grounding action descriptions in videos},
  author={Regneri, Michaela and Rohrbach, Marcus and Wetzel, Dominikus and Thater, Stefan and Schiele, Bernt and Pinkal, Manfred},
  journal={TACL},
  year={2013},
}

@inproceedings{zhang2023video,
  title={Video-llama: An instruction-tuned audio-visual language model for video understanding},
  author={Zhang, Hang and Li, Xin and Bing, Lidong},
  booktitle={EMNLP},
  year={2023}
}

@inproceedings{cao2021pursuit,
  title={On pursuit of designing multi-modal transformer for video grounding},
  author={Cao, Meng and Chen, Long and Shou, Mike Zheng and Zhang, Can and Zou, Yuexian},
  booktitle={EMNLP},
  year={2021}
}

@inproceedings{qian2024momentor,
  title={Momentor: Advancing video large language model with fine-grained temporal reasoning},
  author={Qian, Long and Li, Juncheng and Wu, Yu and Ye, Yaobo and Fei, Hao and Chua, Tat-Seng and Zhuang, Yueting and Tang, Siliang},
  booktitle={ICML},
  year={2024}
}

@inproceedings{gu2024context,
  title={Context-Guided Spatio-Temporal Video Grounding},
  author={Gu, Xin and Fan, Heng and Huang, Yan and Luo, Tiejian and Zhang, Libo},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{kim2024you,
  title={Do You Remember? Dense Video Captioning with Cross-Modal Memory Retrieval},
  author={Kim, Minkuk and Kim, Hyeon Bae and Moon, Jinyoung and Choi, Jinwoo and Kim, Seong Tae},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{wray2019fine,
  title={Fine-grained action retrieval through multiple parts-of-speech embeddings},
  author={Wray, Michael and Larlus, Diane and Csurka, Gabriela and Damen, Dima},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{gabeur2020multi,
  title={Multi-modal transformer for video retrieval},
  author={Gabeur, Valentin and Sun, Chen and Alahari, Karteek and Schmid, Cordelia},
  booktitle={ECCV},
  year={2020}
}

@article{albanie2020end,
  title={The end-of-end-to-end: A video understanding pentathlon challenge (2020)},
  author={Albanie, Samuel and Liu, Yang and Nagrani, Arsha and Miech, Antoine and Coto, Ernesto and Laptev, Ivan and Sukthankar, Rahul and Ghanem, Bernard and Zisserman, Andrew and Gabeur, Valentin and others},
  journal={arXiv},
  year={2020}
}

@inproceedings{alwassel2018diagnosing,
  title={Diagnosing error in temporal action detectors},
  author={Alwassel, Humam and Heilbron, Fabian Caba and Escorcia, Victor and Ghanem, Bernard},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{anderson2018vision,
  title={Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments},
  author={Anderson, Peter and Wu, Qi and Teney, Damien and Bruce, Jake and Johnson, Mark and S{\"u}nderhauf, Niko and Reid, Ian and Gould, Stephen and Van Den Hengel, Anton},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{liu2019use,
  title={Use what you have: Video retrieval using representations from collaborative experts},
  author={Liu, Yang and Albanie, Samuel and Nagrani, Arsha and Zisserman, Andrew},
  booktitle={BMVC},
  year={2019}
}

@inproceedings{mithun2018learning,
  title={Learning joint embedding with multimodal cues for cross-modal video-text retrieval},
  author={Mithun, Niluthpol Chowdhury and Li, Juncheng and Metze, Florian and Roy-Chowdhury, Amit K},
  booktitle={ICMR},
  year={2018}
}

@inproceedings{gordo2017beyond,
  title={Beyond instance-level image retrieval: Leveraging captions to learn a global visual representation for semantic retrieval},
  author={Gordo, Albert and Larlus, Diane},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{wang2016learning,
  title={Learning deep structure-preserving image-text embeddings},
  author={Wang, Liwei and Li, Yin and Lazebnik, Svetlana},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{wray2021semantic,
  title={On semantic similarity in video retrieval},
  author={Wray, Michael and Doughty, Hazel and Damen, Dima},
  booktitle={CVPR},
  year={2021}
}

@article{dong2018predicting,
  title={Predicting visual features from text for image and video caption retrieval},
  author={Dong, Jianfeng and Li, Xirong and Snoek, Cees GM},
  journal={TM},
  year={2018}
}

@inproceedings{otani2016learning,
  title={Learning joint representations of videos and sentences with web image search},
  author={Otani, Mayu and Nakashima, Yuta and Rahtu, Esa and Heikkil{\"a}, Janne and Yokoya, Naokazu},
  booktitle={ECCVw},
  year={2016}
}

@article{torabi2016learning,
  title={Learning language-visual embedding for movie understanding with natural-language},
  author={Torabi, Atousa and Tandon, Niket and Sigal, Leonid},
  journal={arXiv},
  year={2016}
}

@inproceedings{xu2015jointly,
  title={Jointly modeling deep video and compositional text to bridge vision and language in a unified framework},
  author={Xu, Ran and Xiong, Caiming and Chen, Wei and Corso, Jason},
  booktitle={AAAI},
  year={2015}
}

@inproceedings{oncescu2021queryd,
  title={Queryd: A video dataset with high-quality text and audio narrations},
  author={Oncescu, Andreea-Maria and Henriques, Joao F and Liu, Yang and Zisserman, Andrew and Albanie, Samuel},
  booktitle={ICASSP},
  year={2021}
}

@article{yang2022zero,
  title={Zero-shot video question answering via frozen bidirectional language models},
  author={Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
  journal={NeurIPS},
  year={2022}
}

@inproceedings{iashin2020better,
  title={A better use of audio-visual cues: Dense video captioning with bi-modal transformer},
  author={Iashin, Vladimir and Rahtu, Esa},
  booktitle={BMVC},
  year={2020}
}

@inproceedings{iashin2020multi,
  title={Multi-modal dense video captioning},
  author={Iashin, Vladimir and Rahtu, Esa},
  booktitle={CVPRw},
  year={2020}
}

@inproceedings{krishna2017dense,
  title={Dense-captioning events in videos},
  author={Krishna, Ranjay and Hata, Kenji and Ren, Frederic and Fei-Fei, Li and Carlos Niebles, Juan},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{wang2018bidirectional,
  title={Bidirectional attentive fusion with context gating for dense video captioning},
  author={Wang, Jingwen and Jiang, Wenhao and Ma, Lin and Liu, Wei and Xu, Yong},
  booktitle={CVPR},
  year={2018}
}

@article{wang2020event,
  title={Event-centric hierarchical representation for dense video captioning},
  author={Wang, Teng and Zheng, Huicheng and Yu, Mingjing and Tian, Qian and Hu, Haifeng},
  journal={IEEE TCSVT},
  year={2020},
}

@inproceedings{liu2021hair,
  title={Hair: Hierarchical visual-semantic relational reasoning for video question answering},
  author={Liu, Fei and Liu, Jing and Wang, Weining and Lu, Hanqing},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{dang2021hierarchical,
  title={Hierarchical Object-oriented Spatio-Temporal Reasoning for Video Question Answering},
  author={Dang, Long Hoang and Le, Thao Minh and Le, Vuong and Tran, Truyen},
  booktitle={IJCAI},
  year={2021}
}

@inproceedings{zhu2020actbert,
  title={Actbert: Learning global-local video-text representations},
  author={Zhu, Linchao and Yang, Yi},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{chadha2021iperceive,
  title={iPerceive: Applying common-sense reasoning to multi-modal dense video captioning and video question answering},
  author={Chadha, Aman and Arora, Gurneet and Kaloty, Navpreet},
  booktitle={WACV},
  year={2021}
}

@inproceedings{alayrac2016unsupervised,
  title={Unsupervised learning from narrated instruction videos},
  author={Alayrac, Jean-Baptiste and Bojanowski, Piotr and Agrawal, Nishant and Sivic, Josef and Laptev, Ivan and Lacoste-Julien, Simon},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{chen2021towards,
  title={Towards bridging event captioner and sentence localizer for weakly supervised dense event captioning},
  author={Chen, Shaoxiang and Jiang, Yu-Gang},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{dwibedi2020counting,
  title={Counting out time: Class agnostic video repetition counting in the wild},
  author={Dwibedi, Debidatta and Aytar, Yusuf and Tompson, Jonathan and Sermanet, Pierre and Zisserman, Andrew},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{deng2021sketch,
  title={Sketch, ground, and refine: Top-down dense video captioning},
  author={Deng, Chaorui and Chen, Shizhe and Chen, Da and He, Yuan and Wu, Qi},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{li2018jointly,
  title={Jointly localizing and describing events for dense video captioning},
  author={Li, Yehao and Yao, Ting and Pan, Yingwei and Chao, Hongyang and Mei, Tao},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{mun2019streamlined,
  title={Streamlined dense video captioning},
  author={Mun, Jonghwan and Yang, Linjie and Ren, Zhou and Xu, Ning and Han, Bohyung},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{rahman2019watch,
  title={Watch, listen and tell: Multi-modal weakly supervised dense event captioning},
  author={Rahman, Tanzila and Xu, Bicheng and Sigal, Leonid},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{shen2017weakly,
  title={Weakly supervised dense video captioning},
  author={Shen, Zhiqiang and Li, Jianguo and Su, Zhou and Li, Minjun and Chen, Yurong and Jiang, Yu-Gang and Xue, Xiangyang},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{shi2019dense,
  title={Dense procedure captioning in narrated instructional videos},
  author={Shi, Botian and Ji, Lei and Liang, Yaobo and Duan, Nan and Chen, Peng and Niu, Zhendong and Zhou, Ming},
  booktitle={ACL},
  year={2019}
}

@inproceedings{wang2021end,
  title={End-to-end dense video captioning with parallel decoding},
  author={Wang, Teng and Zhang, Ruimao and Lu, Zhichao and Zheng, Feng and Cheng, Ran and Luo, Ping},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{zhou2018end,
  title={End-to-end dense video captioning with masked transformer},
  author={Zhou, Luowei and Zhou, Yingbo and Corso, Jason J and Socher, Richard and Xiong, Caiming},
  booktitle={CVPR},
  year={2018}
}

@article{akbari2021vatt,
  title={Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text},
  author={Akbari, Hassan and Yuan, Liangzhe and Qian, Rui and Chuang, Wei-Hong and Chang, Shih-Fu and Cui, Yin and Gong, Boqing},
  journal={NeurIPS},
  year={2021}
}

@inproceedings{bain2021frozen,
  title={Frozen in time: A joint video and image encoder for end-to-end retrieval},
  author={Bain, Max and Nagrani, Arsha and Varol, G{\"u}l and Zisserman, Andrew},
  booktitle={ICCV},
  year={2021}
}

@article{fu2021violet,
  title={Violet: End-to-end video-language transformers with masked visual-token modeling},
  author={Fu, Tsu-Jui and Li, Linjie and Gan, Zhe and Lin, Kevin and Wang, William Yang and Wang, Lijuan and Liu, Zicheng},
  journal={arxiv},
  year={2021}
}

@inproceedings{ge2022bridging,
  title={Bridging video-text retrieval with multiple choice questions},
  author={Ge, Yuying and Ge, Yixiao and Liu, Xihui and Li, Dian and Shan, Ying and Qie, Xiaohu and Luo, Ping},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{han2022temporal,
  title={Temporal alignment networks for long-term video},
  author={Han, Tengda and Xie, Weidi and Zisserman, Andrew},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{ko2022video,
  title={Video-text representation learning via differentiable weak temporal alignment},
  author={Ko, Dohwan and Choi, Joonmyung and Ko, Juyeon and Noh, Shinyeong and On, Kyoung-Woon and Kim, Eun-Sol and Kim, Hyunwoo J},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{wang2022object,
  title={Object-aware video-language pre-training for retrieval},
  author={Wang, Jinpeng and Ge, Yixiao and Cai, Guanyu and Yan, Rui and Lin, Xudong and Shan, Ying and Qie, Xiaohu and Shou, Mike Zheng},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{xue2022advancing,
  title={Advancing high-resolution video-language representation with large-scale video transcriptions},
  author={Xue, Hongwei and Hang, Tiankai and Zeng, Yanhong and Sun, Yuchong and Liu, Bei and Yang, Huan and Fu, Jianlong and Guo, Baining},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{yang2021just,
  title={Just ask: Learning to answer questions from millions of narrated videos},
  author={Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{li2022align,
  title={Align and prompt: Video-and-language pre-training with entity prompts},
  author={Li, Dongxu and Li, Junnan and Li, Hongdong and Niebles, Juan Carlos and Hoi, Steven CH},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{li2020hero,
  title={Hero: Hierarchical encoder for video+ language omni-representation pre-training},
  author={Li, Linjie and Chen, Yen-Chun and Cheng, Yu and Gan, Zhe and Yu, Licheng and Liu, Jingjing},
  booktitle={EMNLP},
  year={2020}
}

@inproceedings{miech2020end,
  title={End-to-end learning of visual representations from uncurated instructional videos},
  author={Miech, Antoine and Alayrac, Jean-Baptiste and Smaira, Lucas and Laptev, Ivan and Sivic, Josef and Zisserman, Andrew},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{seo2022end,
  title={End-to-end generative pretraining for multimodal video captioning},
  author={Seo, Paul Hongsuck and Nagrani, Arsha and Arnab, Anurag and Schmid, Cordelia},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{seo2021look,
  title={Look before you speak: Visually contextualized utterances},
  author={Seo, Paul Hongsuck and Nagrani, Arsha and Schmid, Cordelia},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{sun2019videobert,
  title={Videobert: A joint model for video and language representation learning},
  author={Sun, Chen and Myers, Austin and Vondrick, Carl and Murphy, Kevin and Schmid, Cordelia},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{wang2023all,
  title={All in one: Exploring unified video-language pre-training},
  author={Wang, Jinpeng and Ge, Yixiao and Yan, Rui and Ge, Yuying and Lin, Kevin Qinghong and Tsutsui, Satoshi and Lin, Xudong and Cai, Guanyu and Wu, Jianping and Shan, Ying and others},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{hu2022transrac,
  title={Transrac: Encoding multi-scale temporal correlation with transformers for repetitive action counting},
  author={Hu, Huazhang and Dong, Sixun and Zhao, Yiqun and Lian, Dongze and Li, Zhengxin and Gao, Shenghua},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{zhang2020context,
  title={Context-aware and scale-insensitive temporal repetition counting},
  author={Zhang, Huaidong and Xu, Xuemiao and Han, Guoqiang and He, Shengfeng},
  booktitle={CVPR},
  year={2020}
}


@article{lu2004repetitive,
  title={{Repetitive Motion Analysis: Segmentation and Event Classification}},
  author={Lu, ChunMei and Ferrier, Nicola J},
  journal={IEEE TPAMI},
  year={2004},
}

@inproceedings{panagiotakis2018unsupervised,
  title={{Unsupervised Detection of Periodic Segments in Videos}},
  author={Panagiotakis, Costas and Karvounas, Giorgos and Argyros, Antonis},
  booktitle={ICIP},
  year={2018}
}

@inproceedings{thangali2005periodic,
  title={Periodic motion detection and estimation via space-time sampling},
  author={Thangali, Ashwin and Sclaroff, Stan},
  booktitle={WACV},
  year={2005}
}

@article{junejo2010view,
  title={View-independent action recognition from temporal self-similarities},
  author={Junejo, Imran N and Dexter, Emilie and Laptev, Ivan and Perez, Patrick},
  journal={IEEE TPAMI},
  year={2010}
}

@inproceedings{korner2013temporal,
  title={Temporal self-similarity for appearance-based action recognition in multi-view setups},
  author={K{\"o}rner, Marco and Denzler, Joachim},
  booktitle={CAIP},
  year={2013},
}

@article{benabdelkader2004gait,
  title={Gait recognition using image self-similarity},
  author={BenAbdelkader, Chiraz and Cutler, Ross G and Davis, Larry S},
  journal={EURASIP},
  year={2004},
}

@inproceedings{runia2018real,
  title={{Real-World Repetition Estimation by Div, Grad and Curl}},
  author={Runia, Tom FH and Snoek, Cees GM and Smeulders, Arnold WM},
  booktitle={CVPR},
  year={2018}
}

@article{ferreira2021deep,
  title={{Deep Learning Approaches for Workout Repetition Counting and Validation}},
  author={Ferreira, Bruno and Ferreira, Pedro M and Pinheiro, Gil and Figueiredo, Nelson and Carvalho, Filipe and Menezes, Paulo and Batista, Jorge},
  journal={PRL},
  year={2021}
}

@article{yao2023poserac,
  title={{PoseRAC: Pose Saliency Transformer for Repetitive Action Counting}},
  author={Yao, Ziyu and Cheng, Xuxin and Zou, Yuexian},
  journal={arxiv},
  year={2023}
}

@article{yang2022learning,
  title={Learning to answer visual questions from web videos},
  author={Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
  journal={IEEE TPAMI},
  year={2022}
}

@inproceedings{yang2022tubedetr,
  title={Tubedetr: Spatio-temporal video grounding with transformers},
  author={Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
  booktitle={CVPR},
  year={2022}
}

@article{sinha2024every,
  title={Every Shot Counts: Using Exemplars for Repetition Counting in Videos},
  author={Sinha, Saptarshi and Stergiou, Alexandros and Damen, Dima},
  journal={arxiv},
  year={2024}
}



@article{lee2016making,
  title={Making stochastic neural networks from deterministic ones},
  author={Lee, Kimin and Kim, Jaehyung and Chong, Song and Shin, Jinwoo},
  year={2016}
}

@inproceedings{sermanet2017unsupervised,
  title={Unsupervised perceptual rewards for imitation learning},
  author={Sermanet, Pierre and Xu, Kelvin and Levine, Sergey},
  booktitle={ICLRw},
  year={2017}
}

@inproceedings{bansal2022my,
  title={My view is the best view: Procedure learning from egocentric videos},
  author={Bansal, Siddhant and Arora, Chetan and Jawahar, CV},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{chaabane2020looking,
  title={Looking ahead: Anticipating pedestrians crossing with future frames prediction},
  author={Chaabane, Mohamed and Trabelsi, Ameni and Blanchard, Nathaniel and Beveridge, Ross},
  booktitle={WACV},
  year={2020}
}

@inproceedings{jin2020exploring,
  title={Exploring spatial-temporal multi-frequency analysis for high-fidelity and temporal-consistency video prediction},
  author={Jin, Beibei and Hu, Yu and Tang, Qiankun and Niu, Jingyu and Shi, Zhiping and Han, Yinhe and Li, Xiaowei},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{wang2020probabilistic,
  title={Probabilistic video prediction from noisy data with a posterior confidence},
  author={Wang, Yunbo and Wu, Jiajun and Long, Mingsheng and Tenenbaum, Joshua B},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{chang2021mau,
  title={Mau: A motion-aware unit for video prediction and beyond},
  author={Chang, Zheng and Zhang, Xinfeng and Wang, Shanshe and Ma, Siwei and Ye, Yan and Xinguang, Xiang and Gao, Wen},
  booktitle={NeurIPS},
  year={2021}
}

@article{su2020convolutional,
  title={Convolutional tensor-train LSTM for spatio-temporal learning},
  author={Su, Jiahao and Byeon, Wonmin and Kossaifi, Jean and Huang, Furong and Kautz, Jan and Anandkumar, Anima},
  journal={NeurIPS},
  year={2020}
}

@inproceedings{wang2018predrnn++,
  title={Predrnn++: Towards a resolution of the deep-in-time dilemma in spatiotemporal predictive learning},
  author={Wang, Yunbo and Gao, Zhifeng and Long, Mingsheng and Wang, Jianmin and Philip, S Yu},
  booktitle={ICML},
  year={2018},
}

@inproceedings{wang2023learning,
  title={Learning from semantic alignment between unpaired multiviews for egocentric video recognition},
  author={Wang, Qitong and Zhao, Long and Yuan, Liangzhe and Liu, Ting and Peng, Xi},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{souvcek2022look,
  title={Look for the change: Learning object states and state-modifying actions from untrimmed web videos},
  author={Sou{\v{c}}ek, Tom{\'a}{\v{s}} and Alayrac, Jean-Baptiste and Miech, Antoine and Laptev, Ivan and Sivic, Josef},
  booktitle={CVPR},
  year={2022}
}


@inproceedings{purushwalkam2019task,
  title={Task-driven modular networks for zero-shot compositional learning},
  author={Purushwalkam, Senthil and Nickel, Maximilian and Gupta, Abhinav and Ranzato, Marc'Aurelio},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{doughty2018s,
  title={Who's better? who's best? pairwise deep ranking for skill determination},
  author={Doughty, Hazel and Damen, Dima and Mayol-Cuevas, Walterio},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{doughty2020action,
  title={Action modifiers: Learning from adverbs in instructional videos},
  author={Doughty, Hazel and Laptev, Ivan and Mayol-Cuevas, Walterio and Damen, Dima},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{moltisanti2023learning,
  title={Learning action changes by measuring verb-adverb textual relationships},
  author={Moltisanti, Davide and Keller, Frank and Bilen, Hakan and Sevilla-Lara, Laura},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{martin2019drive,
  title={Drive\&act: A multi-modal dataset for fine-grained driver behavior recognition in autonomous vehicles},
  author={Martin, Manuel and Roitberg, Alina and Haurilet, Monica and Horne, Matthias and Rei{\ss}, Simon and Voit, Michael and Stiefelhagen, Rainer},
  booktitle={ICCV},
  year={2019}
}

@article{becattini2020done,
  title={Am I done? Predicting action progress in videos},
  author={Becattini, Federico and Uricchio, Tiberio and Seidenari, Lorenzo and Ballan, Lamberto and Bimbo, Alberto Del},
  journal={TOMM},
  year={2020},
}

@inproceedings{price2022unweavenet,
  title={Unweavenet: Unweaving activity stories},
  author={Price, Will and Vondrick, Carl and Damen, Dima},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{tang2020uncertainty,
  title={Uncertainty-aware score distribution learning for action quality assessment},
  author={Tang, Yansong and Ni, Zanlin and Zhou, Jiahuan and Zhang, Danyang and Lu, Jiwen and Wu, Ying and Zhou, Jie},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{parmar2019and,
  title={What and how well you performed? a multitask learning approach to action quality assessment},
  author={Parmar, Paritosh and Morris, Brendan Tran},
  booktitle={CVPR},
  year={2019}
}


@inproceedings{damen2024genhowto,
  title={GenHowTo: Learning to Generate Actions and State Transformations from Instructional Videos},
  author={Sou{\v{c}}ek, Tom{\'a}{\v{s}} and Damen, Dima and Wray, Michael and Laptev, Ivan and Sivic, Josef and others},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{ragusa2021meccano,
  title={The meccano dataset: Understanding human-object interactions from egocentric videos in an industrial-like domain},
  author={Ragusa, Francesco and Furnari, Antonino and Livatino, Salvatore and Farinella, Giovanni Maria},
  booktitle={WACV},
  year={2021}
}

@inproceedings{fathi2013modeling,
  title={Modeling actions through state changes},
  author={Fathi, Alireza and Rehg, James M},
  booktitle={CVPR},
  year={2013}
}

@inproceedings{donahue2024learning,
  title={Learning to predict activity progress by self-supervised video alignment},
  author={Donahue, Gerard and Elhamifar, Ehsan},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{shen2024progress,
  title={Progress-aware online action segmentation for egocentric procedural task videos},
  author={Shen, Yuhan and Elhamifar, Ehsan},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{haresh2021learning,
  title={Learning by aligning videos in time},
  author={Haresh, Sanjay and Kumar, Sateesh and Coskun, Huseyin and Syed, Shahram N and Konin, Andrey and Zia, Zeeshan and Tran, Quoc-Huy},
  booktitle={CVPR},
  year={2021}
}

@article{dvornik2021drop,
  title={Drop-dtw: Aligning common signal between sequences while dropping outliers},
  author={Dvornik, Mikita and Hadji, Isma and Derpanis, Konstantinos G and Garg, Animesh and Jepson, Allan},
  journal={NeurIPS},
  year={2021}
}

@inproceedings{liu2022learning,
  title={Learning to align sequential actions in the wild},
  author={Liu, Weizhe and Tekin, Bugra and Coskun, Huseyin and Vineet, Vibhav and Fua, Pascal and Pollefeys, Marc},
  booktitle={CVPR},
  year={2022}
}

@article{xiong2017pursuit,
  title={A pursuit of temporal accuracy in general activity detection},
  author={Xiong, Yuanjun and Zhao, Yue and Wang, Limin and Lin, Dahua and Tang, Xiaoou},
  journal={arxiv},
  year={2017}
}

@inproceedings{kim2021hotr,
  title={Hotr: End-to-end human-object interaction detection with transformers},
  author={Kim, Bumsoo and Lee, Junhyun and Kang, Jaewoo and Kim, Eun-Sol and Kim, Hyunwoo J},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{ortega2020dmd,
  title={Dmd: A large-scale multi-modal driver monitoring dataset for attention and alertness analysis},
  author={Ortega, Juan Diego and Kose, Neslihan and Ca{\~n}as, Paola and Chao, Min-An and Unnervik, Alexander and Nieto, Marcos and Otaegui, Oihana and Salgado, Luis},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{heidarivincheh2016beyond,
  title={Beyond action recognition: Action completion in rgb-d data},
  author={Heidarivincheh, Farnoosh and Mirmehdi, Majid and Damen, Dima},
  booktitle={BMVC},
  year={2016}
}

@inproceedings{heidarivincheh2018action,
  title={Action completion: A temporal model for moment detection},
  author={Heidarivincheh, Farnoosh and Mirmehdi, Majid and Damen, Dima},
  booktitle={BMVC},
  year={2018}
}

@inproceedings{epstein2020oops,
  title={Oops! predicting unintentional action in video},
  author={Epstein, Dave and Chen, Boyuan and Vondrick, Carl},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{sener2022assembly101,
  title={Assembly101: A large-scale multi-view video dataset for understanding procedural activities},
  author={Sener, Fadime and Chatterjee, Dibyadip and Shelepov, Daniel and He, Kun and Singhania, Dipika and Wang, Robert and Yao, Angela},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{xue2024learning,
  title={Learning object state changes in videos: An open-world perspective},
  author={Xue, Zihui and Ashutosh, Kumar and Grauman, Kristen},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{yang2024active,
  title={Active Object Detection with Knowledge Aggregation and Distillation from Large Models},
  author={Yang, Dejie and Liu, Yang},
  booktitle={CVPR},
  year={2024}
}

@article{dunnhofer2023visual,
  title={Visual object tracking in first person vision},
  author={Dunnhofer, Matteo and Furnari, Antonino and Farinella, Giovanni Maria and Micheloni, Christian},
  journal={IJCV},
  year={2023},
}

@inproceedings{fu2021sequential,
  title={Sequential Decision-Making for Active Object Detection from Hand},
  author={Fu, Qichen and Liu, Xingyu and Kitani, Kris M},
  booktitle={CVPR},
  year={2022}
}
@inproceedings{mittal2024can,
  title={Can't make an Omelette without Breaking some Eggs: Plausible Action Anticipation using Large Video-Language Models},
  author={Mittal, Himangi and Agarwal, Nakul and Lo, Shao-Yuan and Lee, Kwonjoon},
  booktitle={CVPR},
  year={2024}
}

@article{gouidis2023leveraging,
  title={Leveraging knowledge graphs for zero-shot object-agnostic state classification},
  author={Gouidis, Filipos and Patkos, Theodore and Argyros, Antonis and Plexousakis, Dimitris},
  journal={arxiv},
  year={2023}
}

@inproceedings{misra2017red,
  title={From red wine to red tomato: Composition with context},
  author={Misra, Ishan and Gupta, Abhinav and Hebert, Martial},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{nagarajan2018attributes,
  title={Attributes as operators: factorizing unseen attribute-object compositions},
  author={Nagarajan, Tushar and Grauman, Kristen},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{saini2023chop,
  title={Chop \& learn: Recognizing and generating object-state compositions},
  author={Saini, Nirat and Wang, Hanyu and Swaminathan, Archana and Jayasundara, Vinoj and He, Bo and Gupta, Kamal and Shrivastava, Abhinav},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{yu2023video,
  title={Video state-changing object segmentation},
  author={Yu, Jiangwei and Li, Xiang and Zhao, Xinran and Zhang, Hongming and Wang, Yu-Xiong},
  booktitle={ICCV},
  year={2023}
}

@article{alayrac2024multi,
  title={Multi-Task Learning of Object States and State-Modifying Actions from Web Videos},
  author={Alayrac, Jean-Baptiste and Miech, Antoine and Laptev, Ivan and Sivic, Josef and others},
  journal={IEEE TPAMI},
  year={2024},
}

@article{mangalam2023egoschema,
  title={Egoschema: A diagnostic benchmark for very long-form video language understanding},
  author={Mangalam, Karttikeya and Akshulakov, Raiymbek and Malik, Jitendra},
  journal={NeurIPS},
  year={2023}
}

@article{vaina1991object,
  title={Object structure and action requirements: A compatibility model for functional recognition},
  author={Vaina, Lucia M and Jaulent, Marie-Christine},
  journal={IJIS},
  year={1991},
}

@inproceedings{chen2020rethinking,
  title={Rethinking the bottom-up framework for query-based video localization},
  author={Chen, Long and Lu, Chujie and Tang, Siliang and Xiao, Jun and Zhang, Dong and Tan, Chilie and Li, Xiaolin},
  booktitle={AAAI},
  year={2020}
}

@article{hao2022query,
  title={Query-aware video encoder for video moment retrieval},
  author={Hao, Jiachang and Sun, Haifeng and Ren, Pengfei and Wang, Jingyu and Qi, Qi and Liao, Jianxin},
  journal={Neurocomputing},
  year={2022}
}

@inproceedings{liu2022memory,
  title={Memory-guided semantic learning network for temporal sentence grounding},
  author={Liu, Daizong and Qu, Xiaoye and Di, Xing and Cheng, Yu and Xu, Zichuan and Zhou, Pan},
  booktitle={AAAI},
  year={2022}
}

@inproceedings{nan2021interventional,
  title={Interventional video grounding with dual contrastive learning},
  author={Nan, Guoshun and Qiao, Rui and Xiao, Yao and Liu, Jun and Leng, Sicong and Zhang, Hao and Lu, Wei},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{yuan2019find,
  title={To find where you talk: Temporal sentence localization in video with attention based location regression},
  author={Yuan, Yitian and Mei, Tao and Zhu, Wenwu},
  booktitle={AAAI},
  year={2019}
}

@article{zhang2021natural,
  title={Natural language video localization: A revisit in span-based question answering framework},
  author={Zhang, Hao and Sun, Aixin and Jing, Wei and Zhen, Liangli and Zhou, Joey Tianyi and Goh, Rick Siow Mong},
  journal={IEEE TPAMI},
  year={2021}
}

@inproceedings{zhang2023adding,
  title={Adding conditional control to text-to-image diffusion models},
  author={Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{goroshin2015unsupervised,
  title={Unsupervised learning of spatiotemporally coherent metrics},
  author={Goroshin, Ross and Bruna, Joan and Tompson, Jonathan and Eigen, David and LeCun, Yann},
  booktitle={ICCV},
  year={2015}
}

@inproceedings{chen2018temporally,
  title={Temporally grounding natural sentence in video},
  author={Chen, Jingyuan and Chen, Xinpeng and Ma, Lin and Jie, Zequn and Chua, Tat-Seng},
  booktitle={EMNLP},
  year={2018}
}

@inproceedings{ge2019mac,
  title={Mac: Mining activity concepts for language-based temporal localization},
  author={Ge, Runzhou and Gao, Jiyang and Chen, Kan and Nevatia, Ram},
  booktitle={WACV},
  year={2019},
}

@inproceedings{jiang2019cross,
  title={Cross-modal video moment retrieval with spatial and language-temporal attention},
  author={Jiang, Bin and Huang, Xin and Yang, Chao and Yuan, Junsong},
  booktitle={ICMR},
  year={2019}
}

@inproceedings{liu2021context,
  title={Context-aware biaffine localizing network for temporal sentence grounding},
  author={Liu, Daizong and Qu, Xiaoye and Dong, Jianfeng and Zhou, Pan and Cheng, Yu and Wei, Wei and Xu, Zichuan and Xie, Yulai},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{liu2018cross,
  title={Cross-modal moment localization in videos},
  author={Liu, Meng and Wang, Xiang and Nie, Liqiang and Tian, Qi and Chen, Baoquan and Chua, Tat-Seng},
  booktitle={MM},
  year={2018}
}

@inproceedings{qu2020fine,
  title={Fine-grained iterative attention network for temporal language localization in videos},
  author={Qu, Xiaoye and Tang, Pengwei and Zou, Zhikang and Cheng, Yu and Dong, Jianfeng and Zhou, Pan and Xu, Zichuan},
  booktitle={MM},
  year={2020}
}

@inproceedings{wang2020temporally,
  title={Temporally grounding language queries in videos by contextual boundary-aware prediction},
  author={Wang, Jingwen and Ma, Lin and Jiang, Wenhao},
  booktitle={AAAI},
  year={2020}
}

@inproceedings{xu2019multilevel,
  title={Multilevel language and vision integration for text-to-clip retrieval},
  author={Xu, Huijuan and He, Kun and Plummer, Bryan A and Sigal, Leonid and Sclaroff, Stan and Saenko, Kate},
  booktitle={AAAI},
  year={2019}
}

@inproceedings{fernando2017self,
  title={Self-supervised video representation learning with odd-one-out networks},
  author={Fernando, Basura and Bilen, Hakan and Gavves, Efstratios and Gould, Stephen},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{andrew2013deep,
  title={Deep canonical correlation analysis},
  author={Andrew, Galen and Arora, Raman and Bilmes, Jeff and Livescu, Karen},
  booktitle={ICML},
  year={2013},
}

@article{sakoe1978dynamic,
  title={Dynamic programming algorithm optimization for spoken word recognition},
  author={Sakoe, Hiroaki and Chiba, Seibi},
  journal={IEEE TASSP},
  year={1978},
}

@inproceedings{chang2019d3tw,
  title={D3tw: Discriminative differentiable dynamic time warping for weakly supervised action alignment and segmentation},
  author={Chang, Chien-Yi and Huang, De-An and Sui, Yanan and Fei-Fei, Li and Niebles, Juan Carlos},
  booktitle={CVPR},
  year={2019}
}

@article{liu2023video,
  title={Video timeline modeling for news story understanding},
  author={Liu, Meng and Zhang, Mingda and Liu, Jialu and Dai, Hanjun and Yang, Ming-Hsuan and Ji, Shuiwang and Feng, Zheyun and Gong, Boqing},
  journal={NeurIPS},
  year={2023}
}

@inproceedings{park2022exposing,
  title={Exposing the limits of video-text models through contrast sets},
  author={Park, Jae Sung and Shen, Sheng and Farhadi, Ali and Darrell, Trevor and Choi, Yejin and Rohrbach, Anna},
  booktitle={NAACL},
  year={2022}
}

@inproceedings{hakeem2004ontology,
  title={Ontology and taxonomy collaborated framework for meeting classification},
  author={Hakeem, Asaad and Shah, Mubarak},
  booktitle={ICPR},
  year={2004},
}

@article{albanese2010pads,
  title={Pads: A probabilistic activity detection framework for video data},
  author={Albanese, Massimiliano and Chellappa, Rama and Cuntoor, Naresh and Moscato, Vincenzo and Picariello, Antonio and Subrahmanian, VS and Udrea, Octavian},
  journal={IEEE TPAMI},
  year={2010},
}

@article{zhu2023personality,
  title={Personality-aware Human-centric Multimodal Reasoning: A New Task, Dataset and Baselines},
  author={Zhu, Yaochen and Shen, Xiangqing and Xia, Rui},
  journal={arxiv},
  year={2023}
}

@inproceedings{cuturi2017soft,
  title={Soft-dtw: a differentiable loss function for time-series},
  author={Cuturi, Marco and Blondel, Mathieu},
  booktitle={ICML},
  year={2017},
}

@inproceedings{song2024moviechat,
  title={Moviechat: From dense token to sparse memory for long video understanding},
  author={Song, Enxin and Chai, Wenhao and Wang, Guanhong and Zhang, Yucheng and Zhou, Haoyang and Wu, Feiyang and Chi, Haozhe and Guo, Xun and Ye, Tian and Zhang, Yanting and others},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{monfort2021spoken,
  title={Spoken moments: Learning joint audio-visual representations from video descriptions},
  author={Monfort, Mathew and Jin, SouYoung and Liu, Alexander and Harwath, David and Feris, Rogerio and Glass, James and Oliva, Aude},
  booktitle={CVPR},
  year={2021}
}


%% ----- EAP ------

@inproceedings{cao2013recognize,
  title={Recognize human activities from partially observed videos},
  author={Cao, Yu and Barrett, Daniel and Barbu, Andrei and Narayanaswamy, Siddharth and Yu, Haonan and Michaux, Aaron and Lin, Yuewei and Dickinson, Sven and Mark Siskind, Jeffrey and Wang, Song},
  booktitle={CVPR},
  year={2013}
}


@inproceedings{lan2014hierarchical,
  title={A hierarchical representation for future action prediction},
  author={Lan, Tian and Chen, Tsung-Chuan and Savarese, Silvio},
  booktitle={ECCV},
  year={2014}
}

@article{li2014prediction,
  title={Prediction of human activity by discovering temporal sequence patterns},
  author={Li, Kang and Fu, Yun},
  journal={IEEE TPAMI},
  year={2014}
}

@inproceedings{li2012modeling,
  title={Modeling complex temporal composition of actionlets for activity prediction},
  author={Li, Kang and Hu, Jie and Fu, Yun},
  booktitle={ECCV},
  year={2012}
}

@inproceedings{xu2015activity,
  title={Activity auto-completion: Predicting human activities from partial videos},
  author={Xu, Zhen and Qing, Laiyun and Miao, Jun},
  booktitle={CVPR},
  year={2015}
}

@inproceedings{zhou2015temporal,
  title={Temporal perception and prediction in ego-centric video},
  author={Zhou, Yipin and Berg, Tamara L},
  booktitle={ICCV},
  year={2015}
}

@inproceedings{ryoo2011human,
  title={Human activity prediction: Early recognition of ongoing activities from streaming videos},
  author={Ryoo, Michael S},
  booktitle={ICCV},
  year={2011}
}

@article{hoai2014max,
  title={Max-margin early event detectors},
  author={Hoai, Minh and De la Torre, Fernando},
  journal={IJCV},
  year={2014}
}


@inproceedings{kong2014discriminative,
  title={A discriminative model with multiple temporal scales for action prediction},
  author={Kong, Yu and Kit, Dmitry and Fu, Yun},
  booktitle={ECCV},
  year={2014}
}


@inproceedings{pickup2014seeing,
  title={Seeing the arrow of time},
  author={Pickup, Lyndsey C and Pan, Zheng and Wei, Donglai and Shih, YiChang and Zhang, Changshui and Zisserman, Andrew and Scholkopf, Bernhard and Freeman, William T},
  booktitle={CVPR},
  year={2014}
}

@inproceedings{misra2016shuffle,
  title={Shuffle and learn: unsupervised learning using temporal order verification},
  author={Misra, Ishan and Zitnick, C Lawrence and Hebert, Martial},
  booktitle={ECCV},
  year={2016}
}

@inproceedings{babaeizadeh2018stochastic,
  title={Stochastic variational video prediction},
  author={Babaeizadeh, Mohammad and Finn, Chelsea and Erhan, Dumitru and Campbell, Roy H and Levine, Sergey},
  booktitle={ICLR},
  year={2018}
}

@inproceedings{hwang2019adversarial,
  title={Adversarial structure matching for structured prediction tasks},
  author={Hwang, Jyh-Jing and Ke, Tsung-Wei and Shi, Jianbo and Yu, Stella X},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{castrejon2019improved,
  title={Improved conditional vrnns for video prediction},
  author={Castrejon, Lluis and Ballas, Nicolas and Courville, Aaron},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{chang2022strpm,
  title={Strpm: A spatiotemporal residual predictive model for high-resolution video prediction},
  author={Chang, Zheng and Zhang, Xinfeng and Wang, Shanshe and Ma, Siwei and Gao, Wen},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{park2021vid,
  title={Vid-ode: Continuous-time video generation with neural ordinary differential equation},
  author={Park, Sunghyun and Kim, Kangyeol and Lee, Junsoo and Choo, Jaegul and Lee, Joonseok and Kim, Sookyung and Choi, Edward},
  booktitle={AAAI},
  year={2021}
}

@inproceedings{gao2022simvp,
  title={Simvp: Simpler yet better video prediction},
  author={Gao, Zhangyang and Tan, Cheng and Wu, Lirong and Li, Stan Z},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{zhong2023mmvp,
  title={Mmvp: Motion-matrix-based video prediction},
  author={Zhong, Yiqi and Liang, Luming and Zharkov, Ilya and Neumann, Ulrich},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{tang2024vmrnn,
  title={Vmrnn: Integrating vision mamba and lstm for efficient and accurate spatiotemporal forecasting},
  author={Tang, Yujin and Dong, Peijie and Tang, Zhenheng and Chu, Xiaowen and Liang, Junwei},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{gu2023seer,
  title={Seer: Language instructed video prediction with latent diffusion models},
  author={Gu, Xianfan and Wen, Chuan and Ye, Weirui and Song, Jiaming and Gao, Yang},
  booktitle={ICLR},
  year={2024}
}

@inproceedings{ning2023mimo,
  title={MIMO is all you need: A strong multi-in-multi-out baseline for video prediction},
  author={Ning, Shuliang and Lan, Mengcheng and Li, Yanran and Chen, Chaofeng and Chen, Qian and Chen, Xunlai and Han, Xiaoguang and Cui, Shuguang},
  booktitle={AAAI},
  year={2023}
}

@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={NeurIPS},
  year={2020}
}

@article{dhariwal2021diffusion,
  title={Diffusion models beat gans on image synthesis},
  author={Dhariwal, Prafulla and Nichol, Alexander},
  journal={NeurIPS},
  year={2021}
}

@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={CVPR},
  year={2022}
}

@article{lipman2022flow,
  title={Flow matching for generative modeling},
  author={Lipman, Yaron and Chen, Ricky TQ and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt},
  journal={arxiv},
  year={2022}
}

@inproceedings{liu2017video,
  title={Video frame synthesis using deep voxel flow},
  author={Liu, Ziwei and Yeh, Raymond A and Tang, Xiaoou and Liu, Yiming and Agarwala, Aseem},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{tan2023temporal,
  title={Temporal attention unit: Towards efficient spatiotemporal predictive learning},
  author={Tan, Cheng and Gao, Zhangyang and Wu, Lirong and Xu, Yongjie and Xia, Jun and Li, Siyuan and Li, Stan Z},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{hu2023dynamic,
  title={A dynamic multi-scale voxel flow network for video prediction},
  author={Hu, Xiaotao and Huang, Zhewei and Huang, Ailin and Xu, Jun and Zhou, Shuchang},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{nie2024triplet,
  title={Triplet attention transformer for spatiotemporal predictive learning},
  author={Nie, Xuesong and Chen, Xi and Jin, Haoyuan and Zhu, Zhihang and Yan, Yunfeng and Qi, Donglian},
  booktitle={WACV},
  year={2024}
}

@inproceedings{davtyan2023efficient,
  title={Efficient video prediction via sparsely conditioned flow matching},
  author={Davtyan, Aram and Sameni, Sepehr and Favaro, Paolo},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{shrivastava2024video,
  title={Video Prediction by Modeling Videos as Continuous Multi-Dimensional Processes},
  author={Shrivastava, Gaurav and Shrivastava, Abhinav},
  booktitle={CVPR},
  year={2024}
}

@article{harvey2022flexible,
  title={Flexible diffusion modeling of long videos},
  author={Harvey, William and Naderiparizi, Saeid and Masrani, Vaden and Weilbach, Christian and Wood, Frank},
  journal={NeurIPS},
  year={2022}
}

@inproceedings{guen2020disentangling,
  title={Disentangling physical dynamics from unknown factors for unsupervised video prediction},
  author={Guen, Vincent Le and Thome, Nicolas},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{zhang2023modeling,
  title={Modeling video as stochastic processes for fine-grained video representation learning},
  author={Zhang, Heng and Liu, Daqing and Zheng, Qi and Su, Bing},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{wu2021motionrnn,
  title={MotionRNN: A flexible model for video prediction with spacetime-varying motions},
  author={Wu, Haixu and Yao, Zhiyu and Wang, Jianmin and Long, Mingsheng},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{ye2022vptr,
  title={Vptr: Efficient transformers for video prediction},
  author={Ye, Xi and Bilodeau, Guillaume-Alexandre},
  booktitle={ICPR},
  year={2022},
}

@inproceedings{ge2022long,
  title={Long video generation with time-agnostic vqgan and time-sensitive transformer},
  author={Ge, Songwei and Hayes, Thomas and Yang, Harry and Yin, Xi and Pang, Guan and Jacobs, David and Huang, Jia-Bin and Parikh, Devi},
  booktitle={ECCV},
  year={2022},
}

@article{clark2019adversarial,
  title={Adversarial video generation on complex datasets},
  author={Clark, Aidan and Donahue, Jeff and Simonyan, Karen},
  journal={arxiv},
  year={2019}
}

@article{luc2020transformation,
  title={Transformation-based adversarial video prediction on large-scale data},
  author={Luc, Pauline and Clark, Aidan and Dieleman, Sander and Casas, Diego de Las and Doron, Yotam and Cassirer, Albin and Simonyan, Karen},
  journal={arxiv},
  year={2020}
}

@article{vondrick2016generating,
  title={Generating videos with scene dynamics},
  author={Vondrick, Carl and Pirsiavash, Hamed and Torralba, Antonio},
  journal={NeurIPS},
  year={2016}
}

@inproceedings{yugenerating,
  title={Generating Videos with Dynamics-aware Implicit Generative Adversarial Networks},
  author={Yu, Sihyun and Tack, Jihoon and Mo, Sangwoo and Kim, Hyunsu and Kim, Junho and Ha, Jung-Woo and Shin, Jinwoo},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{denton2018stochastic,
  title={Stochastic video generation with a learned prior},
  author={Denton, Emily and Fergus, Rob},
  booktitle={ICML},
  year={2018},
}

@article{smith2024convolutional,
  title={Convolutional state space models for long-range spatiotemporal modeling},
  author={Smith, Jimmy and De Mello, Shalini and Kautz, Jan and Linderman, Scott and Byeon, Wonmin},
  journal={NeurIPS},
  year={2024}
}

@inproceedings{chi2023adamsformer,
  title={Adamsformer for spatial action localization in the future},
  author={Chi, Hyung-gun and Lee, Kwonjoon and Agarwal, Nakul and Xu, Yi and Ramani, Karthik and Choi, Chiho},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{agarwal2020unsupervised,
  title={Unsupervised Domain Adaptation for Spatio-Temporal Action Localization},
  author={Agarwal, Nakul and Chen, Yi Ting and Dariush, Behzad and Yang, Ming Hsuan},
  booktitle={BMVC},
  year={2020}
}

@inproceedings{peng2016multi,
  title={Multi-region two-stream R-CNN for action detection},
  author={Peng, Xiaojiang and Schmid, Cordelia},
  booktitle={ECCV},
  year={2016},
}

@inproceedings{singh2017online,
  title={Online real-time multiple spatiotemporal action localisation and prediction},
  author={Singh, Gurkirt and Saha, Suman and Sapienza, Michael and Torr, Philip HS and Cuzzolin, Fabio},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{weinzaepfel2015learning,
  title={Learning to track for spatio-temporal action localization},
  author={Weinzaepfel, Philippe and Harchaoui, Zaid and Schmid, Cordelia},
  booktitle={ICCV},
  year={2015}
}

@inproceedings{zhang2019structured,
  title={A structured model for action detection},
  author={Zhang, Yubo and Tokmakov, Pavel and Hebert, Martial and Schmid, Cordelia},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{ulutan2020actor,
  title={Actor conditioned attention maps for video action detection},
  author={Ulutan, Oytun and Rallapalli, Swati and Srivatsa, Mudhakar and Torres, Carlos and Manjunath, BS},
  booktitle={WACV},
  year={2020}
}

@inproceedings{sun2018actor,
  title={Actor-centric relation network},
  author={Sun, Chen and Shrivastava, Abhinav and Vondrick, Carl and Murphy, Kevin and Sukthankar, Rahul and Schmid, Cordelia},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{feng2021relation,
  title={Relation modeling in spatio-temporal action localization},
  author={Feng, Yutong and Jiang, Jianwen and Huang, Ziyuan and Qing, Zhiwu and Wang, Xiang and Zhang, Shiwei and Tang, Mingqian and Gao, Yue},
  booktitle={CVPRw},
  year={2021}
}

@inproceedings{wang2018videos,
  title={Videos as space-time region graphs},
  author={Wang, Xiaolong and Gupta, Abhinav},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{pan2021actor,
  title={Actor-context-actor relation network for spatio-temporal action localization},
  author={Pan, Junting and Chen, Siyu and Shou, Mike Zheng and Liu, Yu and Shao, Jing and Li, Hongsheng},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{tang2020asynchronous,
  title={Asynchronous interaction aggregation for action detection},
  author={Tang, Jiajun and Xia, Jin and Mu, Xinzhi and Pang, Bo and Lu, Cewu},
  booktitle={ECCV},
  year={2020},
}

@inproceedings{li2018recurrent,
  title={Recurrent tubelet proposal and recognition networks for action detection},
  author={Li, Dong and Qiu, Zhaofan and Dai, Qi and Yao, Ting and Mei, Tao},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{li2020actions,
  title={Actions as moving points},
  author={Li, Yixuan and Wang, Zixu and Wang, Limin and Wu, Gangshan},
  booktitle={ECCV},
  year={2020},
}

@inproceedings{zhao2019dance,
  title={Dance with flow: Two-in-one stream action detection},
  author={Zhao, Jiaojiao and Snoek, Cees GM},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{song2019tacnet,
  title={Tacnet: Transition-aware context network for spatio-temporal action detection},
  author={Song, Lin and Zhang, Shiwei and Yu, Gang and Sun, Hongbin},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{jain2014action,
  title={Action localization with tubelets from motion},
  author={Jain, Mihir and Van Gemert, Jan and J{\'e}gou, Herv{\'e} and Bouthemy, Patrick and Snoek, Cees GM},
  booktitle={CVPR},
  year={2014}
}

@inproceedings{thoker2023tubelet,
    author={Thoker, Fida Mohammad and Doughty, Hazel and Snoek, Cees G. M.},
    title={Tubelet-Contrastive Self-Supervision for Video-Efficient Generalization},
    booktitle={ICCV},
    year={2023},
}

@inproceedings{hou2017tube,
  title={Tube convolutional neural network (t-cnn) for action detection in videos},
  author={Hou, Rui and Chen, Chen and Shah, Mubarak},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{yang2019step,
  title={Step: Spatio-temporal progressive learning for video action detection},
  author={Yang, Xitong and Yang, Xiaodong and Liu, Ming-Yu and Xiao, Fanyi and Davis, Larry S and Kautz, Jan},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{zhao2022tuber,
  title={Tuber: Tubelet transformer for video action detection},
  author={Zhao, Jiaojiao and Zhang, Yanyi and Li, Xinyu and Chen, Hao and Shuai, Bing and Xu, Mingze and Liu, Chunhui and Kundu, Kaustav and Xiong, Yuanjun and Modolo, Davide and others},
  booktitle={CVPR},
  year={2022}
}

@article{gao2017red,
  title={Red: Reinforced encoder-decoder networks for action anticipation},
  author={Gao, Jiyang and Yang, Zhenheng and Nevatia, Ram},
  journal={arxiv},
  year={2017}
}

@inproceedings{huang2014action,
  title={Action-reaction: Forecasting the dynamics of human interaction},
  author={Huang, De-An and Kitani, Kris M},
  booktitle={ECCV},
  year={2014}
}

@inproceedings{kuehne2014language,
  title={The language of actions: Recovering the syntax and semantics of goal-directed human activities},
  author={Kuehne, Hilde and Arslan, Ali and Serre, Thomas},
  booktitle={CVPR},
  year={2014}
}

@inproceedings{blattmann2023align,
  title={Align your latents: High-resolution video synthesis with latent diffusion models},
  author={Blattmann, Andreas and Rombach, Robin and Ling, Huan and Dockhorn, Tim and Kim, Seung Wook and Fidler, Sanja and Kreis, Karsten},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{nikankin2023sinfusion,
  title={SinFusion: training diffusion models on a single image or video},
  author={Nikankin, Yaniv and Haim, Niv and Irani, Michal},
  booktitle={ICML},
  year={2023}
}

@inproceedings{yang2023video,
  title={Video diffusion models with local-global context guidance},
  author={Yang, Siyuan and Zhang, Lu and Liu, Yu and Jiang, Zhizhuo and He, You},
  booktitle={IJCAI},
  year={2023}
}

@article{yang2023basictad,
  title={Basictad: an astounding rgb-only baseline for temporal action detection},
  author={Yang, Min and Chen, Guo and Zheng, Yin-Dong and Lu, Tong and Wang, Limin},
  journal={CVIU},
  year={2023},
}

@inproceedings{bai2020boundary,
  title={Boundary content graph neural network for temporal action proposal generation},
  author={Bai, Yueran and Wang, Yingying and Tong, Yunhai and Yang, Yang and Liu, Qiyue and Liu, Junhui},
  booktitle={ECCV},
  year={2020},
}

@inproceedings{tan2021relaxed,
  title={Relaxed transformer decoders for direct action proposal generation},
  author={Tan, Jing and Tang, Jiaqi and Wang, Limin and Wu, Gangshan},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{liu2022empirical,
  title={An empirical study of end-to-end temporal action detection},
  author={Liu, Xiaolong and Bai, Song and Bai, Xiang},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{cheng2022stochastic,
  title={Stochastic backpropagation: A memory efficient strategy for training video models},
  author={Cheng, Feng and Xu, Mingze and Xiong, Yuanjun and Chen, Hao and Li, Xinyu and Li, Wei and Xia, Wei},
  booktitle={CVPR},
  year={2022}
}

@article{ho2022video,
  title={Video diffusion models},
  author={Ho, Jonathan and Salimans, Tim and Gritsenko, Alexey and Chan, William and Norouzi, Mohammad and Fleet, David J},
  journal={NeurIPS},
  year={2022}
}

@article{voleti2022mcvd,
  title={Mcvd-masked conditional video diffusion for prediction, generation, and interpolation},
  author={Voleti, Vikram and Jolicoeur-Martineau, Alexia and Pal, Chris},
  journal={NeurIPS},
  year={2022}
}

@article{hoppe2024diffusion,
  title={Diffusion models for video prediction and infilling},
  author={H{\"o}ppe, Tobias and Mehrjou, Arash and Bauer, Stefan and Nielsen, Didrik and Dittadi, Andrea},
  journal={IEEE TMLR},
  year={2024}
}

@article{he2022latent,
  title={Latent video diffusion models for high-fidelity video generation with arbitrary lengths},
  author={He, Yingqing and Yang, Tianyu and Zhang, Yong and Shan, Ying and Chen, Qifeng},
  journal={arxiv},
  year={2022}
}


@inproceedings{zhang2024mart,
  title={Mart: Masked affective representation learning via masked temporal distribution distillation},
  author={Zhang, Zhicheng and Zhao, Pancheng and Park, Eunil and Yang, Jufeng},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{singer2023make,
  title={Make-a-video: Text-to-video generation without text-video data},
  author={Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and others},
  booktitle={ICLR},
  year={2023}
}

@inproceedings{sermanet2018time,
  title={Time-contrastive networks: Self-supervised learning from video},
  author={Sermanet, Pierre and Lynch, Corey and Chebotar, Yevgen and Hsu, Jasmine and Jang, Eric and Schaal, Stefan and Levine, Sergey and Brain, Google},
  booktitle={ICRA},
  year={2018},
}

@inproceedings{wu2023tune,
  title={Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation},
  author={Wu, Jay Zhangjie and Ge, Yixiao and Wang, Xintao and Lei, Stan Weixian and Gu, Yuchao and Shi, Yufei and Hsu, Wynne and Shan, Ying and Qie, Xiaohu and Shou, Mike Zheng},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{wu2020not,
  title={Not only look, but also listen: Learning multimodal violence detection under weak supervision},
  author={Wu, Peng and Liu, Jing and Shi, Yujia and Sun, Yujia and Shao, Fangtao and Wu, Zhaoyang and Yang, Zhiwei},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{liu2018future,
  title={Future frame prediction for anomaly detection--a new baseline},
  author={Liu, Wen and Luo, Weixin and Lian, Dongze and Gao, Shenghua},
  booktitle={CVPR},
  year={2018}
}


@inproceedings{lu2013abnormal,
  title={Abnormal event detection at 150 fps in matlab},
  author={Lu, Cewu and Shi, Jianping and Jia, Jiaya},
  booktitle={ICCV},
  year={2013}
}

@inproceedings{ghodrati2021frameexit,
  title={Frameexit: Conditional early exiting for efficient video recognition},
  author={Ghodrati, Amir and Bejnordi, Babak Ehteshami and Habibian, Amirhossein},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{radevski2023multimodal,
  title={Multimodal distillation for egocentric action recognition},
  author={Radevski, Gorjan and Grujicic, Dusan and Blaschko, Matthew and Moens, Marie-Francine and Tuytelaars, Tinne},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{yang2023vid2seq,
  title={Vid2seq: Large-scale pretraining of a visual language model for dense video captioning},
  author={Yang, Antoine and Nagrani, Arsha and Seo, Paul Hongsuck and Miech, Antoine and Pont-Tuset, Jordi and Laptev, Ivan and Sivic, Josef and Schmid, Cordelia},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{xu2021videoclip,
  title={Videoclip: Contrastive pre-training for zero-shot video-text understanding},
  author={Xu, Hu and Ghosh, Gargi and Huang, Po-Yao and Okhonko, Dmytro and Aghajanyan, Armen and Metze, Florian and Zettlemoyer, Luke and Feichtenhofer, Christoph},
  booktitle={EMNLP},
  year={2021}
}

@article{li2023videochat,
  title={Videochat: Chat-centric video understanding},
  author={Li, KunChang and He, Yinan and Wang, Yi and Li, Yizhuo and Wang, Wenhai and Luo, Ping and Wang, Yali and Wang, Limin and Qiao, Yu},
  journal={arxiv},
  year={2023}
}

@article{jiang2023motiongpt,
  title={Motiongpt: Human motion as a foreign language},
  author={Jiang, Biao and Chen, Xin and Liu, Wen and Yu, Jingyi and Yu, Gang and Chen, Tao},
  journal={NeurIPS},
  year={2023}
}


@article{yang2024vidchapters,
  title={Vidchapters-7m: Video chapters at scale},
  author={Yang, Antoine and Nagrani, Arsha and Laptev, Ivan and Sivic, Josef and Schmid, Cordelia},
  journal={NeurIPS},
  year={2024}
}

@inproceedings{wang2024omnivid,
  title={Omnivid: A generative framework for universal video understanding},
  author={Wang, Junke and Chen, Dongdong and Luo, Chong and He, Bo and Yuan, Lu and Wu, Zuxuan and Jiang, Yu-Gang},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{bansal2024videocon,
  title={Videocon: Robust video-language alignment via contrast captions},
  author={Bansal, Hritik and Bitton, Yonatan and Szpektor, Idan and Chang, Kai-Wei and Grover, Aditya},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{zhou2024streaming,
  title={Streaming dense video captioning},
  author={Zhou, Xingyi and Arnab, Anurag and Buch, Shyamal and Yan, Shen and Myers, Austin and Xiong, Xuehan and Nagrani, Arsha and Schmid, Cordelia},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{han2023autoad,
  title={AutoAD: Movie description in context},
  author={Han, Tengda and Bain, Max and Nagrani, Arsha and Varol, G{\"u}l and Xie, Weidi and Zisserman, Andrew},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{han2023autoadii,
  title={Autoad ii: The sequel-who, when, and what in movie audio description},
  author={Han, Tengda and Bain, Max and Nagrani, Arsha and Varol, Gul and Xie, Weidi and Zisserman, Andrew},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{han2024autoadiii,
  title={AutoAD III: The Prequel-Back to the Pixels},
  author={Han, Tengda and Bain, Max and Nagrani, Arsha and Varol, G{\"u}l and Xie, Weidi and Zisserman, Andrew},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{chen2024panda,
  title={Panda-70m: Captioning 70m videos with multiple cross-modality teachers},
  author={Chen, Tsai-Shien and Siarohin, Aliaksandr and Menapace, Willi and Deyneka, Ekaterina and Chao, Hsiang-wei and Jeon, Byung Eun and Fang, Yuwei and Lee, Hsin-Ying and Ren, Jian and Yang, Ming-Hsuan and others},
  booktitle={CVPR},
  year={2024}
}


@inproceedings{ren2024timechat,
  title={Timechat: A time-sensitive multimodal large language model for long video understanding},
  author={Ren, Shuhuai and Yao, Linli and Li, Shicheng and Sun, Xu and Hou, Lu},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{yan2023unloc,
  title={Unloc: A unified framework for video localization tasks},
  author={Yan, Shen and Xiong, Xuehan and Nagrani, Arsha and Arnab, Anurag and Wang, Zhonghao and Ge, Weina and Ross, David and Schmid, Cordelia},
  booktitle={ICCV},
  year={2023}
}

@article{tan2023egodistill,
  title={Egodistill: Egocentric head motion distillation for efficient video understanding},
  author={Tan, Shuhan and Nagarajan, Tushar and Grauman, Kristen},
  journal={NeurIPS},
  year={2023}
}

@inproceedings{wu2019adaframe,
  title={Adaframe: Adaptive frame selection for fast video recognition},
  author={Wu, Zuxuan and Xiong, Caiming and Ma, Chih-Yao and Socher, Richard and Davis, Larry S},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{wang2022adafocus,
  title={Adafocus v2: End-to-end training of spatial dynamic networks for video recognition},
  author={Wang, Yulin and Yue, Yang and Lin, Yuanze and Jiang, Haojun and Lai, Zihang and Kulikov, Victor and Orlov, Nikita and Shi, Humphrey and Huang, Gao},
  booktitle={CVPR},
  year={2022},
}

@inproceedings{xia2022nsnet,
  title={Nsnet: Non-saliency suppression sampler for efficient video recognition},
  author={Xia, Boyang and Wu, Wenhao and Wang, Haoran and Su, Rui and He, Dongliang and Yang, Haosen and Fan, Xiaoran and Ouyang, Wanli},
  booktitle={ECCV},
  year={2022},
}

@inproceedings{lin2022ocsampler,
  title={Ocsampler: Compressing videos to one clip with single-step sampling},
  author={Lin, Jintao and Duan, Haodong and Chen, Kai and Lin, Dahua and Wang, Limin},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{xia2022temporal,
  title={Temporal saliency query network for efficient video recognition},
  author={Xia, Boyang and Wang, Zhihao and Wu, Wenhao and Wang, Haoran and Han, Jungong},
  booktitle={ECCV},
  year={2022},
}

@inproceedings{nugroho2023audio,
  title={Audio-visual glance network for efficient video recognition},
  author={Nugroho, Muhammad Adi and Woo, Sangmin and Lee, Sumin and Kim, Changick},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{gowda2021smart,
  title={Smart frame selection for action recognition},
  author={Gowda, Shreyank N and Rohrbach, Marcus and Sevilla-Lara, Laura},
  booktitle={AAAI},
  year={2021}
}

@inproceedings{srivastava2024omnivec,
  title={Omnivec: Learning robust representations with cross modal sharing},
  author={Srivastava, Siddharth and Sharma, Gaurav},
  booktitle={WACV},
  year={2024}
}

@inproceedings{lei2021less,
  title={Less is more: Clipbert for video-and-language learning via sparse sampling},
  author={Lei, Jie and Li, Linjie and Zhou, Luowei and Gan, Zhe and Berg, Tamara L and Bansal, Mohit and Liu, Jingjing},
  booktitle={CVPR},
  year={2021}
}

@article{wu2019liteeval,
  title={Liteeval: A coarse-to-fine framework for resource efficient video recognition},
  author={Wu, Zuxuan and Xiong, Caiming and Jiang, Yu-Gang and Davis, Larry S},
  journal={NeurIPS},
  year={2019}
}

@inproceedings{gao2020listen,
  title={Listen to look: Action recognition by previewing audio},
  author={Gao, Ruohan and Oh, Tae-Hyun and Grauman, Kristen and Torresani, Lorenzo},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{korbar2019scsampler,
  title={Scsampler: Sampling salient clips from video for efficient action recognition},
  author={Korbar, Bruno and Tran, Du and Torresani, Lorenzo},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{meng2020ar,
  title={Ar-net: Adaptive frame resolution for efficient action recognition},
  author={Meng, Yue and Lin, Chung-Ching and Panda, Rameswar and Sattigeri, Prasanna and Karlinsky, Leonid and Oliva, Aude and Saenko, Kate and Feris, Rogerio},
  booktitle={ECCV},
  year={2020},
}

@inproceedings{wu2022nuwa,
  title={N{\"u}wa: Visual synthesis pre-training for neural visual world creation},
  author={Wu, Chenfei and Liang, Jian and Ji, Lei and Yang, Fan and Fang, Yuejian and Jiang, Daxin and Duan, Nan},
  booktitle={ECCV},
  year={2022},
}

@inproceedings{villegas2022phenaki,
  title={Phenaki: Variable length video generation from open domain textual descriptions},
  author={Villegas, Ruben and Babaeizadeh, Mohammad and Kindermans, Pieter-Jan and Moraldo, Hernan and Zhang, Han and Saffar, Mohammad Taghi and Castro, Santiago and Kunze, Julius and Erhan, Dumitru},
  booktitle={ICLR},
  year={2022}
}

@inproceedings{chen2017learning,
  title={Learning object-centric transformation for video prediction},
  author={Chen, Xiongtao and Wang, Wenmin and Wang, Jinzhuo and Li, Weimian},
  booktitle={MM},
  year={2017}
}

@inproceedings{jin2017video,
  title={Video scene parsing with predictive feature learning},
  author={Jin, Xiaojie and Li, Xin and Xiao, Huaxin and Shen, Xiaohui and Lin, Zhe and Yang, Jimei and Chen, Yunpeng and Dong, Jian and Liu, Luoqi and Jie, Zequn and others},
  booktitle={ICCV},
  year={2017}
}


@inproceedings{liang2017dual,
  title={Dual motion GAN for future-flow embedded video prediction},
  author={Liang, Xiaodan and Lee, Lisa and Dai, Wei and Xing, Eric P},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{cai2019action,
  title={Action knowledge transfer for action prediction with partial videos},
  author={Cai, Yijun and Li, Haoxin and Hu, Jian-Fang and Zheng, Wei-Shi},
  booktitle={AAAI},
  year={2019}
}

@inproceedings{fernando2021anticipating,
  title={Anticipating human actions by correlating past with the future with jaccard similarity measures},
  author={Fernando, Basura and Herath, Samitha},
  booktitle={CVPR},
  year={2021}
}

@article{hou2020confidence,
  title={Confidence-guided self refinement for action prediction in untrimmed videos},
  author={Hou, Jingyi and Wu, Xinxiao and Wang, Ruiqi and Luo, Jiebo and Jia, Yunde},
  journal={IEEE T-IP},
  year={2020}
}


@article{hu2018early,
  title={Early action prediction by soft regression},
  author={Hu, Jian-Fang and Zheng, Wei-Shi and Ma, Lianyang and Wang, Gang and Lai, Jianhuang and Zhang, Jianguo},
  journal={IEEE TPAMI},
  year={2018}
}

@inproceedings{kong2018action,
  title={Action prediction from videos via memorizing hard-to-predict samples},
  author={Kong, Yu and Gao, Shangqian and Sun, Bin and Fu, Yun},
  booktitle={AAAI},
  year={2018}
}

@inproceedings{stergiou2023wisdom,
  title={The wisdom of crowds: Temporal progressive attention for early action prediction},
  author={Stergiou, Alexandros and Damen, Dima},
  booktitle={CVPR},
  year={2023}
}


@inproceedings{xu2019prediction,
  title={Prediction-cgan: Human action prediction with conditional generative adversarial networks},
  author={Xu, Wanru and Yu, Jian and Miao, Zhenjiang and Wan, Lili and Ji, Qiang},
  booktitle={MM},
  year={2019}
}

@inproceedings{zhao2019spatiotemporal,
  title={Spatiotemporal feature residual propagation for action prediction},
  author={Zhao, He and Wildes, Richard P},
  booktitle={ICCV},
  year={2019}
}


@inproceedings{wang2019progressive,
  title={Progressive teacher-student learning for early action prediction},
  author={Wang, Xionghui and Hu, Jian-Fang and Lai, Jian-Huang and Zhang, Jianguo and Zheng, Wei-Shi},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{foo2022era,
  title={Era: Expert retrieval and assembly for early action prediction},
  author={Foo, Lin Geng and Li, Tianjiao and Rahmani, Hossein and Ke, Qiuhong and Liu, Jun},
  booktitle={ECCV},
  year={2022},
}

@article{wang2023magi,
  title={Magi-net: Meta negative network for early activity prediction},
  author={Wang, Wenqian and Chang, Faliang and Zhang, Junhao and Yan, Rui and Liu, Chunsheng and Wang, Bin and Shou, Mike Zheng},
  journal={IEEE T-IP},
  year={2023}
}

@article{xu2023dynamic,
  title={Dynamic Context Removal: A General Training Strategy for Robust Models on Video Action Predictive Tasks},
  author={Xu, Xinyu and Li, Yong-Lu and Lu, Cewu},
  journal={IJCV},
  year={2023},
}

@article{wu2021spatial,
  title={Spatial--temporal relation reasoning for action prediction in videos},
  author={Wu, Xinxiao and Wang, Ruiqi and Hou, Jingyi and Lin, Hanxi and Luo, Jiebo},
  journal={IJCV},
  year={2021},
}

@inproceedings{wu2021anticipating,
  title={Anticipating future relations via graph growing for action prediction},
  author={Wu, Xinxiao and Zhao, Jianwei and Wang, Ruiqi},
  booktitle={AAAI},
  year={2021}
}

@article{chen2022ambiguousness,
  title={Ambiguousness-aware state evolution for action prediction},
  author={Chen, Lei and Lu, Jiwen and Song, Zhanjie and Zhou, Jie},
  journal={IEEE TCSVT},
  year={2022},
}

@inproceedings{zhang2024extdm,
  title={Extdm: Distribution extrapolation diffusion model for video prediction},
  author={Zhang, Zhicheng and Hu, Junyao and Cheng, Wentao and Paudel, Danda and Yang, Jufeng},
  booktitle={CVPR},
  year={2024}
}

@article{zheng2023egocentric,
  title={Egocentric early action prediction via adversarial knowledge distillation},
  author={Zheng, Na and Song, Xuemeng and Su, Tianyu and Liu, Weifeng and Yan, Yan and Nie, Liqiang},
  journal={ACM TOMM},
  year={2023}
}


%% ----- Anticipation -----
@inproceedings{villegas2018hierarchical,
  title={Hierarchical long-term video prediction without supervision},
  author={Villegas, Ruben and Erhan, Dumitru and Lee, Honglak and others},
  booktitle={ICML},
  year={2018}
}

@inproceedings{villegas2017learning,
  title={Learning to generate long-term future via hierarchical prediction},
  author={Villegas, Ruben and Yang, Jimei and Zou, Yuliang and Sohn, Sungryull and Lin, Xunyu and Lee, Honglak},
  booktitle={ICML},
  year={2017}
}

@inproceedings{gong2022future,
  title={Future transformer for long-term action anticipation},
  author={Gong, Dayoung and Lee, Joonseok and Kim, Manjin and Ha, Seong Jong and Cho, Minsu},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{lee2019set,
  title={Set transformer: A framework for attention-based permutation-invariant neural networks},
  author={Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam and Choi, Seungjin and Teh, Yee Whye},
  booktitle={ICML},
  year={2019}
}

@inproceedings{abu2018will,
  title={When will you do what?-anticipating temporal occurrences of activities},
  author={Abu Farha, Yazan and Richard, Alexander and Gall, Juergen},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{ye2024stdiff,
  title={STDiff: Spatio-Temporal Diffusion for Continuous Stochastic Video Prediction},
  author={Ye, Xi and Bilodeau, Guillaume-Alexandre},
  booktitle={AAAI},
  year={2024}
}


@inproceedings{furnari2019would,
  title={What would you expect? anticipating egocentric actions with rolling-unrolling lstms and modality attention},
  author={Furnari, Antonino and Farinella, Giovanni Maria},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{vondrick2016anticipating,
  title={Anticipating visual representations from unlabeled video},
  author={Vondrick, Carl and Pirsiavash, Hamed and Torralba, Antonio},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{girdhar2021anticipative,
  title={Anticipative video transformer},
  author={Girdhar, Rohit and Grauman, Kristen},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{zhong2023anticipative,
  title={Anticipative feature fusion transformer for multi-modal action anticipation},
  author={Zhong, Zeyun and Schneider, David and Voit, Michael and Stiefelhagen, Rainer and Beyerer, J{\"u}rgen},
  booktitle={WACV},
  year={2023}
}

@article{wu2020learning,
  title={Learning to anticipate egocentric actions by imagination},
  author={Wu, Yu and Zhu, Linchao and Wang, Xiaohan and Yang, Yi and Wu, Fei},
  journal={IEEE T-IP},
  year={2020}
}

@article{dessalene2021forecasting,
  title={Forecasting action through contact representations from first person video},
  author={Dessalene, Eadom and Devaraj, Chinmaya and Maynord, Michael and Ferm{\"u}ller, Cornelia and Aloimonos, Yiannis},
  journal={IEEE TPAMI},
  year={2021}
}

@inproceedings{gammulle2019predicting,
  title={Predicting the future: A jointly learnt model for action anticipation},
  author={Gammulle, Harshala and Denman, Simon and Sridharan, Sridha and Fookes, Clinton},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{furnari2018leveraging,
  title={Leveraging uncertainty to rethink loss functions and evaluation measures for egocentric action anticipation},
  author={Furnari, Antonino and Battiato, Sebastiano and Maria Farinella, Giovanni},
  booktitle={ECCVw},
  year={2018}
}

@inproceedings{guo2024uncertainty,
  title={Uncertainty-aware Action Decoupling Transformer for Action Anticipation},
  author={Guo, Hongji and Agarwal, Nakul and Lo, Shao-Yuan and Lee, Kwonjoon and Ji, Qiang},
  booktitle={CVPR},
  year={2024}
}


@inproceedings{nawhal2022rethinking,
  title={Rethinking learning approaches for long-term action anticipation},
  author={Nawhal, Megha and Jyothi, Akash Abdu and Mori, Greg},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{kataoka2016recognition,
  title={Recognition of Transitional Action for Short-Term Action Prediction using Discriminative Temporal CNN Feature.},
  author={Kataoka, Hirokatsu and Miyashita, Yudai and Hayashi, Masaki and Iwata, Kenji and Satoh, Yutaka},
  booktitle={BMVC},
  year={2016}
}

@inproceedings{abdelsalam2023gepsan,
  title={Gepsan: Generative procedure step anticipation in cooking videos},
  author={Abdelsalam, Mohamed A and Rangrej, Samrudhdhi B and Hadji, Isma and Dvornik, Nikita and Derpanis, Konstantinos G and Fazly, Afsaneh},
  booktitle={ICCV},
  year={2023}
}

@article{roy2021action,
  title={Action anticipation using pairwise human-object interactions and transformers},
  author={Roy, Debaditya and Fernando, Basura},
  journal={IEEE T-IP},
  year={2021}
}

@inproceedings{roy2022action,
  title={Action anticipation using latent goal learning},
  author={Roy, Debaditya and Fernando, Basura},
  booktitle={WACV},
  year={2022}
}

@inproceedings{epstein2021learning,
  title={Learning temporal dynamics from cycles in narrated video},
  author={Epstein, Dave and Wu, Jiajun and Schmid, Cordelia and Sun, Chen},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{shen2018egocentric,
  title={Egocentric activity prediction via event modulated attention},
  author={Shen, Yang and Ni, Bingbing and Li, Zefan and Zhuang, Ning},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{piergiovanni2020adversarial,
  title={Adversarial generative grammars for human activity prediction},
  author={Piergiovanni, AJ and Angelova, Anelia and Toshev, Alexander and Ryoo, Michael S},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{zhao2020diverse,
  title={On diverse asynchronous activity anticipation},
  author={Zhao, He and Wildes, Richard P},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{sun2019relational,
  title={Relational action forecasting},
  author={Sun, Chen and Shrivastava, Abhinav and Vondrick, Carl and Sukthankar, Rahul and Murphy, Kevin and Schmid, Cordelia},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{liu2022hybrid,
  title={A hybrid egocentric activity anticipation framework via memory-augmented recurrent and one-shot representation forecasting},
  author={Liu, Tianshan and Lam, Kin-Man},
  booktitle={CVPR},
  year={2022}
}


@inproceedings{mascaro2023intention,
  title={Intention-conditioned long-term human egocentric action anticipation},
  author={Mascar{\'o}, Esteve Valls and Ahn, Hyemin and Lee, Dongheui},
  booktitle={WACV},
  year={2023}
}

@inproceedings{huang2018makes,
  title={What makes a video a video: Analyzing temporal information in video understanding models and datasets},
  author={Huang, De-An and Ramanathan, Vignesh and Mahajan, Dhruv and Torresani, Lorenzo and Paluri, Manohar and Fei-Fei, Li and Niebles, Juan Carlos},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{roy2024interaction,
  title={Interaction region visual transformer for egocentric action anticipation},
  author={Roy, Debaditya and Rajendiran, Ramanathan and Fernando, Basura},
  booktitle={WACV},
  year={2024}
}


%% ----- Pose and Tracking papers -----

@article{ormoneit2000learning,
  title={Learning and tracking cyclic human motion},
  author={Ormoneit, Dirk and Sidenbladh, Hedvig and Black, Michael and Hastie, Trevor},
  journal={NeurIPS},
  year={2000}
}

@inproceedings{stergiou2024holistic,
  title={Holistic representation learning for multitask trajectory anomaly detection},
  author={Stergiou, Alexandros and De Weerdt, Brent and Deligiannis, Nikos},
  booktitle={WACV},
  year={2024}
}

%% ----- END of Pose and TRacking papers -----


%% ----- Multimodal (more than 2 modalities) ----- 

@inproceedings{girdhar2022omnivore,
  title={Omnivore: A single model for many visual modalities},
  author={Girdhar, Rohit and Singh, Mannat and Ravi, Nikhila and Van Der Maaten, Laurens and Joulin, Armand and Misra, Ishan},
  booktitle={CVPR},
  year={2022}
}

@article{liang2022mind,
  title={Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning},
  author={Liang, Victor Weixin and Zhang, Yuhui and Kwon, Yongchan and Yeung, Serena and Zou, James Y},
  journal={NeurIPS},
  year={2022}
}



%%% Cognitive neuroscience

@article{jeannerod1994representing,
  title={The representing brain: Neural correlates of motor intention and imagery},
  author={Jeannerod, Marc},
  journal={BBS},
  year={1994},
}

@article{gallese1996action,
  title={Action recognition in the premotor cortex},
  author={Gallese, Vittorio and Fadiga, Luciano and Fogassi, Leonardo and Rizzolatti, Giacomo},
  journal={Brain},
  year={1996}
}

@article{calvo2005action,
  title={Action observation and acquired motor skills: an FMRI study with expert dancers},
  author={Calvo-Merino, Beatriz and Glaser, Daniel E and Gr{\`e}zes, Julie and Passingham, Richard E and Haggard, Patrick},
  journal={Cerebral cortex},
  year={2005}
}

@inproceedings{patsch2024long,
  title={Long-Term Action Anticipation Based on Contextual Alignment},
  author={Patsch, Constantin and Zhang, Jinghan and Wu, Yuankai and Zakour, Marsil and Salihu, Driton and Steinbach, Eckehard},
  booktitle={ICASSP},
  year={2024}
}

@article{kohler2002hearing,
  title={Hearing sounds, understanding actions: action representation in mirror neurons},
  author={Kohler, Evelyne and Keysers, Christian and Umilta, M Alessandra and Fogassi, Leonardo and Gallese, Vittorio and Rizzolatti, Giacomo},
  journal={Science},
  year={2002}
}

@article{li2024efficient,
  title={Efficient Action Counting with Dynamic Queries},
  author={Li, Zishi and Ma, Xiaoxuan and Shang, Qiuyan and Zhu, Wentao and Ci, Hai and Qiao, Yu and Wang, Yizhou},
  journal={arxiv},
  year={2024}
}

@article{zhao2024skim,
  title={Skim then Focus: Integrating Contextual and Fine-grained Views for Repetitive Action Counting},
  author={Zhao, Zhengqi and Huang, Xiaohu and Zhou, Hao and Yao, Kun and Ding, Errui and Wang, Jingdong and Wang, Xinggang and Liu, Wenyu and Feng, Bin},
  journal={arxiv},
  year={2024}
}

@inproceedings{wu2023stmixer,
  title={Stmixer: A one-stage sparse action detector},
  author={Wu, Tao and Cao, Mengqi and Gao, Ziteng and Wu, Gangshan and Wang, Limin},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{kumar2022end,
  title={End-to-end semi-supervised learning for video action detection},
  author={Kumar, Akash and Rawat, Yogesh Singh},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{sun2022dancetrack,
  title={Dancetrack: Multi-object tracking in uniform appearance and diverse motion},
  author={Sun, Peize and Cao, Jinkun and Jiang, Yi and Yuan, Zehuan and Bai, Song and Kitani, Kris and Luo, Ping},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{cui2023sportsmot,
  title={Sportsmot: A large multi-object tracking dataset in multiple sports scenes},
  author={Cui, Yutao and Zeng, Chenkai and Zhao, Xiaoyu and Yang, Yichun and Wu, Gangshan and Wang, Limin},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{ntinou2024multiscale,
  title={Multiscale vision transformers meet bipartite matching for efficient single-stage action localization},
  author={Ntinou, Ioanna and Sanchez, Enrique and Tzimiropoulos, Georgios},
  booktitle={CVPR},
  year={2024}
}


@inproceedings{li2022scale,
  title={Scale-aware spatio-temporal relation learning for video anomaly detection},
  author={Li, Guoqiu and Cai, Guanxiong and Zeng, Xingyu and Zhao, Rui},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{dave2022gabriellav2,
  title={Gabriellav2: Towards better generalization in surveillance videos for action detection},
  author={Dave, Ishan and Scheffer, Zacchaeus and Kumar, Akash and Shiraz, Sarah and Rawat, Yogesh Singh and Shah, Mubarak},
  booktitle={WACV},
  year={2022}
}

@inproceedings{xue2023egocentric,
  title={Egocentric video task translation},
  author={Xue, Zihui and Song, Yale and Grauman, Kristen and Torresani, Lorenzo},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{aakur2019perceptual,
  title={A perceptual prediction framework for self supervised event segmentation},
  author={Aakur, Sathyanarayanan N and Sarkar, Sudeep},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{shou2021generic,
  title={Generic event boundary detection: A benchmark for event segmentation},
  author={Shou, Mike Zheng and Lei, Stan Weixian and Wang, Weiyao and Ghadiyaram, Deepti and Feiszli, Matt},
  booktitle={ICCV},
  year={2021}
}

@article{mounir2024streamer,
  title={STREAMER: Streaming representation learning and event segmentation in a hierarchical manner},
  author={Mounir, Ramy and Vijayaraghavan, Sujal and Sarkar, Sudeep},
  journal={NeurIPS},
  year={2024}
}

@inproceedings{kang2022uboco,
  title={Uboco: Unsupervised boundary contrastive learning for generic event boundary detection},
  author={Kang, Hyolim and Kim, Jinwoo and Kim, Taehyun and Kim, Seon Joo},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{wu2023newsnet,
  title={Newsnet: A novel dataset for hierarchical temporal segmentation},
  author={Wu, Haoqian and Chen, Keyu and Liu, Haozhe and Zhuge, Mingchen and Li, Bing and Qiao, Ruizhi and Shu, Xiujun and Gan, Bei and Xu, Liangsheng and Ren, Bo and others},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{hadji2021representation,
  title={Representation learning via global temporal alignment and cycle-consistency},
  author={Hadji, Isma and Derpanis, Konstantinos G and Jepson, Allan D},
  booktitle={CVPR},
  year={2021}
}


@article{ashutosh2024video,
  title={Video-mined task graphs for keystep recognition in instructional videos},
  author={Ashutosh, Kumar and Ramakrishnan, Santhosh Kumar and Afouras, Triantafyllos and Grauman, Kristen},
  journal={NeurIPS},
  year={2024}
}

@inproceedings{zhou2023procedure,
  title={Procedure-aware pretraining for instructional video understanding},
  author={Zhou, Honglu and Mart{\'\i}n-Mart{\'\i}n, Roberto and Kapadia, Mubbasir and Savarese, Silvio and Niebles, Juan Carlos},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{ong2023chaotic,
  title={Chaotic World: A Large and Challenging Benchmark for Human Behavior Understanding in Chaotic Events},
  author={Ong, Kian Eng and Ng, Xun Long and Li, Yanchao and Ai, Wenjie and Zhao, Kuangyi and Yeo, Si Yong and Liu, Jun},
  booktitle={ICCV},
  year={2023}
}

@article{rizzolatti2001neurophysiological,
  title={Neurophysiological mechanisms underlying the understanding and imitation of action},
  author={Rizzolatti, Giacomo and Fogassi, Leonardo and Gallese, Vittorio},
  journal={Nature reviews neuroscience},
  year={2001}
}

@article{spunt2011identifying,
  title={Identifying the what, why, and how of an observed action: an fMRI study of mentalizing and mechanizing during action observation},
  author={Spunt, Robert P and Satpute, Ajay B and Lieberman, Matthew D},
  journal={JCN},
  year={2011},
}

@article{fogassi2005parietal,
  title={Parietal lobe: from action organization to intention understanding},
  author={Fogassi, Leonardo and Ferrari, Pier Francesco and Gesierich, Benno and Rozzi, Stefano and Chersi, Fabian and Rizzolatti, Giacomo},
  journal={Science},
  year={2005},
}

@article{kilner2011more,
  title={More than one pathway to action understanding},
  author={Kilner, James M},
  journal={Trends in cognitive sciences},
  year={2011},
}

@article{uithol2011understanding,
  title={Understanding motor resonance},
  author={Uithol, Sebo and van Rooij, Iris and Bekkering, Harold and Haselager, Pim},
  journal={Social neuroscience},
  year={2011}
}

@article{thompson2019conceptualizing,
  title={Conceptualizing and testing action understanding},
  author={Thompson, Emma L and Bird, Geoffrey and Catmur, Caroline},
  journal={NBR},
  year={2019}
}

@inproceedings{huang2020inset,
  title={INSET: Sentence Infilling with INter-SEntential Transformer},
  author={Huang, Yichen and Zhang, Yizhe and Elachqar, Oussama and Cheng, Yu},
  booktitle={ACL},
  year={2020}
}

@inproceedings{ippolito2019unsupervised,
  title={Unsupervised hierarchical story infilling},
  author={Ippolito, Daphne and Grangier, David and Callison-Burch, Chris and Eck, Douglas},
  booktitle={WNU},
  year={2019}
}

@inproceedings{saini2022disentangling,
  title={Disentangling visual embeddings for attributes and objects},
  author={Saini, Nirat and Pham, Khoi and Shrivastava, Abhinav},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{nagarajan2019grounded,
  title={Grounded human-object interaction hotspots from video},
  author={Nagarajan, Tushar and Feichtenhofer, Christoph and Grauman, Kristen},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{zhuo2019explainable,
  title={Explainable video action reasoning via prior knowledge and state transitions},
  author={Zhuo, Tao and Cheng, Zhiyong and Zhang, Peng and Wong, Yongkang and Kankanhalli, Mohan},
  booktitle={MM},
  year={2019}
}

@inproceedings{wang2021adaptive,
  title={Adaptive focus for efficient video recognition},
  author={Wang, Yulin and Chen, Zhaoxi and Jiang, Haojun and Song, Shiji and Han, Yizeng and Huang, Gao},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{wang2022adafocusv3,
  title={Adafocusv3: On unified spatial-temporal dynamic video recognition},
  author={Wang, Yulin and Yue, Yang and Xu, Xinhong and Hassani, Ali and Kulikov, Victor and Orlov, Nikita and Song, Shiji and Shi, Humphrey and Huang, Gao},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{hong2021transformation,
  title={Transformation driven visual reasoning},
  author={Hong, Xin and Lan, Yanyan and Pang, Liang and Guo, Jiafeng and Cheng, Xueqi},
  booktitle={CVPR},
  year={2021}
}

@article{thakur2024anticipating,
  title={Anticipating next active objects for egocentric videos},
  author={Thakur, Sanket and Beyan, Cigdem and Morerio, Pietro and Murino, Vittorio and Del Bue, Alessio},
  journal={IEEE Access},
  year={2024}
}


@inproceedings{acsintoae2022ubnormal,
  title={Ubnormal: New benchmark for supervised open-set video anomaly detection},
  author={Acsintoae, Andra and Florescu, Andrei and Georgescu, Mariana-Iuliana and Mare, Tudor and Sumedrea, Paul and Ionescu, Radu Tudor and Khan, Fahad Shahbaz and Shah, Mubarak},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{micorek2024mulde,
  title={MULDE: Multiscale Log-Density Estimation via Denoising Score Matching for Video Anomaly Detection},
  author={Micorek, Jakub and Possegger, Horst and Narnhofer, Dominik and Bischof, Horst and Kozinski, Mateusz},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{chen2023mgfn,
  title={Mgfn: Magnitude-contrastive glance-and-focus network for weakly-supervised video anomaly detection},
  author={Chen, Yingxian and Liu, Zhengzhe and Zhang, Baoheng and Fok, Wilton and Qi, Xiaojuan and Wu, Yik-Chung},
  booktitle={AAAI},
  year={2023}
}

@article{pu2023learning,
  title={Learning prompt-enhanced context features for weakly-supervised video anomaly detection},
  author={Pu, Yujiang and Wu, Xiaoyu and Yang, Lulu and Wang, Shengjin},
  journal={arxiv},
  year={2023}
}

@inproceedings{sultani2018real,
  title={Real-world anomaly detection in surveillance videos},
  author={Sultani, Waqas and Chen, Chen and Shah, Mubarak},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{dubey20193d,
  title={3d resnet with ranking loss function for abnormal activity detection in videos},
  author={Dubey, Shikha and Boragule, Abhijeet and Jeon, Moongu},
  booktitle={ICCAIS},
  year={2019}
}

@inproceedings{zhong2019graph,
  title={Graph convolutional label noise cleaner: Train a plug-and-play action classifier for anomaly detection},
  author={Zhong, Jia-Xing and Li, Nannan and Kong, Weijie and Liu, Shan and Li, Thomas H and Li, Ge},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{zhang2019temporal,
  title={Temporal convolutional network with complementary inner bag loss for weakly supervised anomaly detection},
  author={Zhang, Jiangong and Qing, Laiyun and Miao, Jun},
  booktitle={ICIP},
  year={2019},
}


@inproceedings{zhu2019motion,
  title={Motion-aware feature for improved video anomaly detection},
  author={Zhu, Yi and Newsam, Shawn},
  booktitle={BMVC},
  year={2019}
}

@inproceedings{wang2019gods,
  title={Gods: Generalized one-class discriminative subspaces for anomaly detection},
  author={Wang, Jue and Cherian, Anoop},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{markovitz2020graph,
  title={Graph embedded pose clustering for anomaly detection},
  author={Markovitz, Amir and Sharir, Gilad and Friedman, Itamar and Zelnik-Manor, Lihi and Avidan, Shai},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{morais2019learning,
  title={Learning regularity in skeleton trajectories for anomaly detection in videos},
  author={Morais, Romero and Le, Vuong and Tran, Truyen and Saha, Budhaditya and Mansour, Moussa and Venkatesh, Svetha},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{flaborea2023multimodal,
  title={Multimodal motion conditioned diffusion model for skeleton-based video anomaly detection},
  author={Flaborea, Alessandro and Collorone, Luca and Di Melendugno, Guido Maria D'Amely and D'Arrigo, Stefano and Prenkaj, Bardh and Galasso, Fabio},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{georgescu2021anomaly,
  title={Anomaly detection in video via self-supervised and multi-task learning},
  author={Georgescu, Mariana-Iuliana and Barbalau, Antonio and Ionescu, Radu Tudor and Khan, Fahad Shahbaz and Popescu, Marius and Shah, Mubarak},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{feng2021mist,
  title={Mist: Multiple instance self-training framework for video anomaly detection},
  author={Feng, Jia-Chang and Hong, Fa-Ting and Zheng, Wei-Shi},
  booktitle={CVPR},
  year={2021}
}


@inproceedings{purwanto2021dance,
  title={Dance with self-attention: A new look of conditional random fields on anomaly detection in videos},
  author={Purwanto, Didik and Chen, Yie-Tarng and Fang, Wen-Hsien},
  booktitle={ICCV},
  year={2021}
}


@inproceedings{tian2021weakly,
  title={Weakly-supervised video anomaly detection with robust temporal feature magnitude learning},
  author={Tian, Yu and Pang, Guansong and Chen, Yuanhong and Singh, Rajvinder and Verjans, Johan W and Carneiro, Gustavo},
  booktitle={ICCV},
  year={2021}
}

@article{zaheer2020self,
  title={A self-reasoning framework for anomaly detection using video-level labels},
  author={Zaheer, Muhammad Zaigham and Mahmood, Arif and Shin, Hochul and Lee, Seung-Ik},
  journal={IEEE SPL},
  year={2020},
}

@inproceedings{zaheer2020claws,
  title={Claws: Clustering assisted weakly supervised learning with normalcy suppression for anomalous event detection},
  author={Zaheer, Muhammad Zaigham and Mahmood, Arif and Astrid, Marcella and Lee, Seung-Ik},
  booktitle={ECCV},
  year={2020},
}

@inproceedings{li2021deep,
  title={Deep unsupervised anomaly detection},
  author={Li, Tangqing and Wang, Zheng and Liu, Siying and Lin, Wen-Yan},
  booktitle={WACV},
  year={2021}
}

@article{lee2019bman,
  title={BMAN: Bidirectional multi-scale aggregation networks for abnormal event detection},
  author={Lee, Sangmin and Kim, Hak Gu and Ro, Yong Man},
  journal={IEEE T-IP},
  year={2019},
}


@article{wang2021robust,
  title={Robust unsupervised video anomaly detection by multipath frame prediction},
  author={Wang, Xuanzhao and Che, Zhengping and Jiang, Bo and Xiao, Ning and Yang, Ke and Tang, Jian and Ye, Jieping and Wang, Jingyu and Qi, Qi},
  journal={IEEE TNNLS},
  year={2021},
}


@inproceedings{astrid2021learning,
  title={Learning not to reconstruct anomalies},
  author={Astrid, Marcella and Zaheer, Muhammad Zaigham and Lee, Jae-Yeong and Lee, Seung-Ik},
  booktitle={BMVC},
  year={2021}
}

@article{cho2022unsupervised,
  title={Unsupervised video anomaly detection via normalizing flows with implicit latent features},
  author={Cho, MyeongAh and Kim, Taeoh and Kim, Woo Jin and Cho, Suhwan and Lee, Sangyoun},
  journal={PR},
  year={2022},
}

@inproceedings{astrid2021synthetic,
  title={Synthetic temporal anomaly guided end-to-end video anomaly detection},
  author={Astrid, Marcella and Zaheer, Muhammad Zaigham and Lee, Seung-Ik},
  booktitle={ICCVw},
  year={2021}
}

@inproceedings{nguyen2019anomaly,
  title={Anomaly detection in video sequence with appearance-motion correspondence},
  author={Nguyen, Trong-Nguyen and Meunier, Jean},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{park2020learning,
  title={Learning memory-guided normality for anomaly detection},
  author={Park, Hyunjong and Noh, Jongyoun and Ham, Bumsub},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{luo2017revisit,
  title={A revisit of sparse coding based anomaly detection in stacked rnn framework},
  author={Luo, Weixin and Liu, Wen and Gao, Shenghua},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{hasan2016learning,
  title={Learning temporal regularity in video sequences},
  author={Hasan, Mahmudul and Choi, Jonghyun and Neumann, Jan and Roy-Chowdhury, Amit K and Davis, Larry S},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{almarri2024multi,
  title={A Multi-Head Approach with Shuffled Segments for Weakly-Supervised Video Anomaly Detection},
  author={AlMarri, Salem and Zaheer, Muhammad Zaigham and Nandakumar, Karthik},
  booktitle={WACVw},
  year={2024}
}

@inproceedings{fioresi2023ted,
  title={Ted-spad: Temporal distinctiveness for self-supervised privacy-preservation for video anomaly detection},
  author={Fioresi, Joseph and Dave, Ishan Rajendrakumar and Shah, Mubarak},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{yang2024text,
  title={Text Prompt with Normality Guidance for Weakly Supervised Video Anomaly Detection},
  author={Yang, Zhiwei and Liu, Jing and Wu, Peng},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{zanella2024harnessing,
  title={Harnessing Large Language Models for Training-free Video Anomaly Detection},
  author={Zanella, Luca and Menapace, Willi and Mancini, Massimiliano and Wang, Yiming and Ricci, Elisa},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{skorokhodov2022stylegan,
  title={Stylegan-v: A continuous video generator with the price, image quality and perks of stylegan2},
  author={Skorokhodov, Ivan and Tulyakov, Sergey and Elhoseiny, Mohamed},
  booktitle={CVPR},
  year={2022}
}

@article{dietterich1997solving,
  title={Solving the multiple instance problem with axis-parallel rectangles},
  author={Dietterich, Thomas G and Lathrop, Richard H and Lozano-P{\'e}rez, Tom{\'a}s},
  journal={Artificial intelligence},
  year={1997},
}

@article{brooks2022generating,
  title={Generating long videos of dynamic scenes},
  author={Brooks, Tim and Hellsten, Janne and Aittala, Miika and Wang, Ting-Chun and Aila, Timo and Lehtinen, Jaakko and Liu, Ming-Yu and Efros, Alexei and Karras, Tero},
  journal={NeurIPS},
  year={2022}
}

@inproceedings{shen2023mostgan,
  title={Mostgan-v: Video generation with temporal motion styles},
  author={Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  booktitle={CVPR},
  year={2023}
}

@article{liu2024sora,
  title={Sora: A review on background, technology, limitations, and opportunities of large vision models},
  author={Liu, Yixin and Zhang, Kai and Li, Yuan and Yan, Zhiling and Gao, Chujie and Chen, Ruoxi and Yuan, Zhengqing and Huang, Yue and Sun, Hanchi and Gao, Jianfeng and others},
  journal={arxiv},
  year={2024}
}


@article{hong2022cogvideo,
  title={Cogvideo: Large-scale pretraining for text-to-video generation via transformers},
  author={Hong, Wenyi and Ding, Ming and Zheng, Wendi and Liu, Xinghan and Tang, Jie},
  journal={arxiv},
  year={2022}
}


@inproceedings{yu2023avideo,
  title={Video probabilistic diffusion models in projected latent space},
  author={Yu, Sihyun and Sohn, Kihyuk and Kim, Subin and Shin, Jinwoo},
  booktitle={CVPR},
  year={2023}
}


@inproceedings{zeng2024make,
  title={Make pixels dance: High-dynamic video generation},
  author={Zeng, Yan and Wei, Guoqiang and Zheng, Jiani and Zou, Jiaxin and Wei, Yang and Zhang, Yuchen and Li, Hang},
  booktitle={CVPR},
  year={2024}
}

@article{gupta2023photorealistic,
  title={Photorealistic video generation with diffusion models},
  author={Gupta, Agrim and Yu, Lijun and Sohn, Kihyuk and Gu, Xiuye and Hahn, Meera and Fei-Fei, Li and Essa, Irfan and Jiang, Lu and Lezama, Jos{\'e}},
  journal={arxiv},
  year={2023}
}

@inproceedings{zhuang2024vlogger,
  title={Vlogger: Make your dream a vlog},
  author={Zhuang, Shaobin and Li, Kunchang and Chen, Xinyuan and Wang, Yaohui and Liu, Ziwei and Qiao, Yu and Wang, Yali},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{fu2023tell,
  title={Tell me what happened: Unifying text-guided video completion via multimodal masked video generation},
  author={Fu, Tsu-Jui and Yu, Licheng and Zhang, Ning and Fu, Cheng-Yang and Su, Jong-Chyi and Wang, William Yang and Bell, Sean},
  booktitle={CVPR},
  year={2023}
}

@article{ho2022imagen,
  title={Imagen video: High definition video generation with diffusion models},
  author={Ho, Jonathan and Chan, William and Saharia, Chitwan and Whang, Jay and Gao, Ruiqi and Gritsenko, Alexey and Kingma, Diederik P and Poole, Ben and Norouzi, Mohammad and Fleet, David J and others},
  journal={arxiv},
  year={2022}
}

@inproceedings{moltisanti2019action,
  title={Action recognition from single timestamp supervision in untrimmed videos},
  author={Moltisanti, Davide and Fidler, Sanja and Damen, Dima},
  booktitle={CVPR},
  year={2019}
}


@inproceedings{yang2020localizing,
  title={Localizing the common action among a few videos},
  author={Yang, Pengwan and Hu, Vincent Tao and Mettes, Pascal and Snoek, Cees GM},
  booktitle={ECCV},
  year={2020},
}

@inproceedings{han2022show,
  title={Show me what and tell me how: Video synthesis via multimodal conditioning},
  author={Han, Ligong and Ren, Jian and Lee, Hsin-Ying and Barbieri, Francesco and Olszewski, Kyle and Minaee, Shervin and Metaxas, Dimitris and Tulyakov, Sergey},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{hu2022make,
  title={Make it move: controllable image-to-video generation with text descriptions},
  author={Hu, Yaosi and Luo, Chong and Chen, Zhenzhong},
  booktitle={CVPR},
  year={2022}
}

@article{wu2021godiva,
  title={Godiva: Generating open-domain videos from natural descriptions},
  author={Wu, Chenfei and Huang, Lun and Zhang, Qianxi and Li, Binyang and Ji, Lei and Yang, Fan and Sapiro, Guillermo and Duan, Nan},
  journal={arxiv},
  year={2021}
}

@inproceedings{zhao2011online,
  title={Online detection of unusual events in videos via dynamic sparse coding},
  author={Zhao, Bin and Fei-Fei, Li and Xing, Eric P},
  booktitle={CVPR},
  year={2011},
}

@inproceedings{kitani2012activity,
  title={Activity forecasting},
  author={Kitani, Kris M and Ziebart, Brian D and Bagnell, James Andrew and Hebert, Martial},
  booktitle={ECCV},
  year={2012},
}

@article{koppula2015anticipating,
  title={Anticipating human activities using object affordances for reactive robotic response},
  author={Koppula, Hema S and Saxena, Ashutosh},
  journal={IEEE TPAMI},
  year={2015}
}

@inproceedings{liu2020forecasting,
  title={Forecasting human-object interaction: joint prediction of motor attention and actions in first person video},
  author={Liu, Miao and Tang, Siyu and Li, Yin and Rehg, James M},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{luc2018predicting,
  title={Predicting future instance segmentation by forecasting convolutional features},
  author={Luc, Pauline and Couprie, Camille and Lecun, Yann and Verbeek, Jakob},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{miech2019leveraging,
  title={Leveraging the present to anticipate the future in videos},
  author={Miech, Antoine and Laptev, Ivan and Sivic, Josef and Wang, Heng and Torresani, Lorenzo and Tran, Du},
  booktitle={CVPRw},
  year={2019}
}

@inproceedings{zatsarynna2021multi,
  title={Multi-modal temporal convolutional network for anticipating actions in egocentric videos},
  author={Zatsarynna, Olga and Abu Farha, Yazan and Gall, Juergen},
  booktitle={CVPRw},
  pages={2249--2258},
  year={2021}
}

@misc{randal2009movie,
 author = {Munroe Randall},
 title = {Movie Narrative Charts},
 url = {https://xkcd.com/657/},
 year = {2009}
}
@inproceedings{ashutosh2023hiervl,
  title={Hiervl: Learning hierarchical video-language embeddings},
  author={Ashutosh, Kumar and Girdhar, Rohit and Torresani, Lorenzo and Grauman, Kristen},
  booktitle={CVPR},
  year={2023}
}

@article{yan2021videogpt,
  title={Videogpt: Video generation using vq-vae and transformers},
  author={Yan, Wilson and Zhang, Yunzhi and Abbeel, Pieter and Srinivas, Aravind},
  journal={arXiv},
  year={2021}
}

@inproceedings{menapace2021playable,
  title={Playable video generation},
  author={Menapace, Willi and Lathuiliere, Stephane and Tulyakov, Sergey and Siarohin, Aliaksandr and Ricci, Elisa},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{yu2024efficient,
  title={Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition},
  author={Yu, Sihyun and Nie, Weili and Huang, De-An and Li, Boyi and Shin, Jinwoo and Anandkumar, Anima},
  booktitle={ICLR},
  year={2024}
}

@inproceedings{arandjelovic2018objects,
  title={Objects that sound},
  author={Arandjelovic, Relja and Zisserman, Andrew},
  booktitle={ECCV},
  year={2018}
}

@article{kaiser2017one,
  title={One model to learn them all},
  author={Kaiser, Lukasz and Gomez, Aidan N and Shazeer, Noam and Vaswani, Ashish and Parmar, Niki and Jones, Llion and Uszkoreit, Jakob},
  journal={arxiv},
  year={2017}
}

@article{recasens2023zorro,
  title={Zorro: the masked multimodal transformer},
  author={Recasens, Adri{\`a} and Lin, Jason and Carreira, Jo{\=a}o and Jaegle, Drew and Wang, Luyu and Alayrac, Jean-baptiste and Luc, Pauline and Miech, Antoine and Smaira, Lucas and Hemsley, Ross and others},
  journal={arxiv},
  year={2023}
}

@article{xiao2020audiovisual,
  title={Audiovisual slowfast networks for video recognition},
  author={Xiao, Fanyi and Lee, Yong Jae and Grauman, Kristen and Malik, Jitendra and Feichtenhofer, Christoph},
  journal={arXiv},
  year={2020}
}

@inproceedings{saito2017temporal,
  title={Temporal generative adversarial nets with singular value clipping},
  author={Saito, Masaki and Matsumoto, Eiichi and Saito, Shunta},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{yu2022generating,
  title={Generating videos with dynamics-aware implicit generative adversarial networks},
  author={Yu, Sihyun and Tack, Jihoon and Mo, Sangwoo and Kim, Hyunsu and Kim, Junho and Ha, Jung-Woo and Shin, Jinwoo},
  booktitle={ICLR},
  year={2022}
}

@inproceedings{ke2019time,
  title={Time-conditioned action anticipation in one shot},
  author={Ke, Qiuhong and Fritz, Mario and Schiele, Bernt},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{munro2020multi,
  title={Multi-modal domain adaptation for fine-grained action recognition},
  author={Munro, Jonathan and Damen, Dima},
  booktitle={CVPR},
  year={2020}
}

@article{jiang2021predicting,
  title={Predicting short-term next-active-object through visual attention and hand position},
  author={Jiang, Jingjing and Nan, Zhixiong and Chen, Hui and Chen, Shitao and Zheng, Nanning},
  journal={Neurocomputing},
  year={2021}
}

@inproceedings{liu2022joint,
  title={Joint hand motion and interaction hotspots prediction from egocentric videos},
  author={Liu, Shaowei and Tripathi, Subarna and Majumdar, Somdeb and Wang, Xiaolong},
  booktitle={CVPR},
  year={2022}
}


@article{mendonca2023structured,
  title={Structured world models from human videos},
  author={Mendonca, Russell and Bahl, Shikhar and Pathak, Deepak},
  journal={arxiv arXiv:2308.10901},
  year={2023}
}

@inproceedings{cheng2024egothink,
  title={EgoThink: Evaluating First-Person Perspective Thinking Capability of Vision-Language Models},
  author={Cheng, Sijie and Guo, Zhicheng and Wu, Jingwen and Fang, Kechen and Li, Peng and Liu, Huaping and Liu, Yang},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{diko2024semantically,
  title={Semantically Guided Representation Learning For Action Anticipation},
  author={Diko, Anxhelo and Avola, Danilo and Prenkaj, Bardh and Fontana, Federico and Cinque, Luigi},
  booktitle={ECCV},
  year={2024}
}

@article{chang2024look,
  title={Look ma, no hands! agent-environment factorization of egocentric videos},
  author={Chang, Matthew and Prakash, Aditya and Gupta, Saurabh},
  journal={NeurIPS},
  year={2024}
}

@article{mur2024aff,
  title={AFF-ttention! Affordances and Attention models for Short-Term Object Interaction Anticipation},
  author={Mur-Labadia, Lorenzo and Martinez-Cantin, Ruben and Guerrero, Josechu and Farinella, Giovanni Maria and Furnari, Antonino},
  journal={arxiv arXiv:2406.01194},
  year={2024}
}

@inproceedings{perrett2019ddlstm,
  title={DDLSTM: dual-domain LSTM for cross-dataset action recognition},
  author={Perrett, Toby and Damen, Dima},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{zatsarynna2024gated,
  title={Gated Temporal Diffusion for Stochastic Long-Term Dense Anticipation},
  author={Zatsarynna, Olga and Bahrami, Emad and Farha, Yazan Abu and Francesca, Gianpiero and Gall, Juergen},
  booktitle={ECCV},
  year={2024}
}

@inproceedings{bokhari2017long,
  title={Long-term activity forecasting using first-person vision},
  author={Bokhari, Syed Zahir and Kitani, Kris M},
  booktitle={ACCV},
  year={2017}
}

@inproceedings{abu2021long,
  title={Long-term anticipation of activities with cycle consistency},
  author={Abu Farha, Yazan and Ke, Qiuhong and Schiele, Bernt and Gall, Juergen},
  booktitle={DAGM GCPR},
  year={2021},
}

@article{lee2006efficient,
  title={Efficient sparse coding algorithms},
  author={Lee, Honglak and Battle, Alexis and Raina, Rajat and Ng, Andrew},
  journal={NeurIPS},
  year={2006}
}