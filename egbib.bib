%% ---- SURVEYS PAPERS ----

@inproceedings{aggarwal1994articulated,
  title={Articulated and elastic non-rigid motion: A review},
  author={Aggarwal, Jake K and Cai, Qin and Liao, Wen and Sabata, Bikash},
  booktitle={Workshop on Motion of Non-rigid and Articulated Objects},
  year={1994}
}

@article{cedras1995motion,
  title={Motion-based recognition a survey},
  author={Cedras, Claudette and Shah, Mubarak},
  journal={IVC},
  year={1995},
}

@inproceedings{park2019relational,
  title={Relational knowledge distillation},
  author={Park, Wonpyo and Kim, Dongju and Lu, Yan and Cho, Minsu},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{rangrej2023glitr,
  title={GliTr: Glimpse Transformers with Spatiotemporal Consistency for Online Action Prediction},
  author={Rangrej, Samrudhdhi B and Liang, Kevin J and Hassner, Tal and Clark, James J},
  booktitle={WACV},
  year={2023}
}

@article{aggarwal1998nonrigid,
  title={Nonrigid motion analysis: Articulated and elastic motion},
  author={Aggarwal, Jake K and Cai, Quin and Liao, W and Sabata, Bikash},
  journal={CVIU},
  year={1998}
}

@article{aggarwal1999human,
  title={Human motion analysis: A review},
  author={Aggarwal, Jake K and Cai, Quin},
  journal={CVIU},
  year={1999}
}

@article{moeslund2001survey,
  title={A survey of computer vision-based human motion capture},
  author={Moeslund, Thomas B and Granum, Erik},
  journal={CVIU},
  year={2001}
}

@article{buxton2003learning,
  title={Learning and understanding dynamic scene activity: a review},
  author={Buxton, Hilary},
  journal={IVC},
  year={2003},
}

@article{moeslund2006survey,
  title={A survey of advances in vision-based human motion capture and analysis},
  author={Moeslund, Thomas B and Hilton, Adrian and Kr{\"u}ger, Volker},
  journal={CVIU},
  year={2006}
}

@article{yilmaz2006object,
  title={Object tracking: A survey},
  author={Yilmaz, Alper and Javed, Omar and Shah, Mubarak},
  journal={CSUR},
  year={2006}
}

@article{poppe2007vision,
  title={Vision-based human motion analysis: An overview},
  author={Poppe, Ronald},
  journal={CVIU},
  year={2007}
}

@article{turaga2008machine,
  title={Machine recognition of human activities: A survey},
  author={Turaga, Pavan and Chellappa, Rama and Subrahmanian, Venkatramana S and Udrea, Octavian},
  journal={IEEE TCSVT},
  year={2008}
}

@article{poppe2010survey,
  title={A survey on vision-based human action recognition},
  author={Poppe, Ronald},
  journal={IVC},
  year={2010}
}

@article{weinland2011survey,
  title={A survey of vision-based methods for action representation, segmentation and recognition},
  author={Weinland, Daniel and Ronfard, Remi and Boyer, Edmond},
  journal={CVIU},
  year={2011}
}

@article{chaaraoui2012review,
  title={A review on vision techniques applied to human behaviour analysis for ambient-assisted living},
  author={Chaaraoui, Alexandros Andr{\'e} and Climent-P{\'e}rez, Pau and Fl{\'o}rez-Revuelta, Francisco},
  journal={ESWA},
  year={2012}
}

@article{metaxas2013review,
  title={A review of motion analysis methods for human nonverbal communication computing},
  author={Metaxas, Dimitris and Zhang, Shaoting},
  journal={IVC},
  year={2013}
}

@article{vishwakarma2013survey,
  title={A survey on activity recognition and behavior understanding in video surveillance},
  author={Vishwakarma, Sarvesh and Agrawal, Anupam},
  journal={TVC},
  year={2013}
}

@article{herath2017going,
  title={Going deeper into action recognition: A survey},
  author={Herath, Samitha and Harandi, Mehrtash and Porikli, Fatih},
  journal={IVC},
  year={2017}
}

@article{wang2018rgb,
  title={RGB-D-based human motion recognition with deep learning: A survey},
  author={Wang, Pichao and Li, Wanqing and Ogunbona, Philip and Wan, Jun and Escalera, Sergio},
  journal={CVIU},
  year={2018}
}

@article{dhiman2019review,
  title={A review of state-of-the-art techniques for abnormal human activity recognition},
  author={Dhiman, Chhavi and Vishwakarma, Dinesh Kumar},
  journal={EAAI},
  year={2019}
}

@article{hussain2019different,
  title={Different approaches for human activity recognition: A survey},
  author={Hussain, Zawar and Sheng, Michael and Zhang, Wei Emma},
  journal={ arXiv:1906.05074},
  year={2019}
}

@article{beddiar2020vision,
  title={Vision-based human activity recognition: a survey},
  author={Beddiar, Djamila Romaissa and Nini, Brahim and Sabokrou, Mohammad and Hadid, Abdenour},
  journal={MTA},
  year={2020}
}

@article{zheng2020deep,
  title={Deep learning-based human pose estimation: A survey},
  author={Zheng, Ce and Wu, Wenhan and Chen, Chen and Yang, Taojiannan and Zhu, Sijie and Shen, Ju and Kehtarnavaz, Nasser and Shah, Mubarak},
  journal={CSUR},
  year={2020}
}


@article{pareek2021survey,
  title={A survey on video-based human action recognition: recent updates, datasets, challenges, and applications},
  author={Pareek, Preksha and Thakkar, Ankit},
  journal={UMT-AIR},
  year={2021}
}

@article{stergiou2019analyzing,
  title={Analyzing human--human interactions: A survey},
  author={Stergiou, Alexandros and Poppe, Ronald},
  journal={CVIU},
  year={2019}
}

@article{rasouli2020deep,
  title={Deep learning for vision-based prediction: A survey},
  author={Rasouli, Amir},
  journal={arXiv:2007.00095},
  year={2020}
}

@article{yao2019review,
  title={A review of convolutional-neural-network-based action recognition},
  author={Yao, Guangle and Lei, Tao and Zhong, Jiandan},
  journal={PRL},
  year={2019}
}

@article{zhang2019comprehensive,
  title={A comprehensive survey of vision-based human action recognition methods},
  author={Zhang, Hong-Bo and Zhang, Yi-Xiang and Zhong, Bineng and Lei, Qing and Yang, Lijie and Du, Ji-Xiang and Chen, Duan-Sheng},
  journal={Sensors},
  year={2019}
}

@article{ramachandra2020survey,
  title={A survey of single-scene video anomaly detection},
  author={Ramachandra, Bharathkumar and Jones, Michael J and Vatsavai, Ranga Raju},
  journal={IEEE TPAMI},
  year={2020}
}

@article{rodin2021predicting,
  title={Predicting the future from first person (egocentric) vision: A survey},
  author={Rodin, Ivan and Furnari, Antonino and Mavroeidis, Dimitrios and Farinella, Giovanni Maria},
  journal={CVIU},
  year={2021},
}

@article{song2021human,
  title={Human pose estimation and its application to action recognition: A survey},
  author={Song, Liangchen and Yu, Gang and Yuan, Junsong and Liu, Zicheng},
  journal={JVCIR},
  year={2021}
}

@article{sun2022human,
  title={Human action recognition from various data modalities: A review},
  author={Sun, Zehua and Ke, Qiuhong and Rahmani, Hossein and Bennamoun, Mohammed and Wang, Gang and Liu, Jun},
  journal={IEEE TPAMI},
  year={2022}
}

@article{kong2022human,
  title={Human action recognition and prediction: A survey},
  author={Kong, Yu and Fu, Yun},
  journal={IJCV},
  year={2022}
}

@article{oprea2022review,
  title={A review on deep learning techniques for video prediction},
  author={Oprea, Sergiu and Martinez-Gonzalez, Pablo and Garcia-Garcia, Alberto and Castro-Vargas, John Alejandro and Orts-Escolano, Sergio and Garcia-Rodriguez, Jose and Argyros, Antonis},
  journal={IEEE TPAMI},
  year={2022}
}

@article{hu2022online,
  title={Online human action detection and anticipation in videos: A survey},
  author={Hu, Xuejiao and Dai, Jingzhao and Li, Ming and Peng, Chenglei and Li, Yang and Du, Sidan},
  journal={Neurocomputing},
  year={2022},
}

@article{schiappa2023self,
  title={Self-supervised learning for videos: A survey},
  author={Schiappa, Madeline C and Rawat, Yogesh S and Shah, Mubarak},
  journal={CSUR},
  year={2023}
}

@article{selva2023video,
  title={Video transformers: A survey},
  author={Selva, Javier and Johansen, Anders S and Escalera, Sergio and Nasrollahi, Kamal and Moeslund, Thomas B and Clap{\'e}s, Albert},
  journal={IEEE TPAMI},
  year={2023}
}

@article{ding2023temporal,
  title={Temporal action segmentation: An analysis of modern techniques},
  author={Ding, Guodong and Sener, Fadime and Yao, Angela},
  journal={IEEE TPAMI},
  year={2023}
}

@article{zhong2023survey,
  title={A survey on deep learning techniques for action anticipation},
  author={Zhong, Zeyun and Martin, Manuel and Voit, Michael and Gall, Juergen and Beyerer, J{\"u}rgen},
  journal={arXiv:2309.17257},
  year={2023}
}

@article{plizzari2024outlook,
  title={An Outlook into the Future of Egocentric Vision},
  author={Plizzari, Chiara and Goletto, Gabriele and Furnari, Antonino and Bansal, Siddhant and Ragusa, Francesco and Farinella, Giovanni Maria and Damen, Dima and Tommasi, Tatiana},
  journal={IJCV},
  year={2024}
}

@article{madan2024foundation,
  title={Foundation Models for Video Understanding: A Survey},
  author={Madan, Neelu and Moegelmose, Andreas and Modi, Rajat and Rawat, Yogesh S. and Moeslund, Thomas B.},
  journal={arxiv arXiv:2405.03770},
  year={2024}
}


%% ---- MOTION-BASED PAPERS ----
@article{johansson1975visual,
  title={Visual motion perception},
  author={Johansson, Gunnar},
  journal={Scientific American},
  year={1975}
}

@article{marr1982representation,
  title={Representation and recognition of the movements of shapes},
  author={Marr, David and Vaina, Lucia},
  journal={Series B. Biological Sciences},
  year={1982}
}

@article{hogg1983model,
  title={Model-based vision: a program to see a walking person},
  author={Hogg, David},
  journal={IVC},
  year={1983}
}

@article{flinchbaugh1981theory,
  title={A theory of spatio-temporal aggregation for vision},
  author={Flinchbaugh, Bruce E and Chandrasekaran, B},
  journal={Artificial Intelligence},
  year={1981}
}

@article{potter1977scene,
  title={Scene segmentation using motion information},
  author={Potter, Jerry L},
  journal={CGIP},
  year={1977}
}

@article{orourke1980model,
  title={Model-based image analysis of human motion using constraint propagation},
  author={O'rourke, Joseph and Badler, Norman I},
  journal={IEEE TPAMI},
  year={1980}
}

@article{roach1979computer,
  title={Computer tracking of objects moving in space},
  author={Roach, John W and Aggarwal, JK},
  journal={IEEE TPAMI},
  year={1979}
}

@book{ullman1979interpretation,
  title={The interpretation of visual motion},
  author={Ullman, Shimon},
  year={1979},
  publisher={MIT Press}
}

@inproceedings{ni2014multiple,
  title={Multiple granularity analysis for fine-grained action detection},
  author={Ni, Bingbing and Paramathayalan, Vignesh R and Moulin, Pierre},
  booktitle={CVPR},
  year={2014}
}


@article{rohr1994towards,
  title={Towards model-based recognition of human movements in image sequences},
  author={Rohr, Karl},
  journal={CVGIP},
  year={1994}
}

@article{isard1998condensation,
  title={Condensation—conditional density propagation for visual tracking},
  author={Isard, Michael and Blake, Andrew},
  journal={IJCV},
  year={1998}
}

@inproceedings{cipolla1990dynamic,
  title={The dynamic analysis of apparent contours},
  author={Cipolla, Roberto and Blake, Andrew},
  booktitle={ICCV},
  year={1990}
}




%% ----- END OF MOTION-BASED PAPERS -----

@inproceedings{forstner1987fast,
  title={A fast operator for detection and precise location of distinct points, corners and centres of circular features},
  author={F{\"o}rstner, Wolfgang and G{\"u}lch, Eberhard},
  booktitle={ICFPPD},
  year={1987}
}


@inproceedings{harris1988combined,
  title={A combined corner and edge detector},
  author={Harris, Chris and Stephens, Mike and others},
  booktitle={AVC},
  year={1988}
}



@inproceedings{schuldt2004recognizing,
  title={Recognizing human actions: a local SVM approach},
  author={Schuldt, Christian and Laptev, Ivan and Caputo, Barbara},
  booktitle={ICPR},
  year={2004}
}

@article{gorelick2007actions,
  title={Actions as space-time shapes},
  author={Gorelick, Lena and Blank, Moshe and Shechtman, Eli and Irani, Michal and Basri, Ronen},
  journal={IEEE TPAMI},
  year={2007}
}

@inproceedings{laptev2007retrieving,
  title={Retrieving actions in movies},
  author={Laptev, Ivan and P{\'e}rez, Patrick},
  booktitle={ICCV},
  year={2007}
}

@inproceedings{wang2007human,
  title={Human activity recognition based on r transform},
  author={Wang, Ying and Huang, Kaiqi and Tan, Tieniu},
  booktitle={CVPR},
  year={2007}
}

@inproceedings{nowozin2007discriminative,
  title={Discriminative subsequence mining for action classification},
  author={Nowozin, Sebastian and Bakir, Gokhan and Tsuda, Koji},
  booktitle={ICCV},
  year={2007}
}



@inproceedings{efros2003recognizing,
  title={Recognizing action at a distance},
  author={Efros, Alexei and Berg, Alexander and Mori, Greg and Malik, Jitendra},
  booktitle={ICCV},
  year={2003}
}

@inproceedings{liu2008learning,
  title={Learning human actions via information maximization},
  author={Liu, Jingen and Shah, Mubarak},
  booktitle={CVPR},
  year={2008}
}


@inproceedings{zelnik2001event,
  title={Event-based analysis of video},
  author={Zelnik-Manor, Lihi and Irani, Michal},
  booktitle={CVPR},
  year={2001}
}

@article{gorelick2006shape,
  title={Shape representation and classification using the poisson equation},
  author={Gorelick, Lena and Galun, Meirav and Sharon, Eitan and Basri, Ronen and Brandt, Achi},
  journal={IEEE TPAMI},
  year={2006}
}

@inproceedings{stergiou2021multi,
  title={Multi-temporal convolutions for human action recognition in videos},
  author={Stergiou, Alexandros and Poppe, Ronald},
  booktitle={IJCNN},
  year={2021}
}

@inproceedings{chen2019drop,
  title={Drop an octave: Reducing spatial redundancy in convolutional neural networks with octave convolution},
  author={Chen, Yunpeng and Fan, Haoqi and Xu, Bing and Yan, Zhicheng and Kalantidis, Yannis and Rohrbach, Marcus and Yan, Shuicheng and Feng, Jiashi},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{jia2008human,
  title={Human action recognition using local spatio-temporal discriminant embedding},
  author={Jia, Kui and Yeung, Dit-Yan},
  booktitle={CVPR},
  year={2008}
}



@inproceedings{rodriguez2008action,
  title={Action mach a spatio-temporal maximum average correlation height filter for action recognition},
  author={Rodriguez, Mikel D and Ahmed, Javed and Shah, Mubarak},
  booktitle={CVPR},
  year={2008}
}

@inproceedings{laptev2008learning,
  title={Learning realistic human actions from movies},
  author={Laptev, Ivan and Marszalek, Marcin and Schmid, Cordelia and Rozenfeld, Benjamin},
  booktitle={CVPR},
  year={2008}
}

@article{chen2024video,
  title={Video mamba suite: State space model as a versatile alternative for video understanding},
  author={Chen, Guo and Huang, Yifei and Xu, Jilan and Pei, Baoqi and Chen, Zhe and Li, Zhiqi and Wang, Jiahao and Li, Kunchang and Lu, Tong and Wang, Limin},
  journal={arXiv preprint arXiv:2403.09626},
  year={2024}
}

@inproceedings{zeng2024unimd,
  title={UniMD: Towards Unifying Moment Retrieval and Temporal Action Detection},
  author={Zeng, Yingsen and Zhong, Yujie and Feng, Chengjian and Ma, Lin},
  booktitle={ECCV},
  year={2024}
}

@inproceedings{dai2021pdan,
  title={Pdan: Pyramid dilated attention network for action detection},
  author={Dai, Rui and Das, Srijan and Minciullo, Luca and Garattoni, Lorenzo and Francesca, Gianpiero and Bremond, Fran{\c{c}}ois},
  booktitle={WACV},
  year={2021}
}

@inproceedings{he2020momentum,
  title={Momentum contrast for unsupervised visual representation learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={ICML},
  year={2020},
}


@inproceedings{hjelm2018learning,
  title={Learning deep representations by mutual information estimation and maximization},
  author={Hjelm, R Devon and Fedorov, Alex and Lavoie-Marchildon, Samuel and Grewal, Karan and Bachman, Phil and Trischler, Adam and Bengio, Yoshua},
  booktitle={ICLR},
  year={2018}
}

@article{sun2019learning,
  title={Learning video representations using contrastive bidirectional transformer},
  author={Sun, Chen and Baradel, Fabien and Murphy, Kevin and Schmid, Cordelia},
  journal={arXiv:1906.05743},
  year={2019}
}

@inproceedings{bai2022salient,
  title={Salient-to-broad transition for video person re-identification},
  author={Bai, Shutao and Ma, Bingpeng and Chang, Hong and Huang, Rui and Chen, Xilin},
  booktitle={CVPR},
  year={2022}
}

@article{cai2022heterogeneous,
  title={Heterogeneous graph contrastive learning network for personalized micro-video recommendation},
  author={Cai, Desheng and Qian, Shengsheng and Fang, Quan and Hu, Jun and Ding, Wenkui and Xu, Changsheng},
  journal={IEEE TMM},
  year={2022}
}

@inproceedings{feng2023mutual,
  title={Mutual information-based temporal difference learning for human pose estimation in video},
  author={Feng, Runyang and Gao, Yixing and Ma, Xueqing and Tse, Tze Ho Elden and Chang, Hyung Jin},
  booktitle={CVPR},
  year={2023}
}

@article{gordon2020watching,
  title={Watching the world go by: Representation learning from unlabeled videos},
  author={Gordon, Daniel and Ehsani, Kiana and Fox, Dieter and Farhadi, Ali},
  journal={arXiv:2003.07990},
  year={2020}
}

@inproceedings{sun2021composable,
  title={Composable augmentation encoding for video representation learning},
  author={Sun, Chen and Nagrani, Arsha and Tian, Yonglong and Schmid, Cordelia},
  booktitle={ICCV},
  year={2021}
}

@article{hjelm2020representation,
  title={Representation learning with video deep infomax},
  author={Hjelm, R Devon and Bachman, Philip},
  journal={arXiv:2007.13278},
  year={2020}
}

@inproceedings{sameni2023spatio,
  title={Spatio-Temporal Crop Aggregation for Video Representation Learning},
  author={Sameni, Sepehr and Jenni, Simon and Favaro, Paolo},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{wei2018learning,
  title={Learning and using the arrow of time},
  author={Wei, Donglai and Lim, Joseph J and Zisserman, Andrew and Freeman, William T},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{wu2021contrastive,
  title={Contrastive learning of image representations with cross-video cycle-consistency},
  author={Wu, Haiping and Wang, Xiaolong},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{badamdorj2022contrastive,
  title={Contrastive learning for unsupervised video highlight detection},
  author={Badamdorj, Taivanbat and Rochan, Mrigank and Wang, Yang and Cheng, Li},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{ying2023ctvis,
  title={Ctvis: Consistent training for online video instance segmentation},
  author={Ying, Kaining and Zhong, Qing and Mao, Weian and Wang, Zhenhua and Chen, Hao and Wu, Lin Yuanbo and Liu, Yifan and Fan, Chengxiang and Zhuge, Yunzhi and Shen, Chunhua},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{ranasinghe2022self,
  title={Self-supervised video transformer},
  author={Ranasinghe, Kanchana and Naseer, Muzammal and Khan, Salman and Khan, Fahad Shahbaz and Ryoo, Michael S},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{zhang2021video,
  title={Video corpus moment retrieval with contrastive learning},
  author={Zhang, Hao and Sun, Aixin and Jing, Wei and Nan, Guoshun and Zhen, Liangli and Zhou, Joey Tianyi and Goh, Rick Siow Mong},
  booktitle={SIGIR},
  year={2021}
}

@inproceedings{xu2021rethinking,
  title={Rethinking self-supervised correspondence learning: A video frame-level similarity perspective},
  author={Xu, Jiarui and Wang, Xiaolong},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{wang2024videocutler,
  title={Videocutler: Surprisingly simple unsupervised video instance segmentation},
  author={Wang, Xudong and Misra, Ishan and Zeng, Ziyun and Girdhar, Rohit and Darrell, Trevor},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{wang2020self,
  title={Self-supervised video representation learning by pace prediction},
  author={Wang, Jiangliu and Jiao, Jianbo and Liu, Yun-Hui},
  booktitle={ECCV},
  year={2020}
}

@article{oord2018representation,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}

@article{xie2021self,
  title={Self-supervised learning with swin transformers},
  author={Xie, Zhenda and Lin, Yutong and Yao, Zhuliang and Zhang, Zheng and Dai, Qi and Cao, Yue and Hu, Han},
  journal={arXiv preprint arXiv:2105.04553},
  year={2021}
}

@article{caron2020unsupervised,
  title={Unsupervised learning of visual features by contrasting cluster assignments},
  author={Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
  journal={NeurIPS},
  year={2020}
}

@inproceedings{wei2022inter,
  title={Inter-Intra Cross-Modality Self-Supervised Video Representation Learning by Contrastive Clustering},
  author={Wei, Jiutong and Luo, Guan and Li, Bing and Hu, Weiming},
  booktitle={ICPR},
  year={2022},
}

@inproceedings{yan2020clusterfit,
  title={Clusterfit: Improving generalization of visual representations},
  author={Yan, Xueting and Misra, Ishan and Gupta, Abhinav and Ghadiyaram, Deepti and Mahajan, Dhruv},
  booktitle={CVPR},
  year={2020}
}

@article{grill2020bootstrap,
  title={Bootstrap your own latent-a new approach to self-supervised learning},
  author={Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and others},
  journal={NeurIPS},
  year={2020}
}

@inproceedings{coskun2022goca,
  title={GOCA: Guided online cluster assignment for self-supervised video representation learning},
  author={Coskun, Huseyin and Zareian, Alireza and Moore, Joshua L and Tombari, Federico and Wang, Chen},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{ponimatkin2023simple,
  title={A simple and powerful global optimization for unsupervised video object segmentation},
  author={Ponimatkin, Georgy and Samet, Nermin and Xiao, Yang and Du, Yuming and Marlet, Renaud and Lepetit, Vincent},
  booktitle={WACV},
  year={2023}
}

@inproceedings{salehi2023time,
  title={Time does tell: Self-supervised time-tuning of dense image representations},
  author={Salehi, Mohammadreza and Gavves, Efstratios and Snoek, Cees G. M. and Asano, Yuki M},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{long2023cross,
  title={Cross-modal scalable hyperbolic hierarchical clustering},
  author={Long, Teng and van Noord, Nanne},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{toering2022self,
  title={Self-supervised video representation learning with cross-stream prototypical contrasting},
  author={Toering, Martine and Gatopoulos, Ioannis and Stol, Maarten and Hu, Vincent Tao},
  booktitle={WACV},
  year={2022}
}

@article{escontrela2023video,
  title={Video prediction models as rewards for reinforcement learning},
  author={Escontrela, Alejandro and Adeniji, Ademi and Yan, Wilson and Jain, Ajay and Peng, Xue Bin and Goldberg, Ken and Lee, Youngwoon and Hafner, Danijar and Abbeel, Pieter},
  journal={NeurIPS},
  year={2023}
}

@inproceedings{xiong2021self,
  title={Self-supervised representation learning from flow equivariance},
  author={Xiong, Yuwen and Ren, Mengye and Zeng, Wenyuan and Urtasun, Raquel},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{liu2022funnynet,
  title={Funnynet: Audiovisual learning of funny moments in videos},
  author={Liu, Zhisong and Courant, Robin and Kalogeiton, Vicky},
  booktitle={ACCV},
  year={2022}
}

@article{sarkar2023uncovering,
  title={Uncovering the hidden dynamics of video self-supervised learning under distribution shifts},
  author={Sarkar, Pritam and Beirami, Ahmad and Etemad, Ali},
  journal={NeurIPS},
  year={2023}
}

@inproceedings{zhang2022contrastive,
  title={Contrastive spatio-temporal pretext learning for self-supervised video representation},
  author={Zhang, Yujia and Po, Lai-Man and Xu, Xuyuan and Liu, Mengyang and Wang, Yexin and Ou, Weifeng and Zhao, Yuzhi and Yu, Wing-Yin},
  booktitle={AAAI},
  year={2022}
}

@inproceedings{morales2022leveraging,
  title={Leveraging Unlabeled Data for Sketch-based Understanding},
  author={Morales, Javier and Murrugarra-Llerena, Nils and Saavedra, Jose M},
  booktitle={CVPRw},
  year={2022}
}

@inproceedings{caron2021emerging,
  title={Emerging properties in self-supervised vision transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{fan2023unsupervised,
  title={Unsupervised Open-Vocabulary Object Localization in Videos},
  author={Fan, Ke and Bai, Zechen and Xiao, Tianjun and Zietlow, Dominik and Horn, Max and Zhao, Zixu and Simon-Gabriel, Carl-Johann and Shou, Mike Zheng and Locatello, Francesco and Schiele, Bernt and others},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{ding2024betrayed,
  title={Betrayed by Attention: A Simple yet Effective Approach for Self-supervised Video Object Segmentation},
  author={Ding, Shuangrui and Qian, Rui and Xu, Haohang and Lin, Dahua and Xiong, Hongkai},
  booktitle={ECCV},
  year={2024}
}

@inproceedings{jiang2023single,
  title={Single-stage visual query localization in egocentric videos},
  author={Jiang, Hanwen and Ramakrishnan, Santhosh Kumar and Grauman, Kristen},
  booktitle={NeurIPS},
  year={2023}
}

@inproceedings{huang2024vbench,
  title={Vbench: Comprehensive benchmark suite for video generative models},
  author={Huang, Ziqi and He, Yinan and Yu, Jiashuo and Zhang, Fan and Si, Chenyang and Jiang, Yuming and Zhang, Yuanhan and Wu, Tianxing and Jin, Qingyang and Chanpaisit, Nattapol and others},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{huang2024uvis,
  title={UVIS: Unsupervised Video Instance Segmentation},
  author={Huang, Shuaiyi and Suri, Saksham and Gupta, Kamal and Rambhatla, Sai Saketh and Lim, Ser-nam and Shrivastava, Abhinav},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{zbontar2021barlow,
  title={Barlow twins: Self-supervised learning via redundancy reduction},
  author={Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, St{\'e}phane},
  booktitle={ICML},
  year={2021}
}

@inproceedings{da2022unsupervised,
  title={Unsupervised domain adaptation for video transformers in action recognition},
  author={Da Costa, Victor G Turrisi and Zara, Giacomo and Rota, Paolo and Oliveira-Santos, Thiago and Sebe, Nicu and Murino, Vittorio and Ricci, Elisa},
  booktitle={ICPR},
  year={2022}
}

@article{peh2024learning,
  title={Learning to Visually Connect Actions and their Effects},
  author={Peh, Eric and Parmar, Paritosh and Fernando, Basura},
  journal={arXiv:2401.10805},
  year={2024}
}

@inproceedings{zhang2022skeletal,
  title={Skeletal twins: Unsupervised skeleton-based action representation learning},
  author={Zhang, Haoyuan and Hou, Yonghong and Zhang, Wenjing},
  booktitle={ICME},
  year={2022},
}

@inproceedings{sun2023unified,
  title={Unified multi-modal unsupervised representation learning for skeleton-based action understanding},
  author={Sun, Shengkai and Liu, Daizong and Dong, Jianfeng and Qu, Xiaoye and Gao, Junyu and Yang, Xun and Wang, Xun and Wang, Meng},
  booktitle={ACM MM},
  year={2023}
}

@inproceedings{zhou2023self,
  title={Self-supervised action representation learning from partial spatio-temporal skeleton sequences},
  author={Zhou, Yujie and Duan, Haodong and Rao, Anyi and Su, Bing and Wang, Jiaqi},
  booktitle={AAAI},
  year={2023}
}

@inproceedings{bardes2021vicreg,
  title={Vicreg: Variance-invariance-covariance regularization for self-supervised learning},
  author={Bardes, Adrien and Ponce, Jean and LeCun, Yann},
  booktitle={ICLR},
  year={2021}
}

@article{yang2023contrastive,
  title={Contrastive self-supervised representation learning without negative samples for multimodal human action recognition},
  author={Yang, Huaigang and Ren, Ziliang and Yuan, Huaqiang and Xu, Zhenyu and Zhou, Jun},
  journal={Frontiers in Neuroscience},
  year={2023}
}

@article{bardes2023mc,
  title={Mc-jepa: A joint-embedding predictive architecture for self-supervised learning of motion and content features},
  author={Bardes, Adrien and Ponce, Jean and LeCun, Yann},
  journal={arXiv:2307.12698},
  year={2023}
}

@inproceedings{yu2024evolve,
  title={Evolve: Enhancing unsupervised continual learning with multiple experts},
  author={Yu, Xiaofan and Rosing, Tajana and Guo, Yunhui},
  booktitle={WACV},
  year={2024}
}

@inproceedings{tirupattur2021modeling,
  title={Modeling multi-label action dependencies for temporal action localization},
  author={Tirupattur, Praveen and Duarte, Kevin and Rawat, Yogesh S and Shah, Mubarak},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{ryoo2009spatio,
  title={Spatio-temporal relationship match: Video structure comparison for recognition of complex human activities},
  author={Ryoo, Michael S and Aggarwal, Jake K},
  booktitle={ICCV},
  year={2009}
}

@inproceedings{liu2009recognizing,
  title={Recognizing realistic actions from videos ``in the wild'''},
  author={Liu, Jingen and Luo, Jiebo and Shah, Mubarak},
  booktitle={CVPR},
  year={2009}
}

@inproceedings{wong2007extracting,
  title={Extracting spatiotemporal interest points using global information},
  author={Wong, Shu-Fai and Cipolla, Roberto},
  booktitle={ICCV},
  year={2007},
}


@inproceedings{marszalek2009actions,
  title={Actions in context},
  author={Marszalek, Marcin and Laptev, Ivan and Schmid, Cordelia},
  booktitle={CVPR},
  year={2009}
}

@inproceedings{patron2010high,
  title={High Five: Recognising human interactions in TV shows.},
  author={Patron-Perez, Alonso and Marszalek, Marcin and Zisserman, Andrew and Reid, Ian},
  booktitle={BMVC},
  year={2010}
}

@article{reddy2013recognizing,
  title={Recognizing 50 human action categories of web videos},
  author={Reddy, Kishore K and Shah, Mubarak},
  journal={MVA},
  year={2013}
}

@inproceedings{niebles2010modeling,
  title={Modeling temporal structure of decomposable motion segments for activity classification},
  author={Niebles, Juan Carlos and Chen, Chih-Wei and Fei-Fei, Li},
  booktitle={ECCV},
  year={2010}
}

@inproceedings{kuehne2011hmdb,
  title={HMDB: a large video database for human motion recognition},
  author={Kuehne, Hildegard and Jhuang, Hueihan and Garrote, Est{\'\i}baliz and Poggio, Tomaso and Serre, Thomas},
  booktitle={ICCV},
  year={2011}
}

@inproceedings{jiang2011consumer,
  title={Consumer video understanding: A benchmark database and an evaluation of human and machine performance},
  author={Jiang, Yu-Gang and Ye, Guangnan and Chang, Shih-Fu and Ellis, Daniel and Loui, Alexander C},
  booktitle={ICMR},
  year={2011}
}

@article{soomro2012ucf101,
  title={UCF101: A dataset of 101 human actions classes from videos in the wild},
  author={Soomro, Khurram and Zamir, Amir Roshan and Shah, Mubarak},
  journal={arXiv:1212.0402},
  year={2012}
}

@inproceedings{sung2012unstructured,
  title={Unstructured human activity detection from rgbd images},
  author={Sung, Jaeyong and Ponce, Colin and Selman, Bart and Saxena, Ashutosh},
  booktitle={ICRA},
  year={2012}
}

@inproceedings{rohrbach2012database,
  title={A database for fine grained activity detection of cooking activities},
  author={Rohrbach, Marcus and Amin, Sikandar and Andriluka, Mykhaylo and Schiele, Bernt},
  booktitle={CVPR},
  year={2012}
}

@inproceedings{stein2013combining,
  title={Combining embedded accelerometers with computer vision for recognizing food preparation activities},
  author={Stein, Sebastian and McKenna, Stephen J},
  booktitle={UbiComp},
  year={2013}
}

@article{koppula2013learning,
  title={Learning human activities and object affordances from rgb-d videos},
  author={Koppula, Hema Swetha and Gupta, Rudhir and Saxena, Ashutosh},
  journal={IJRR},
  year={2013},
}

@inproceedings{karpathy2014large,
  title={Large-scale video classification with convolutional neural networks},
  author={Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and Fei-Fei, Li},
  booktitle={CVPR},
  year={2014}
}

@inproceedings{li2015delving,
  title={Delving into egocentric actions},
  author={Li, Yin and Ye, Zhefan and Rehg, James M},
  booktitle={CVPR},
  year={2015}
}

@inproceedings{caba2015activitynet,
  title={Activitynet: A large-scale video benchmark for human activity understanding},
  author={Caba Heilbron, Fabian and Escorcia, Victor and Ghanem, Bernard and Carlos Niebles, Juan},
  booktitle={CVPR},
  year={2015}
}

@inproceedings{wu2015watch,
  title={Watch-n-patch: Unsupervised understanding of actions and relations},
  author={Wu, Chenxia and Zhang, Jiemi and Savarese, Silvio and Saxena, Ashutosh},
  booktitle={CVPR},
  year={2015}
}

@inproceedings{kahatapitiya2024victr,
  title={Victr: Video-conditioned text representations for activity recognition},
  author={Kahatapitiya, Kumara and Arnab, Anurag and Nagrani, Arsha and Ryoo, Michael S},
  booktitle={CVPR},
  year={2024}
}


@inproceedings{srivastava2024omnivec2,
  title={OmniVec2-A Novel Transformer based Network for Large Scale Multimodal and Multitask Learning},
  author={Srivastava, Siddharth and Sharma, Gaurav},
  booktitle={CVPR},
  year={2024}
}

@article{dai2022one,
  title={One model, multiple modalities: A sparsely activated approach for text, sound, image, video and code},
  author={Dai, Yong and Tang, Duyu and Liu, Liangxin and Tan, Minghuan and Zhou, Cong and Wang, Jingquan and Feng, Zhangyin and Zhang, Fan and Hu, Xueyu and Shi, Shuming},
  journal={arXiv:2205.06126,},
  year={2022}
}

@inproceedings{xue2023dynamic,
  title={Dynamic multimodal fusion},
  author={Xue, Zihui and Marculescu, Radu},
  booktitle={CVPR},
  year={2023}
}

@article{abu2016youtube,
  title={Youtube-8m: A large-scale video classification benchmark},
  author={Abu-El-Haija, Sami and Kothari, Nisarg and Lee, Joonseok and Natsev, Paul and Toderici, George and Varadarajan, Balakrishnan and Vijayanarasimhan, Sudheendra},
  journal={arXiv:1609.08675},
  year={2016}
}

@inproceedings{sigurdsson2016hollywood,
  title={Hollywood in homes: Crowdsourcing data collection for activity understanding},
  author={Sigurdsson, Gunnar A and Varol, G{\"u}l and Wang, Xiaolong and Farhadi, Ali and Laptev, Ivan and Gupta, Abhinav},
  booktitle={ECCV},
  year={2016}
}

@inproceedings{van2016spatio,
  title={Spatio-temporal detection of fine-grained dyadic human interactions},
  author={Van Gemeren, Coert and Poppe, Ronald and Veltkamp, Remco C},
  booktitle={HBU},
  year={2016}
}

@inproceedings{li2016recognition,
  title={Recognition of ongoing complex activities by sequence prediction over a hierarchical label space},
  author={Li, Wenbin and Fritz, Mario},
  booktitle={WACV},
  year={2016}
}

@article{edwards2016pose,
  title={From pose to activity: Surveying datasets and introducing CONVERSE},
  author={Edwards, Michael and Deng, Jingjing and Xie, Xianghua},
  journal={CVIU},
  year={2016}
}

@inproceedings{de2016online,
  title={Online action detection},
  author={De Geest, Roeland and Gavves, Efstratios and Ghodrati, Amir and Li, Zhenyang and Snoek, Cees G. M. and Tuytelaars, Tinne},
  booktitle={ECCV},
  year={2016}
}

@inproceedings{barekatain2017okutama,
  title={Okutama-action: An aerial view video dataset for concurrent human action detection},
  author={Barekatain, Mohammadamin and Mart{\'\i}, Miquel and Shih, Hsueh-Fu and Murray, Samuel and Nakayama, Kotaro and Matsuo, Yutaka and Prendinger, Helmut},
  booktitle={ICCVw},
  year={2017}
}

@article{kay2017kinetics,
  title={The kinetics human action video dataset},
  author={Kay, Will and Carreira, Joao and Simonyan, Karen and Zhang, Brian and Hillier, Chloe and Vijayanarasimhan, Sudheendra and Viola, Fabio and Green, Tim and Back, Trevor and Natsev, Paul and others},
  journal={arXiv:1705.06950},
  year={2017}
}

@inproceedings{goyal2017something,
  title={The" something something" video database for learning and evaluating visual common sense},
  author={Goyal, Raghav and Ebrahimi Kahou, Samira and Michalski, Vincent and Materzynska, Joanna and Westphal, Susanne and Kim, Heuna and Haenel, Valentin and Fruend, Ingo and Yianilos, Peter and Mueller-Freitag, Moritz and others},
  booktitle={ICCV},
  year={2017}
}

@article{monfort2019moments,
  title={Moments in time dataset: one million videos for event understanding},
  author={Monfort, Mathew and Andonian, Alex and Zhou, Bolei and Ramakrishnan, Kandan and Bargal, Sarah Adel and Yan, Tom and Brown, Lisa and Fan, Quanfu and Gutfreund, Dan and Vondrick, Carl and others},
  journal={IEEE TPAMI},
  year={2019}
}

@article{yeung2018every,
  title={Every moment counts: Dense detailed labeling of actions in complex videos},
  author={Yeung, Serena and Russakovsky, Olga and Jin, Ning and Andriluka, Mykhaylo and Mori, Greg and Fei-Fei, Li},
  journal={IJCV},
  year={2018}
}

@article{weinzaepfel2016towards,
  title={Towards weakly-supervised action localization},
  author={Weinzaepfel, Philippe and Martin, Xavier and Schmid, Cordelia},
  journal={arXiv:1605.05197 },
  year={2016}
}

@inproceedings{li2018resound,
  title={Resound: Towards action recognition without representation bias},
  author={Li, Yingwei and Li, Yi and Vasconcelos, Nuno},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{damen2018scaling,
  title={Scaling egocentric vision: The epic-kitchens dataset},
  author={Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria and Fidler, Sanja and Furnari, Antonino and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan and Perrett, Toby and Price, Will and others},
  booktitle={ECCV},
  year={2018}
}

@article{carreira2018short,
  title={A short note about kinetics-600},
  author={Carreira, Joao and Noland, Eric and Banki-Horvath, Andras and Hillier, Chloe and Zisserman, Andrew},
  journal={arXiv:1808.01340},
  year={2018}
}

@inproceedings{fouhey2018lifestyle,
  title={From lifestyle vlogs to everyday interactions},
  author={Fouhey, David F and Kuo, Wei-cheng and Efros, Alexei A and Malik, Jitendra},
  booktitle={CVPR},
  year={2018}
}

@article{carreira2019short,
  title={A short note on the kinetics-700 human action dataset},
  author={Carreira, Joao and Noland, Eric and Hillier, Chloe and Zisserman, Andrew},
  journal={arXiv:1907.06987},
  year={2019}
}

@inproceedings{zhao2019hacs,
  title={Hacs: Human action clips and segments dataset for recognition and temporal localization},
  author={Zhao, Hang and Torralba, Antonio and Torresani, Lorenzo and Yan, Zhicheng},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{ghadiyaram2019large,
  title={Large-scale weakly-supervised pre-training for video action recognition},
  author={Ghadiyaram, Deepti and Tran, Du and Mahajan, Dhruv},
  booktitle={CVPR},
  year={2019}
}

@article{piergiovanni2020avid,
  title={Avid dataset: Anonymized videos from diverse countries},
  author={Piergiovanni, AJ and Ryoo, Michael},
  journal={NeurIPS},
  year={2020}
}

@inproceedings{diba2020large,
  title={Large scale holistic video understanding},
  author={Diba, Ali and Fayyaz, Mohsen and Sharma, Vivek and Paluri, Manohar and Gall, J{\"u}rgen and Stiefelhagen, Rainer and Van Gool, Luc},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{chung2021haa500,
  title={Haa500: Human-centric atomic action dataset with curated videos},
  author={Chung, Jihoon and Wuu, Cheng-hsin and Yang, Hsuan-ru and Tai, Yu-Wing and Tang, Chi-Keung},
  booktitle={ICCV},
  year={2021}
}

@article{smaira2020short,
  title={A short note on the kinetics-700-2020 human action dataset},
  author={Smaira, Lucas and Carreira, Jo{\~a}o and Noland, Eric and Clancy, Ellen and Wu, Amy and Zisserman, Andrew},
  journal={arXiv:2010.10864},
  year={2020}
}

@inproceedings{shao2020finegym,
  title={Finegym: A hierarchical video dataset for fine-grained action understanding},
  author={Shao, Dian and Zhao, Yue and Dai, Bo and Lin, Dahua},
  booktitle={CVPR},
  year={2020}
}

@article{damen2022rescaling,
  title={Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100},
  author={Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria and Furnari, Antonino and Kazakos, Evangelos and Ma, Jian and Moltisanti, Davide and Munro, Jonathan and Perrett, Toby and Price, Will and others},
  journal={IJCV},
  year={2022}
}

%% ---- ACTION RECOGNITION DATASETS ----


%% ----- END OF ACTION REC DATASETS -----


%% ---- ACTION RECOGNITION PAPERS ----

@inproceedings{dollar2005behavior,
  title={Behavior recognition via sparse spatio-temporal features},
  author={Doll{\'a}r, Piotr and Rabaud, Vincent and Cottrell, Garrison and Belongie, Serge},
  booktitle={VS-PETS},
  year={2005}
}

@inproceedings{blank2005actions,
  title={Actions as space-time shapes},
  author={Blank, Moshe and Gorelick, Lena and Shechtman, Eli and Irani, Michal and Basri, Ronen},
  booktitle={ICCV},
  year={2005}
}

@inproceedings{yilmaz2005recognizing,
  title={Recognizing human actions in videos acquired by uncalibrated moving cameras},
  author={Yilmaz, Alper and Shah, Mubarak},
  booktitle={ICCV},
  year={2005},
}

@inproceedings{oikonomopoulos2005spatiotemporal,
  title={Spatiotemporal saliency for human action recognition},
  author={Oikonomopoulos, Antonios and Patras, Ioannis and Pantic, Maja},
  booktitle={ICME},
  year={2005}
}

@inproceedings{weinland2007action,
  title={Action recognition from arbitrary views using 3d exemplars},
  author={Weinland, Daniel and Boyer, Edmond and Ronfard, Remi},
  booktitle={ICCV},
  year={2007}
}

@inproceedings{niebles2007hierarchical,
  title={A hierarchical model of shape and appearance for human action classification},
  author={Niebles, Juan Carlos and Fei-Fei, Li},
  booktitle={CVPR},
  year={2007}
}

@article{gupta2009observing,
  title={Observing human-object interactions: Using spatial and functional compatibility for recognition},
  author={Gupta, Abhinav and Kembhavi, Aniruddha and Davis, Larry S},
  journal={IEEE TPAMI},
  year={2009}
}

@inproceedings{ikizler2010object,
  title={Object, scene and actions: Combining multiple features for human action recognition},
  author={Ikizler-Cinbis, Nazli and Sclaroff, Stan},
  booktitle={ECCV},
  year={2010}
}


@inproceedings{pishchulin2013strong,
  title={Strong appearance and expressive spatial models for human pose estimation},
  author={Pishchulin, Leonid and Andriluka, Mykhaylo and Gehler, Peter and Schiele, Bernt},
  booktitle={CVPR},
  year={2013}
}

@article{kviatkovsky2014online,
  title={Online action recognition using covariance of shape and motion},
  author={Kviatkovsky, Igor and Rivlin, Ehud and Shimshoni, Ilan},
  journal={CVIU},
  year={2014}
}


@inproceedings{rahmani2014real,
  title={Real time action recognition using histograms of depth gradients and random decision forests},
  author={Rahmani, Hossein and Mahmood, Arif and Huynh, Du Q and Mian, Ajmal},
  booktitle={WACV},
  year={2014}
}



@inproceedings{yao2010modeling,
  title={Modeling mutual context of object and human pose in human-object interaction activities},
  author={Yao, Bangpeng and Fei-Fei, Li},
  booktitle={CVPR},
  year={2010}
}


@inproceedings{ke2007spatio,
  title={Spatio-temporal shape and flow correlation for action recognition},
  author={Ke, Yan and Sukthankar, Rahul and Hebert, Martial},
  booktitle={CVPR},
  year={2007}
}

@inproceedings{wong2007learning,
  title={Learning motion categories using both semantic and structural information},
  author={Wong, Shu-Fai and Kim, Tae-Kyun and Cipolla, Roberto},
  booktitle={CVPR},
  year={2007}
}

@article{wang2007learning,
  title={Learning and matching of dynamic shape manifolds for human action recognition},
  author={Wang, Liang and Suter, David},
  journal={IEEE T-IP},
  year={2007}
}

@inproceedings{ikizler2007human,
  title={Human action recognition using distribution of oriented rectangular patches},
  author={Ikizler, Nazl{\i} and Duygulu, P{\i}nar},
  booktitle={Workshop on Human Motion},
  year={2007},
}

@inproceedings{ikizler2007searching,
  title={Searching video for complex activities with finite state models},
  author={Ikizler, Nazli and Forsyth, David},
  booktitle={CVPR},
  year={2007}
}

@inproceedings{husz2007human,
  title={Human activity recognition with action primitives},
  author={Husz, Zsolt L and Wallace, Andrew M and Green, Patrick R},
  booktitle={AVSS},
  year={2007}
}

@inproceedings{schindler2008action,
  title={Action snippets: How many frames does human action recognition require?},
  author={Schindler, Konrad and Van Gool, Luc},
  booktitle={CVPR},
  year={2008}
}

@article{ali2008human,
  title={Human action recognition in videos using kinematic features and multiple instance learning},
  author={Ali, Saad and Shah, Mubarak},
  journal={IEEE TPAMI},
  year={2008}
}

@inproceedings{fathi2008action,
  title={Action recognition by learning mid-level motion features},
  author={Fathi, Alireza and Mori, Greg},
  booktitle={CVPR},
  year={2008}
}

@inproceedings{tran2008human,
  title={Human activity recognition with metric learning},
  author={Tran, Du and Sorokin, Alexander},
  booktitle={ECCV},
  year={2008}
}

@inproceedings{liu2008recognizing,
  title={Recognizing human actions using multiple features},
  author={Liu, Jingen and Ali, Saad and Shah, Mubarak},
  booktitle={CVPR},
  year={2008}
}

@inproceedings{weinland2008action,
  title={Action recognition using exemplar-based embedding},
  author={Weinland, Daniel and Boyer, Edmond},
  booktitle={CVPR},
  year={2008}
}

@inproceedings{zhang2008motion,
  title={Motion context: A new representation for human action recognition},
  author={Zhang, Ziming and Hu, Yiqun and Chan, Syin and Chia, Liang-Tien},
  booktitle={ECCV},
  year={2008}
}

@inproceedings{yan2008learning,
  title={Learning 4D action feature models for arbitrary view action recognition},
  author={Yan, Pingkun and Khan, Saad M and Shah, Mubarak},
  booktitle={CVPR},
  year={2008}
}

@inproceedings{natarajan2008online,
  title={Online, real-time tracking and recognition of human actions},
  author={Natarajan, Pradeep and Nevatia, Ramakant},
  booktitle={WMVC},
  year={2008}
}

@inproceedings{batra2008space,
  title={Space-time shapelets for action recognition},
  author={Batra, Dhruv and Chen, Tsuhan and Sukthankar, Rahul},
  booktitle={WMVC},
  year={2008}
}

@article{tran2012part,
  title={Part-based motion descriptor image for human action recognition},
  author={Tran, Khai N and Kakadiaris, Ioannis A and Shah, Shishir K},
  journal={PR},
  year={2012}
}


@article{liang2008learning,
  title={Learning atomic human actions using variable-length Markov models},
  author={Liang, Yu-Ming and Shih, Sheng-Wen and Shih, Arthur Chun-Chieh and Liao, Hong-Yuan Mark and Lin, Cheng-Chung},
  journal={IEEE TSMC},
  year={2008}
}

@inproceedings{wang2009evaluation,
  title={Evaluation of local spatio-temporal features for action recognition},
  author={Wang, Heng and Ullah, Muhammad Muneeb and Klaser, Alexander and Laptev, Ivan and Schmid, Cordelia},
  booktitle={BMVC},
  year={2009}
}

@inproceedings{yeffet2009local,
  title={Local trinary patterns for human action recognition},
  author={Yeffet, Lahav and Wolf, Lior},
  booktitle={ICCV},
  year={2009}
}


@inproceedings{duchenne2009automatic,
  title={Automatic annotation of human actions in video},
  author={Duchenne, Olivier and Laptev, Ivan and Sivic, Josef and Bach, Francis and Ponce, Jean},
  booktitle={ICCV},
  year={2009}
}


@article{iosifidis2012view,
  title={View-invariant action recognition based on artificial neural networks},
  author={Iosifidis, Alexandros and Tefas, Anastasios and Pitas, Ioannis},
  journal={IEEE TNNLS},
  year={2012}
}

@article{wang2009human,
  title={Human action recognition by semilatent topic models},
  author={Wang, Yang and Mori, Greg},
  journal={IEEE TPAMI},
  year={2009}
}

@article{escobar2009action,
  title={Action recognition using a bio-inspired feedforward spiking network},
  author={Escobar, Maria-Jose and Masson, Guillaume S and Vieville, Thierry and Kornprobst, Pierre},
  journal={IJCV},
  year={2009}
}

@inproceedings{sheikh2005exploring,
  title={Exploring the space of a human action},
  author={Sheikh, Yaser and Sheikh, Mumtaz and Shah, Mubarak},
  booktitle={ICCV},
  year={2005}
}

@article{niebles2008unsupervised,
  title={Unsupervised learning of human action categories using spatial-temporal words},
  author={Niebles, Juan Carlos and Wang, Hongcheng and Fei-Fei, Li},
  journal={IJCV},
  year={2008}
}

@inproceedings{sun2009action,
  title={Action recognition via local descriptors and holistic features},
  author={Sun, Xinghua and Chen, Mingyu and Hauptmann, Alexander},
  booktitle={CVPRw},
  year={2009}
}

@inproceedings{reddy2009incremental,
  title={Incremental action recognition using feature-tree},
  author={Reddy, Kishore K and Liu, Jingen and Shah, Mubarak},
  booktitle={ICCV},
  year={2009}
}

@inproceedings{willems2009exemplar,
  title={Exemplar-based Action Recognition in Video.},
  author={Willems, Geert and Becker, Jan Hendrik and Tuytelaars, Tinne and Van Gool, Luc},
  booktitle={BMVC},
  year={2009}
}

@article{gilbert2010action,
  title={Action recognition using mined hierarchical compound features},
  author={Gilbert, Andrew and Illingworth, John and Bowden, Richard},
  journal={IEEE TPAMI},
  year={2010}
}

@inproceedings{taylor2010convolutional,
  title={Convolutional learning of spatio-temporal features},
  author={Taylor, Graham W and Fergus, Rob and LeCun, Yann and Bregler, Christoph},
  booktitle={ECCV},
  year={2010}
}

@inproceedings{bilen2016dynamic,
  title={Dynamic image networks for action recognition},
  author={Bilen, Hakan and Fernando, Basura and Gavves, Efstratios and Vedaldi, Andrea and Gould, Stephen},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{yue2015beyond,
  title={Beyond short snippets: Deep networks for video classification},
  author={Yue-Hei Ng, Joe and Hausknecht, Matthew and Vijayanarasimhan, Sudheendra and Vinyals, Oriol and Monga, Rajat and Toderici, George},
  booktitle={CVPR},
  year={2015}
}

@article{simonyan2014two,
  title={Two-stream convolutional networks for action recognition in videos},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={NeurIPS},
  year={2014}
}

@article{ji20123d,
  title={3D convolutional neural networks for human action recognition},
  author={Ji, Shuiwang and Xu, Wei and Yang, Ming and Yu, Kai},
  journal={IEEE TPAMI},
  year={2012}
}

@inproceedings{tran2015learning,
  title={Learning spatiotemporal features with 3d convolutional networks},
  author={Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
  booktitle={ICCV},
  year={2015}
}

@inproceedings{sadanand2012action,
  title={Action bank: A high-level representation of activity in video},
  author={Sadanand, Sreemanananth and Corso, Jason J},
  booktitle={CVPR},
  year={2012}
}

@inproceedings{laptev2003space,
  title={Space-time Interest Points},
  author={Laptev, Ivan and Lindeberg, Tony},
  booktitle={ICCV},
  year={2003}
}

@inproceedings{amer2012sum,
  title={Sum-product networks for modeling activities with stochastic structure},
  author={Amer, Mohamed R and Todorovic, Sinisa},
  booktitle={CVPR},
  year={2012}
}


@article{bobick2001recognition,
  title={The recognition of human movement using temporal templates},
  author={Bobick, Aaron F. and Davis, James W.},
  journal={IEEE TPAMI},
  year={2001}
}

@inproceedings{kim2009observe,
  title={Observe locally, infer globally: a space-time MRF for detecting abnormal activities with incremental updates},
  author={Kim, Jaechul and Grauman, Kristen},
  booktitle={CVPR},
  year={2009},
}

@article{wren1997pfinder,
  title={Pfinder: Real-time tracking of the human body},
  author={Wren, Christopher Richard and Azarbayejani, Ali and Darrell, Trevor and Pentland, Alex Paul},
  journal={IEEE TPAMI},
  year={1997}
}


@inproceedings{ko2023open,
  title={Open-Vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models},
  author={Ko, Dohwan and Lee, Ji Soo and Choi, Miso and Chu, Jaewon and Park, Jihwan and Kim, Hyunwoo J},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{pan2024synthesizing,
  title={Synthesizing coherent story with auto-regressive latent diffusion models},
  author={Pan, Xichen and Qin, Pengda and Li, Yuhong and Xue, Hui and Chen, Wenhu},
  booktitle={WACV},
  year={2024}
}

@inproceedings{ni2023conditional,
  title={Conditional image-to-video generation with latent flow diffusion models},
  author={Ni, Haomiao and Shi, Changhao and Li, Kai and Huang, Sharon X and Min, Martin Renqiang},
  booktitle={CVPR},
  year={2023}
}

@article{tewel2022zero,
  title={Zero-shot video captioning with evolving pseudo-tokens},
  author={Tewel, Yoad and Shalev, Yoav and Nadler, Roy and Schwartz, Idan and Wolf, Lior},
  journal={arXiv:2207.11100},
  year={2022}
}

@inproceedings{bachmann2022multimae,
  title={Multimae: Multi-modal multi-task masked autoencoders},
  author={Bachmann, Roman and Mizrahi, David and Atanov, Andrei and Zamir, Amir},
  booktitle={ECCV},
  year={2022},
}

@article{yilmaz2006matching,
  title={Matching actions in presence of camera motion},
  author={Yilmaz, Alper and Shah, Mubarak},
  journal={CVIU},
  year={2006}
}

@inproceedings{shechtman2005space,
  title={Space-time behavior based correlation},
  author={Shechtman, Eli and Irani, Michal},
  booktitle={CVPR},
  year={2005}
}



@inproceedings{jain2015modeep,
  title={Modeep: A deep learning framework using motion features for human pose estimation},
  author={Jain, Arjun and Tompson, Jonathan and LeCun, Yann and Bregler, Christoph},
  booktitle={ACCV},
  year={2015}
}

@inproceedings{le2011learning,
  title={Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis},
  author={Le, Quoc V and Zou, Will Y and Yeung, Serena Y and Ng, Andrew Y},
  booktitle={CVPR},
  year={2011}
}

@inproceedings{baccouche2011sequential,
  title={Sequential deep learning for human action recognition},
  author={Baccouche, Moez and Mamalet, Franck and Wolf, Christian and Garcia, Christophe and Baskurt, Atilla},
  booktitle={HBU},
  year={2011}
}

@inproceedings{sharma2015action,
  title={Action recognition using visual attention},
  author={Sharma, Shikhar and Kiros, Ryan and Salakhutdinov, Ruslan},
  booktitle={ICLR},
  year={2015}
}

@inproceedings{ballas2015delving,
  title={Delving deeper into convolutional networks for learning video representations},
  author={Ballas, Nicolas and Yao, Li and Pal, Chris and Courville, Aaron},
  booktitle={ICLR},
  year={2015}
}

@inproceedings{sun2015human,
  title={Human action recognition using factorized spatio-temporal convolutional networks},
  author={Sun, Lin and Jia, Kui and Yeung, Dit-Yan and Shi, Bertram E},
  booktitle={ICCV},
  year={2015}
}

@inproceedings{fernando2015modeling,
  title={Modeling video evolution for action recognition},
  author={Fernando, Basura and Gavves, Efstratios and Oramas, Jose M and Ghodrati, Amir and Tuytelaars, Tinne},
  booktitle={CVPR},
  year={2015}
}

@article{fernando2016rank,
  title={Rank pooling for action recognition},
  author={Fernando, Basura and Gavves, Efstratios and Oramas, Jos{\'e} and Ghodrati, Amir and Tuytelaars, Tinne},
  journal={IEEE TPAMI},
  year={2016}
}

@inproceedings{feichtenhofer2017spatiotemporal,
  title={Spatiotemporal multiplier networks for video action recognition},
  author={Feichtenhofer, Christoph and Pinz, Axel and Wildes, Richard P},
  booktitle={CVPR},
  year={2017}
}

@article{ullah2017action,
  title={Action recognition in video sequences using deep bi-directional LSTM with CNN features},
  author={Ullah, Amin and Ahmad, Jamil and Muhammad, Khan and Sajjad, Muhammad and Baik, Sung Wook},
  journal={IEEE access},
  year={2017}
}

@article{varol2017long,
  title={Long-term temporal convolutions for action recognition},
  author={Varol, G{\"u}l and Laptev, Ivan and Schmid, Cordelia},
  journal={IEEE TPAMI},
  year={2017}
}

@article{du2017recurrent,
  title={Recurrent spatial-temporal attention network for action recognition in videos},
  author={Du, Wenbin and Wang, Yali and Qiao, Yu},
  journal={IEEE T-IP},
  year={2017}
}

@inproceedings{zhang2016real,
  title={Real-time action recognition with enhanced motion vector CNNs},
  author={Zhang, Bowen and Wang, Limin and Wang, Zhe and Qiao, Yu and Wang, Hanli},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{zhu2016key,
  title={A key volume mining deep framework for action recognition},
  author={Zhu, Wangjiang and Hu, Jie and Sun, Gang and Cao, Xudong and Qiao, Yu},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{zhou2018temporal,
  title={Temporal relational reasoning in videos},
  author={Zhou, Bolei and Andonian, Alex and Oliva, Aude and Torralba, Antonio},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{chen2021watch,
  title={Watch only once: An end-to-end video action detection framework},
  author={Chen, Shoufa and Sun, Peize and Xie, Enze and Ge, Chongjian and Wu, Jiannan and Ma, Lan and Shen, Jiajun and Luo, Ping},
  booktitle={ICCV},
  year={2021}
}


@inproceedings{liu2016ssd,
  title={Ssd: Single shot multibox detector},
  author={Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C},
  booktitle={ECCV},
  year={2016}
}

@article{gulati2020conformer,
  title={Conformer: Convolution-augmented Transformer for Speech Recognition},
  author={Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and others},
  journal={Interspeech},
  year={2020},
}

@inproceedings{liu2022learning_the,
  title={Learning the spectrogram temporal resolution for audio classification},
  author={Liu, Haohe and Liu, Xubo and Kong, Qiuqiang and Wang, Wenwu and Plumbley, Mark D},
  booktitle={AAAI},
  year={2022}
}

@article{huang2022masked,
  title={Masked autoencoders that listen},
  author={Huang, Po-Yao and Xu, Hu and Li, Juncheng and Baevski, Alexei and Auli, Michael and Galuba, Wojciech and Metze, Florian and Feichtenhofer, Christoph},
  journal={NeurIPS},
  year={2022}
}

@article{morrongiello1998developmental,
  title={Developmental changes in associations between auditory-visual events},
  author={Morrongiello, Barbara A and Fenwick, Kimberley D and Nutley, Tanya},
  journal={Infant Behavior and Development},
  year={1998}
}

@article{chen1998audio,
  title={Audio-visual integration in multimodal communication},
  author={Chen, Tsuhan and Rao, Ram R},
  journal={Proceedings of the IEEE},
  year={1998}
}

@article{matthews2002extraction,
  title={Extraction of visual features for lipreading},
  author={Matthews, Iain and Cootes, Timothy F and Bangham, J Andrew and Cox, Stephen and Harvey, Richard},
  journal={IEEE TPAMI},
  year={2002}
}

@article{aleksic2006audio,
  title={Audio-visual biometrics},
  author={Aleksic, Petar S and Katsaggelos, Aggelos K},
  journal={Proceedings of the IEEE},
  year={2006}
}

@inproceedings{feng2024coarse,
  title={From Coarse to Fine: Efficient Training for Audio Spectrogram Transformers},
  author={Feng, Jiu and Erol, Mehmet Hamza and Chung, Joon Son and Senocak, Arda},
  booktitle={ICASSP},
  year={2024}
}

@inproceedings{redmon2016you,
  title={You only look once: Unified, real-time object detection},
  author={Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  booktitle={CVPR},
  year={2016}
}


@inproceedings{sevilla2019integration,
  title={On the integration of optical flow and action recognition},
  author={Sevilla-Lara, Laura and Liao, Yiyi and G{\"u}ney, Fatma and Jampani, Varun and Geiger, Andreas and Black, Michael J},
  booktitle={GCPR},
  year={2019}
}

@inproceedings{hoai2015improving,
  title={Improving human action recognition using score distribution and ranking},
  author={Hoai, Minh and Zisserman, Andrew},
  booktitle={ACCV},
  year={2015}
}

@article{girdhar2017attentional,
  title={Attentional pooling for action recognition},
  author={Girdhar, Rohit and Ramanan, Deva},
  journal={NeurIPS},
  year={2017}
}

@article{zong2021motion,
  title={Motion saliency based multi-stream multiplier ResNets for action recognition},
  author={Zong, Ming and Wang, Ruili and Chen, Xiubo and Chen, Zhe and Gong, Yuanhao},
  journal={IVC},
  year={2021}
}

@inproceedings{chung2016signs,
  title={Signs in time: Encoding human motion as a temporal image},
  author={Chung, J and Zisserman, A},
  booktitle={ECCVw},
  year={2016}
}

@inproceedings{tran2018closer,
  title={A closer look at spatiotemporal convolutions for action recognition},
  author={Tran, Du and Wang, Heng and Torresani, Lorenzo and Ray, Jamie and LeCun, Yann and Paluri, Manohar},
  booktitle={CVPR},
  year={2018}
}

@article{chen20182,
  title={A\^{} 2-nets: Double attention networks},
  author={Chen, Yunpeng and Kalantidis, Yannis and Li, Jianshu and Yan, Shuicheng and Feng, Jiashi},
  journal={NeurIPS},
  year={2018}
}

@inproceedings{chen2018multi,
  title={Multi-fiber networks for video recognition},
  author={Chen, Yunpeng and Kalantidis, Yannis and Li, Jianshu and Yan, Shuicheng and Feng, Jiashi},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{hegde2018morph,
  title={Morph: Flexible acceleration for 3d cnn-based video understanding},
  author={Hegde, Kartik and Agrawal, Rohit and Yao, Yulun and Fletcher, Christopher W},
  booktitle={MICRO},
  year={2018}
}

@inproceedings{dwibedi2018temporal,
  title={Temporal reasoning in videos using convolutional gated recurrent units},
  author={Dwibedi, Debidatta and Sermanet, Pierre and Tompson, Jonathan},
  booktitle={CVPRw},
  year={2018}
}

@inproceedings{jiang2019stm,
  title={Stm: Spatiotemporal and motion encoding for action recognition},
  author={Jiang, Boyuan and Wang, MengMeng and Gan, Weihao and Wu, Wei and Yan, Junjie},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{tran2019video,
  title={Video classification with channel-separated convolutional networks},
  author={Tran, Du and Wang, Heng and Torresani, Lorenzo and Feiszli, Matt},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{xu2019self,
  title={Self-supervised spatiotemporal learning via video clip order prediction},
  author={Xu, Dejing and Xiao, Jun and Zhao, Zhou and Shao, Jian and Xie, Di and Zhuang, Yueting},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{qiu2017learning,
  title={Learning spatio-temporal representation with pseudo-3d residual networks},
  author={Qiu, Zhaofan and Yao, Ting and Mei, Tao},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{wang2018appearance,
  title={Appearance-and-relation networks for video classification},
  author={Wang, Limin and Li, Wei and Li, Wen and Van Gool, Luc},
  booktitle={CVPR},
  year={2018}
}

@techreport{deguide2008guide,
  author = {Fernando De la Torre Frade and Jessica K. Hodgins and Adam W. Bargteil and Xavier Martin Artal and Justin C. Macey and Alexandre Collado I Castells and Josep Beltran},
  title = {Guide to the Carnegie Mellon University Multimodal Activity (CMU-MMAC) Database},
  year = {2008},
  institution={CMU}
}

@inproceedings{ye2023unified,
  title={A unified model for continuous conditional video prediction},
  author={Ye, Xi and Bilodeau, Guillaume-Alexandre},
  booktitle={CVPRw},
  year={2023}
}


@inproceedings{jhuang2013towards,
  title = {Towards understanding action recognition},
  author = {H. Jhuang and J. Gall and S. Zuffi and C. Schmid and M. J. Black},
  booktitle = {ICCV},
  year = {2013}
}

@inproceedings{ibrahim2016hierarchical,
  title={A hierarchical deep temporal model for group activity recognition},
  author={Ibrahim, Mostafa S and Muralidharan, Srikanth and Deng, Zhiwei and Vahdat, Arash and Mori, Greg},
  booktitle={CVPR},
  year={2016}
}

@article{sigurdsson2018charades,
  title={Charades-ego: A large-scale dataset of paired third and first person videos},
  author={Sigurdsson, Gunnar A and Gupta, Abhinav and Schmid, Cordelia and Farhadi, Ali and Alahari, Karteek},
  journal={arXiv:1804.09626},
  year={2018}
}

@article{dai2022toyota,
  title={Toyota smarthome untrimmed: Real-world untrimmed videos for activity detection},
  author={Dai, Rui and Das, Srijan and Sharma, Saurav and Minciullo, Luca and Garattoni, Lorenzo and Bremond, Francois and Francesca, Gianpiero},
  journal={IEEE TPAMI},
  year={2022},
}

@article{miech2020rareact,
  title={Rareact: A video dataset of unusual interactions},
  author={Miech, Antoine and Alayrac, Jean-Baptiste and Laptev, Ivan and Sivic, Josef and Zisserman, Andrew},
  journal={arXiv:2008.01018},
  year={2020}
}

@inproceedings{rai2021home,
  title={Home action genome: Cooperative compositional action understanding},
  author={Rai, Nishant and Chen, Haofeng and Ji, Jingwei and Desai, Rishi and Kozuka, Kazuki and Ishizaka, Shun and Adeli, Ehsan and Niebles, Juan Carlos},
  booktitle={CVPR},
  year={2021}
}

@article{liu2022fineaction,
  title={Fineaction: A fine-grained video dataset for temporal action localization},
  author={Liu, Yi and Wang, Limin and Wang, Yali and Ma, Xiao and Qiao, Yu},
  journal={IEEE T-IP},
  year={2022}
}

@inproceedings{doughty2022you,
  title={How do you do it? fine-grained action understanding with pseudo-adverbs},
  author={Doughty, Hazel and Snoek, Cees G. M.},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{pirsiavash2012detecting,
  title={Detecting activities of daily living in first-person camera views},
  author={Pirsiavash, Hamed and Ramanan, Deva},
  booktitle={CVPR},
  year={2012},
}

@inproceedings{mueller2017real,
  title={Real-time hand tracking under occlusion from an egocentric rgb-d sensor},
  author={Mueller, Franziska and Mehta, Dushyant and Sotnychenko, Oleksandr and Sridhar, Srinath and Casas, Dan and Theobalt, Christian},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{garcia2018first,
  title={First-person hand action benchmark with rgb-d videos and 3d hand pose annotations},
  author={Garcia-Hernando, Guillermo and Yuan, Shanxin and Baek, Seungryul and Kim, Tae-Kyun},
  booktitle={CVPR},
  year={2018}
}
@inproceedings{chao2021dexycb,
  title={DexYCB: A benchmark for capturing hand grasping of objects},
  author={Chao, Yu-Wei and Yang, Wei and Xiang, Yu and Molchanov, Pavlo and Handa, Ankur and Tremblay, Jonathan and Narang, Yashraj S and Van Wyk, Karl and Iqbal, Umar and Birchfield, Stan and others},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{hampali2020honnotate,
  title={Honnotate: A method for 3d annotation of hand and object poses},
  author={Hampali, Shreyas and Rad, Mahdi and Oberweger, Markus and Lepetit, Vincent},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{moon2020interhand2,
  title={Interhand2. 6m: A dataset and baseline for 3d interacting hand pose estimation from a single rgb image},
  author={Moon, Gyeongsik and Yu, Shoou-I and Wen, He and Shiratori, Takaaki and Lee, Kyoung Mu},
  booktitle={ECCV},
  year={2020},
}

@inproceedings{ohkawa2023assemblyhands,
    title     = {{AssemblyHands:} Towards Egocentric Activity Understanding via 3D Hand Pose Estimation},
    author    = {Takehiko Ohkawa and Kun He and Fadime Sener and Tomas Hodan and Luan Tran and Cem Keskin},
    booktitle = {CVPR},
    year      = {2023},
}

@inproceedings{grauman2024ego,
  title={Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives},
  author={Grauman, Kristen and Westbury, Andrew and Torresani, Lorenzo and Kitani, Kris and Malik, Jitendra and Afouras, Triantafyllos and Ashutosh, Kumar and Baiyya, Vijay and Bansal, Siddhant and Boote, Bikram and others},
  booktitle={CVPR},
  year={2024}
}
    

@inproceedings{hara2018can,
  title={Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?},
  author={Hara, Kensho and Kataoka, Hirokatsu and Satoh, Yutaka},
  booktitle={CVPR},
  year={2018}
}

@article{luo2021moma,
  title={Moma: Multi-object multi-actor activity parsing},
  author={Luo, Zelun and Xie, Wanze and Kapoor, Siddharth and Liang, Yiyun and Cooper, Michael and Niebles, Juan Carlos and Adeli, Ehsan and Li, Fei-Fei},
  journal={NeurIPS},
  year={2021}
}

@inproceedings{luo2019grouped,
  title={Grouped spatial-temporal aggregation for efficient action recognition},
  author={Luo, Chenxu and Yuille, Alan L},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{qiu2019learning,
  title={Learning spatio-temporal representation with local and global diffusion},
  author={Qiu, Zhaofan and Yao, Ting and Ngo, Chong-Wah and Tian, Xinmei and Mei, Tao},
  booktitle={CVPR},
  year={2019}
}

@article{choi2019can,
  title={Why can't I dance in the mall? learning to mitigate scene bias in action recognition},
  author={Choi, Jinwoo and Gao, Chen and Messou, Joseph CE and Huang, Jia-Bin},
  journal={NeurIPS},
  year={2019}
}

@inproceedings{suris2021learning,
  title={Learning the predictability of the future},
  author={Sur{\'\i}s, D{\'\i}dac and Liu, Ruoshi and Vondrick, Carl},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{liang2022visual,
  title={Visual abductive reasoning},
  author={Liang, Chen and Wang, Wenguan and Zhou, Tianfei and Yang, Yi},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{de2023there,
  title={Is there progress in activity progress prediction?},
  author={de Boer, Frans and van Gemert, Jan C and Dijkstra, Jouke and Pintea, Silvia L},
  booktitle={ICCVw},
  year={2023}
}

@inproceedings{li2023intentqa,
  title={Intentqa: Context-aware video intent reasoning},
  author={Li, Jiapeng and Wei, Ping and Han, Wenjuan and Fan, Lifeng},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{pei2011parsing,
  title={Parsing video events with goal inference and intent prediction},
  author={Pei, Mingtao and Jia, Yunde and Zhu, Song-Chun},
  booktitle={ICCV},
  year={2011}
}

@inproceedings{wang2021enhancing,
  title={Enhancing unsupervised video representation learning by decoupling the scene and the motion},
  author={Wang, Jinpeng and Gao, Yuting and Li, Ke and Hu, Jianguo and Jiang, Xinyang and Guo, Xiaowei and Ji, Rongrong and Sun, Xing},
  booktitle={AAAI},
  year={2021}
}

@inproceedings{lin2019tsm,
  title={Tsm: Temporal shift module for efficient video understanding},
  author={Lin, Ji and Gan, Chuang and Han, Song},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{feichtenhofer2019slowfast,
  title={Slowfast networks for video recognition},
  author={Feichtenhofer, Christoph and Fan, Haoqi and Malik, Jitendra and He, Kaiming},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{feichtenhofer2020x3d,
  title={X3d: Expanding architectures for efficient video recognition},
  author={Feichtenhofer, Christoph},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{kondratyuk2021movinets,
  title={Movinets: Mobile video networks for efficient video recognition},
  author={Kondratyuk, Dan and Yuan, Liangzhe and Li, Yandong and Zhang, Li and Tan, Mingxing and Brown, Matthew and Gong, Boqing},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{liu2022convnet,
  title={A convnet for the 2020s},
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{hussein2019timeception,
  title={Timeception for complex action recognition},
  author={Hussein, Noureldien and Gavves, Efstratios and Smeulders, Arnold WM},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{ji2020action,
  title={Action genome: Actions as compositions of spatio-temporal scene graphs},
  author={Ji, Jingwei and Krishna, Ranjay and Fei-Fei, Li and Niebles, Juan Carlos},
  booktitle={CVPR},
  year={2020}
}

@article{yang2020video,
  title={Video representation learning with visual tempo consistency},
  author={Yang, Ceyuan and Xu, Yinghao and Dai, Bo and Zhou, Bolei},
  journal={arXiv:2006.15489},
  year={2020}
}

@inproceedings{chen2021rspnet,
  title={Rspnet: Relative speed perception for unsupervised video representation learning},
  author={Chen, Peihao and Huang, Deng and He, Dongliang and Long, Xiang and Zeng, Runhao and Wen, Shilei and Tan, Mingkui and Gan, Chuang},
  booktitle={AAAI},
  year={2021}
}

@article{stergiou2021learn,
  title={Learn to cycle: Time-consistent feature discovery for action recognition},
  author={Stergiou, Alexandros and Poppe, Ronald},
  journal={PRL},
  year={2021}
}

@inproceedings{wang2018non,
  title={Non-local neural networks},
  author={Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{carreira2017quo,
  title={Quo vadis, action recognition? a new model and the kinetics dataset},
  author={Carreira, Joao and Zisserman, Andrew},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{zhou2018mict,
  title={Mict: Mixed 3d/2d convolutional tube for human action recognition},
  author={Zhou, Yizhou and Sun, Xiaoyan and Zha, Zheng-Jun and Zeng, Wenjun},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{feichtenhofer2016convolutional,
  title={Convolutional two-stream network fusion for video action recognition},
  author={Feichtenhofer, Christoph and Pinz, Axel and Zisserman, Andrew},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{wang2016temporal,
  title={Temporal segment networks: Towards good practices for deep action recognition},
  author={Wang, Limin and Xiong, Yuanjun and Wang, Zhe and Qiao, Yu and Lin, Dahua and Tang, Xiaoou and Van Gool, Luc},
  booktitle={ECCV},
  year={2016}
}

@inproceedings{wang2017spatiotemporal,
  title={Spatiotemporal pyramid network for video action recognition},
  author={Wang, Yunbo and Long, Mingsheng and Wang, Jianmin and Yu, Philip S},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{yang2020temporal,
  title={Temporal pyramid network for action recognition},
  author={Yang, Ceyuan and Xu, Yinghao and Shi, Jianping and Dai, Bo and Zhou, Bolei},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{srivastava2015unsupervised,
  title={Unsupervised learning of video representations using lstms},
  author={Srivastava, Nitish and Mansimov, Elman and Salakhudinov, Ruslan},
  booktitle={ICML},
  year={2015}
}

@inproceedings{donahue2015long,
  title={Long-term recurrent convolutional networks for visual recognition and description},
  author={Donahue, Jeffrey and Hendricks, Lisa Anne and Guadarrama, Sergio and Rohrbach, Marcus and Venugopalan, Subhashini and Saenko, Kate and Darrell, Trevor},
  booktitle={CVPR},
  year={2015}
}

@inproceedings{kwon2021h2o,
  title={H2o: Two hands manipulating objects for first person interaction recognition},
  author={Kwon, Taein and Tekin, Bugra and St{\"u}hmer, Jan and Bogo, Federica and Pollefeys, Marc},
  booktitle={ICCV},
  year={2021}
}


@inproceedings{singh2016multi,
  title={A multi-stream bi-directional recurrent neural network for fine-grained action detection},
  author={Singh, Bharat and Marks, Tim K and Jones, Michael and Tuzel, Oncel and Shao, Ming},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{wang2017recurrent,
  title={Recurrent modeling of interaction context for collective activity recognition},
  author={Wang, Minsi and Ni, Bingbing and Yang, Xiaokang},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{girdhar2019video,
  title={Video action transformer network},
  author={Girdhar, Rohit and Carreira, Joao and Doersch, Carl and Zisserman, Andrew},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{kalogeiton2017action,
  title={Action tubelet detector for spatio-temporal action localization},
  author={Kalogeiton, Vicky and Weinzaepfel, Philippe and Ferrari, Vittorio and Schmid, Cordelia},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{faure2023holistic,
  title={Holistic interaction transformer network for action detection},
  author={Faure, Gueter Josmy and Chen, Min-Hung and Lai, Shang-Hong},
  booktitle={WACV},
  year={2023}
}

@inproceedings{nguyen2024hig,
  title={Hig: Hierarchical interlacement graph approach to scene graph generation in video understanding},
  author={Nguyen, Trong-Thuan and Nguyen, Pha and Luu, Khoa},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{chen2023efficient,
  title={Efficient video action detection with token dropout and context refinement},
  author={Chen, Lei and Tong, Zhan and Song, Yibing and Wu, Gangshan and Wang, Limin},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{li2021multisports,
  title={Multisports: A multi-person video dataset of spatio-temporally localized sports actions},
  author={Li, Yixuan and Chen, Lei and He, Runyu and Wang, Zhenzhi and Wu, Gangshan and Wang, Limin},
  booktitle={ICCV},
  year={2021}
}


@inproceedings{bacharidis2023repetition,
  title={{Repetition-aware Image Sequence Sampling for Recognizing Repetitive Human Actions}},
  author={Bacharidis, Konstantinos and Argyros, Antonis},
  booktitle={ICCVw},
  year={2023}
}

@inproceedings{zhang2021repetitive,
  title={{Repetitive Activity Counting by Sight and Sound}},
  author={Zhang, Yunhua and Shao, Ling and Snoek, Cees G. M.},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{destro2024cyclecl,
  title={{CycleCL: Self-supervised Learning for Periodic Videos}},
  author={Destro, Matteo and Gygli, Michael},
  booktitle={WACV},
  year={2024}
}

@inproceedings{li2024repetitive,
  title={{Repetitive Action Counting With Motion Feature Learning}},
  author={Li, Xinjie and Xu, Huijuan},
  booktitle={WACV},
  year={2024}
}


@inproceedings{ousman2008segmentation,
  title={{Segmentation of Periodically Moving Objects}},
  author={Azy, Ousman and Ahuja, Narendra},
  booktitle={ICPR},
  year={2008}
}
@article{briassouli2007extraction,
  title={{Extraction and Analysis of Multiple Periodic Motions in Video Sequences}},
  author={Briassouli, Alexia and Ahuja, Narendra},
  journal={IEEE TPAMI},
  year={2007},
}
@article{ross2000robust,
  title={{Robust Real-Time Periodic Motion Detection, Analysis, and Applications}},
  author={Cutler, Ross and Davis, Larry S.},
  journal={IEEE TPAMI},
  year={2000}
}
@inproceedings{pogalin2008visual,
  title={{Visual Quasi-Periodicity}},
  author={Pogalin, Erik and Smeulders, Arnold WM and Thean, Andrew HC},
  booktitle={CVPR},
  year={2008}
}
@article{branzan2008generic,
  title={{Generic Temporal Segmentation of Cyclic Human Motion}},
  author={Albu, A Branzan and Bergevin, Robert and Quirion, S{\'e}bastien},
  journal={PR   },
  year={2008},
}
@inproceedings{fourier6_periodic,
  title={{Periodic Motion Detection and Segmentation via Approximate Sequence Alignment}},
  author={Laptev, Ivan and Belongie, Serge J and P{\'e}rez, Patrick and Wills, Josh},
  booktitle={ICCV},
  year={2005}
}


@inproceedings{kapidis2019multitask,
  title={Multitask learning to improve egocentric action recognition},
  author={Kapidis, Georgios and Poppe, Ronald and Van Dam, Elsbeth and Noldus, Lucas and Veltkamp, Remco},
  booktitle={ICCVw},
  year={2019}
}

@article{kapidis2023multi,
  title={Multi-dataset, multitask learning of egocentric vision tasks},
  author={Kapidis, Georgios and Poppe, Ronald and Veltkamp, Remco C},
  journal={IEEE TPAMI},
  year={2023}
}

@inproceedings{plizzari2023can,
  title={What can a cook in Italy teach a mechanic in India? Action Recognition Generalisation Over Scenarios and Locations},
  author={Plizzari, Chiara and Perrett, Toby and Caputo, Barbara and Damen, Dima},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{perrett2024s,
  title={It's Just Another Day: Unique Video Captioning by Discriminative Prompting},
  author={Perrett, Toby and Han, Tengda and Damen, Dima and Zisserman, Andrew},
  booktitle={ACCV},
  year={2024}
}

@inproceedings{cui2022contrastive,
  title={Contrastive vision-language pre-training with limited resources},
  author={Cui, Quan and Zhou, Boyan and Guo, Yu and Yin, Weidong and Wu, Hao and Yoshie, Osamu and Chen, Yubo},
  booktitle={ECCV},
  year={2022}
}

@article{kahana2022improving,
  title={Improving zero-shot models with label distribution priors},
  author={Kahana, Jonathan and Cohen, Niv and Hoshen, Yedid},
  journal={arXiv:2212.00784},
  year={2022}
}

@article{peng2023sgva,
  title={Sgva-clip: Semantic-guided visual adapting of vision-language models for few-shot image classification},
  author={Peng, Fang and Yang, Xiaoshan and Xiao, Linhui and Wang, Yaowei and Xu, Changsheng},
  journal={IEEE TMM},
  year={2023}
}

@inproceedings{chowdhury2023apollo,
  title={APoLLo: Unified Adapter and Prompt Learning for Vision Language Models},
  author={Chowdhury, Sanjoy and Nag, Sayan and Manocha, Dinesh},
  booktitle={EMNLP},
  pages={10173--10187}
}

@inproceedings{nag2024safari,
  title={Safari: Adaptive sequence transformer for weakly supervised referring expression segmentation},
  author={Nag, Sayan and Goswami, Koustava and Karanam, Srikrishna},
  booktitle={ECCV},
  year={2024}
}

@inproceedings{zhuang2024falip,
  title={FALIP: Visual Prompt as Foveal Attention Boosts CLIP Zero-Shot Performance},
  author={Zhuang, Jiedong and Hu, Jiaqi and Mu, Lianrui and Hu, Rui and Liang, Xiaoyu and Ye, Jiangnan and Hu, Haoji},
  booktitle={ECCV},
  year={2024}
}

@article{gao2022pyramidclip,
  title={Pyramidclip: Hierarchical feature alignment for vision-language model pretraining},
  author={Gao, Yuting and Liu, Jinfeng and Xu, Zihan and Zhang, Jun and Li, Ke and Ji, Rongrong and Shen, Chunhua},
  journal={NeurIPS},
  year={2022}
}

@article{ranasinghe2023language,
  title={Language-based action concept spaces improve video self-supervised learning},
  author={Ranasinghe, Kanchana and Ryoo, Michael S},
  journal={NeurIPS},
  year={2023}
}

@inproceedings{zhang2024enhanced,
  title={Enhanced Motion-Text Alignment for Image-to-Video Transfer Learning},
  author={Zhang, Wei and Wan, Chaoqun and Liu, Tongliang and Tian, Xinmei and Shen, Xu and Ye, Jieping},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{kariyappa2023cocktail,
  title={Cocktail party attack: Breaking aggregation-based privacy in federated learning using independent component analysis},
  author={Kariyappa, Sanjay and Guo, Chuan and Maeng, Kiwan and Xiong, Wenjie and Suh, G Edward and Qureshi, Moinuddin K and Lee, Hsien-Hsin S},
  booktitle={ICML},
  year={2023}
}

@inproceedings{fang2023gifd,
  title={Gifd: A generative gradient inversion method with feature domain optimization},
  author={Fang, Hao and Chen, Bin and Wang, Xuan and Wang, Zhi and Xia, Shu-Tao},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{hatamizadeh2022gradvit,
  title={Gradvit: Gradient inversion of vision transformers},
  author={Hatamizadeh, Ali and Yin, Hongxu and Roth, Holger R and Li, Wenqi and Kautz, Jan and Xu, Daguang and Molchanov, Pavlo},
  booktitle={CVPR},
  year={2022}
}

@article{liang2024vl,
  title={Vl-trojan: Multimodal instruction backdoor attacks against autoregressive visual language models},
  author={Liang, Jiawei and Liang, Siyuan and Luo, Man and Liu, Aishan and Han, Dongchen and Chang, Ee-Chien and Cao, Xiaochun},
  journal={arXiv:2402.13851},
  year={2024}
}

@inproceedings{bai2024badclip,
  title={BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP},
  author={Bai, Jiawang and Gao, Kuofeng and Min, Shaobo and Xia, Shu-Tao and Li, Zhifeng and Liu, Wei},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{carlini2022poisoning,
  title={Poisoning and backdooring contrastive learning},
  author={Carlini, Nicholas and Terzis, Andreas},
  booktitle={ICLR},
  year={2022}
}

@inproceedings{wu2019long,
  title={Long-term feature banks for detailed video understanding},
  author={Wu, Chao-Yuan and Feichtenhofer, Christoph and Fan, Haoqi and He, Kaiming and Krahenbuhl, Philipp and Girshick, Ross},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{chao2018rethinking,
  title={Rethinking the faster r-cnn architecture for temporal action localization},
  author={Chao, Yu-Wei and Vijayanarasimhan, Sudheendra and Seybold, Bryan and Ross, David A and Deng, Jia and Sukthankar, Rahul},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{huang2019decoupling,
  title={Decoupling localization and classification in single shot temporal action detection},
  author={Huang, Yupan and Dai, Qi and Lu, Yutong},
  booktitle={ICME},
  year={2019},
}

@inproceedings{alwassel2021tsp,
  title={Tsp: Temporally-sensitive pretraining of video encoders for localization tasks},
  author={Alwassel, Humam and Giancola, Silvio and Ghanem, Bernard},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{zhang2022actionformer,
  title={Actionformer: Localizing moments of actions with transformers},
  author={Zhang, Chen-Lin and Wu, Jianxin and Li, Yin},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{shou2016temporal,
  title={Temporal action localization in untrimmed videos via multi-stage cnns},
  author={Shou, Zheng and Wang, Dongang and Chang, Shih-Fu},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{wang2013action,
  title={Action recognition with improved trajectories},
  author={Wang, Heng and Schmid, Cordelia},
  booktitle={ICCV},
  year={2013}
}

@inproceedings{oneata2013action,
  title={Action and event recognition with fisher vectors on a compact feature set},
  author={Oneata, Dan and Verbeek, Jakob and Schmid, Cordelia},
  booktitle={ICCV},
  year={2013}
}

@inproceedings{wang2017untrimmednets,
  title={Untrimmednets for weakly supervised action recognition and detection},
  author={Wang, Limin and Xiong, Yuanjun and Lin, Dahua and Van Gool, Luc},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{zeng2019graph,
  title={Graph convolutional networks for temporal action localization},
  author={Zeng, Runhao and Huang, Wenbing and Tan, Mingkui and Rong, Yu and Zhao, Peilin and Huang, Junzhou and Gan, Chuang},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{paul2018w,
  title={W-talc: Weakly-supervised temporal activity localization and classification},
  author={Paul, Sujoy and Roy, Sourya and Roy-Chowdhury, Amit K},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{luo2020weakly,
  title={Weakly-supervised action localization with expectation-maximization multi-instance learning},
  author={Luo, Zhekun and Guillory, Devin and Shi, Baifeng and Ke, Wei and Wan, Fang and Darrell, Trevor and Xu, Huijuan},
  booktitle={ECCV},
  year={2020}
}


@inproceedings{rizve2023pivotal,
  title={Pivotal: Prior-driven supervision for weakly-supervised temporal action localization},
  author={Rizve, Mamshad Nayeem and Mittal, Gaurav and Yu, Ye and Hall, Matthew and Sajeev, Sandra and Shah, Mubarak and Chen, Mei},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{shou2017cdc,
  title={Cdc: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos},
  author={Shou, Zheng and Chan, Jonathan and Zareian, Alireza and Miyazawa, Kazuyuki and Chang, Shih-Fu},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{xu2017r,
  title={R-c3d: Region convolutional 3d network for temporal activity detection},
  author={Xu, Huijuan and Das, Abir and Saenko, Kate},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{lin2018bsn,
  title={Bsn: Boundary sensitive network for temporal action proposal generation},
  author={Lin, Tianwei and Zhao, Xu and Su, Haisheng and Wang, Chongjing and Yang, Ming},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{lin2021learning,
  title={Learning salient boundary feature for anchor-free temporal action localization},
  author={Lin, Chuming and Xu, Chengming and Luo, Donghao and Wang, Yabiao and Tai, Ying and Wang, Chengjie and Li, Jilin and Huang, Feiyue and Fu, Yanwei},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{cheng2022tallformer,
  title={Tallformer: Temporal action localization with a long-memory transformer},
  author={Cheng, Feng and Bertasius, Gedas},
  booktitle={ECCV},
  year={2022},
}

@inproceedings{liu2020progressive,
  title={Progressive boundary refinement network for temporal action detection},
  author={Liu, Qinying and Wang, Zilei},
  booktitle={AAAI},
  year={2020}
}

@inproceedings{liu2021weakly,
  title={Weakly supervised temporal action localization through learning explicit subspaces for action and context},
  author={Liu, Ziyi and Wang, Le and Tang, Wei and Yuan, Junsong and Zheng, Nanning and Hua, Gang},
  booktitle={AAAI},
  year={2021}
}

@inproceedings{shi2023tridet,
  title={Tridet: Temporal action detection with relative boundary modeling},
  author={Shi, Dingfeng and Zhong, Yujie and Cao, Qiong and Ma, Lin and Li, Jia and Tao, Dacheng},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{shou2018autoloc,
  title={Autoloc: Weakly-supervised temporal action localization in untrimmed videos},
  author={Shou, Zheng and Gao, Hang and Zhang, Lei and Miyazawa, Kazuyuki and Chang, Shih-Fu},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{mettes2016spot,
  title={Spot on: Action localization from pointly-supervised proposals},
  author={Mettes, Pascal and Van Gemert, Jan C and Snoek, Cees G. M.},
  booktitle={ECCV},
  year={2016},
}

@inproceedings{liu2024end,
  title={End-to-end temporal action detection with 1b parameters across 1000 frames},
  author={Liu, Shuming and Zhang, Chen-Lin and Zhao, Chen and Ghanem, Bernard},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{zhao2023re2tal,
  title={Re2TAL: Rewiring pretrained video backbones for reversible temporal action localization},
  author={Zhao, Chen and Liu, Shuming and Mangalam, Karttikeya and Ghanem, Bernard},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{shao2023action,
  title={Action sensitivity learning for temporal action localization},
  author={Shao, Jiayi and Wang, Xiaohan and Quan, Ruijie and Zheng, Junjun and Yang, Jiang and Yang, Yi},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{zhu2024dual,
  title={Dual DETRs for Multi-Label Temporal Action Detection},
  author={Zhu, Yuhan and Zhang, Guozhen and Tan, Jing and Wu, Gangshan and Wang, Limin},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{zhang2021cola,
  title={Cola: Weakly-supervised temporal action localization with snippet contrastive learning},
  author={Zhang, Can and Cao, Meng and Yang, Dongming and Chen, Jie and Zou, Yuexian},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{chen2022dcan,
  title={Dcan: improving temporal action detection via dual context aggregation},
  author={Chen, Guo and Zheng, Yin-Dong and Wang, Limin and Lu, Tong},
  booktitle={AAAI},
  year={2022}
}

@inproceedings{zhao2021video,
  title={Video self-stitching graph network for temporal action localization},
  author={Zhao, Chen and Thabet, Ali K and Ghanem, Bernard},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{zhai2020two,
  title={Two-stream consensus network for weakly-supervised temporal action localization},
  author={Zhai, Yuanhao and Wang, Le and Tang, Wei and Zhang, Qilin and Yuan, Junsong and Hua, Gang},
  booktitle={ECCV},
  year={2020},
}

@inproceedings{he2022asm,
  title={Asm-loc: Action-aware segment modeling for weakly-supervised temporal action localization},
  author={He, Bo and Yang, Xitong and Kang, Le and Cheng, Zhiyu and Zhou, Xin and Shrivastava, Abhinav},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{nag2023difftad,
  title={Difftad: Temporal action detection with proposal denoising diffusion},
  author={Nag, Sauradip and Zhu, Xiatian and Deng, Jiankang and Song, Yi-Zhe and Xiang, Tao},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{qing2021temporal,
  title={Temporal context aggregation network for temporal action proposal refinement},
  author={Qing, Zhiwu and Su, Haisheng and Gan, Weihao and Wang, Dongliang and Wu, Wei and Wang, Xiang and Qiao, Yu and Yan, Junjie and Gao, Changxin and Sang, Nong},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{zhang2022unsupervised,
  title={Unsupervised pre-training for temporal action localization tasks},
  author={Zhang, Can and Yang, Tianyu and Weng, Junwu and Cao, Meng and Wang, Jue and Zou, Yuexian},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{hong2022spotting,
  title={Spotting temporally precise, fine-grained events in video},
  author={Hong, James and Zhang, Haotian and Gharbi, Micha{\"e}l and Fisher, Matthew and Fatahalian, Kayvon},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{ju2023distilling,
  title={Distilling vision-language pre-training to collaborate with weakly-supervised temporal action localization},
  author={Ju, Chen and Zheng, Kunhao and Liu, Jinxiang and Zhao, Peisen and Zhang, Ya and Chang, Jianlong and Tian, Qi and Wang, Yanfeng},
  booktitle={CVPR},
  year={2023}
}


@inproceedings{carion2020end,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={ECCV},
  year={2020}
}

@article{wang2023temporal,
  title={Temporal action localization in the deep learning era: A survey},
  author={Wang, Binglu and Zhao, Yongqiang and Yang, Le and Long, Teng and Li, Xuelong},
  journal={IEEE TPAMI},
  year={2023}
}


@inproceedings{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{arnab2021vivit,
  title={Vivit: A video vision transformer},
  author={Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu{\v{c}}i{\'c}, Mario and Schmid, Cordelia},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{bertasius2021space,
  title={Is space-time attention all you need for video understanding?},
  author={Bertasius, Gedas and Wang, Heng and Torresani, Lorenzo},
  booktitle={ICML},
  year={2021}
}

@article{patrick2021keeping,
  title={Keeping your eye on the ball: Trajectory attention in video transformers},
  author={Patrick, Mandela and Campbell, Dylan and Asano, Yuki and Misra, Ishan and Metze, Florian and Feichtenhofer, Christoph and Vedaldi, Andrea and Henriques, Joao F},
  journal={NeurIPS},
  year={2021}
}

@inproceedings{fan2021multiscale,
  title={Multiscale vision transformers},
  author={Fan, Haoqi and Xiong, Bo and Mangalam, Karttikeya and Li, Yanghao and Yan, Zhicheng and Malik, Jitendra and Feichtenhofer, Christoph},
  booktitle={ICCV},
  year={2021}
}

@article{ryoo2021tokenlearner,
  title={Tokenlearner: Adaptive space-time tokenization for videos},
  author={Ryoo, Michael and Piergiovanni, AJ and Arnab, Anurag and Dehghani, Mostafa and Angelova, Anelia},
  journal={NeurIPS},
  year={2021}
}

@article{bulat2021space,
  title={Space-time mixing attention for video transformer},
  author={Bulat, Adrian and Perez Rua, Juan Manuel and Sudhakaran, Swathikiran and Martinez, Brais and Tzimiropoulos, Georgios},
  journal={NeurIPS},
  year={2021}
}

@inproceedings{li2021groupformer,
  title={Groupformer: Group activity recognition with clustered spatial-temporal transformer},
  author={Li, Shuaicheng and Cao, Qianggang and Liu, Lingbo and Yang, Kunlin and Liu, Shinan and Hou, Jun and Yi, Shuai},
  booktitle={ICCV},
  year={2021}
}

@article{kim2021relational,
  title={Relational self-attention: What's missing in attention for video understanding},
  author={Kim, Manjin and Kwon, Heeseung and Wang, Chunyu and Kwak, Suha and Cho, Minsu},
  journal={NeurIPS},
  year={2021}
}

@inproceedings{liu2022video,
  title={Video swin transformer},
  author={Liu, Ze and Ning, Jia and Cao, Yue and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Hu, Han},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{li2022mvitv2,
  title={Mvitv2: Improved multiscale vision transformers for classification and detection},
  author={Li, Yanghao and Wu, Chao-Yuan and Fan, Haoqi and Mangalam, Karttikeya and Xiong, Bo and Malik, Jitendra and Feichtenhofer, Christoph},
  booktitle={CVPR},
  year={2022}
}

@article{zha2021shifted,
  title={Shifted chunk transformer for spatio-temporal representational learning},
  author={Zha, Xuefan and Zhu, Wentao and Xun, Lv and Yang, Sen and Liu, Ji},
  journal={NeurIPS},
  year={2021}
}

@inproceedings{wu2022memvit,
  title={Memvit: Memory-augmented multiscale vision transformer for efficient long-term video recognition},
  author={Wu, Chao-Yuan and Li, Yanghao and Mangalam, Karttikeya and Fan, Haoqi and Xiong, Bo and Malik, Jitendra and Feichtenhofer, Christoph},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{yan2022multiview,
  title={Multiview transformers for video recognition},
  author={Yan, Shen and Xiong, Xuehan and Arnab, Anurag and Lu, Zhichao and Zhang, Mi and Sun, Chen and Schmid, Cordelia},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{ryali2023hiera,
  title={Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles},
  author={Ryali, Chaitanya and Hu, Yuan-Ting and Bolya, Daniel and Wei, Chen and Fan, Haoqi and Huang, Po-Yao and Aggarwal, Vaibhav and Chowdhury, Arkabandhu and Poursaeed, Omid and Hoffman, Judy and others},
  booktitle={ICML},
  year={2023}
}

@inproceedings{li2022uniformer,
  title={UniFormer: Unified Transformer for Efficient Spatial-Temporal Representation Learning},
  author={Li, Kunchang and Wang, Yali and Peng, Gao and Song, Guanglu and Liu, Yu and Li, Hongsheng and Qiao, Yu},
  booktitle={ICLR},
  year={2022}
}

@inproceedings{sudhakaran2020gate,
  title={Gate-shift networks for video action recognition},
  author={Sudhakaran, Swathikiran and Escalera, Sergio and Lanz, Oswald},
  booktitle={CVPR},
  year={2020}
}

@article{bao2021beit,
  title={Beit: Bert pre-training of image transformers},
  author={Bao, Hangbo and Dong, Li and Piao, Songhao and Wei, Furu},
  journal={arXiv preprint arXiv:2106.08254},
  year={2021}
}

@inproceedings{wang2022bevt,
  title={Bevt: Bert pretraining of video transformers},
  author={Wang, Rui and Chen, Dongdong and Wu, Zuxuan and Chen, Yinpeng and Dai, Xiyang and Liu, Mengchen and Jiang, Yu-Gang and Zhou, Luowei and Yuan, Lu},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{ju2022prompting,
  title={Prompting visual-language models for efficient video understanding},
  author={Ju, Chen and Han, Tengda and Zheng, Kunhao and Zhang, Ya and Xie, Weidi},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{nag2022zero,
  title={Zero-shot temporal action detection via vision-language prompting},
  author={Nag, Sauradip and Zhu, Xiatian and Song, Yi-Zhe and Xiang, Tao},
  booktitle={ECCV},
  year={2022},
}

@inproceedings{phan2024zeetad,
  title={ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot End-to-End Temporal Action Detection},
  author={Phan, Thinh and Vo, Khoa and Le, Duy and Doretto, Gianfranco and Adjeroh, Donald and Le, Ngan},
  booktitle={WACV},
  year={2024}
}

@article{aklilu2024zero,
      title={Zero-shot Action Localization via the Confidence of Large Vision-Language Models}, 
      author={Josiah Aklilu and Xiaohan Wang and Serena Yeung-Levy},
      year={2024},
      journal={2410.14340}
}

@inproceedings{liberatori2024test,
  title={Test-Time Zero-Shot Temporal Action Localization},
  author={Liberatori, Benedetta and Conti, Alessandro and Rota, Paolo and Wang, Yiming and Ricci, Elisa},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{furnari2022towards,
  title={Towards streaming egocentric action anticipation},
  author={Furnari, Antonino and Farinella, Giovanni Maria},
  booktitle={ICPR},
  year={2022}
}

@inproceedings{girase2023latency,
  title={Latency matters: Real-time action forecasting transformer},
  author={Girase, Harshayu and Agarwal, Nakul and Choi, Chiho and Mangalam, Karttikeya},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{lee2024modeling,
  title={Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations},
  author={Lee, Sangmin and Lai, Bolin and Ryan, Fiona and Boote, Bikram and Rehg, James M},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{zadeh2019social,
  title={Social-iq: A question answering benchmark for artificial social intelligence},
  author={Zadeh, Amir and Chan, Michael and Liang, Paul Pu and Tong, Edmund and Morency, Louis-Philippe},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{cheng2023vindlu,
  title={Vindlu: A recipe for effective video-and-language pretraining},
  author={Cheng, Feng and Wang, Xizi and Lei, Jie and Crandall, David and Bansal, Mohit and Bertasius, Gedas},
  booktitle={CVPR},
  year={2023}
}

@article{tan2021vimpac,
  title={Vimpac: Video pre-training via masked token prediction and contrastive learning},
  author={Tan, Hao and Lei, Jie and Wolf, Thomas and Bansal, Mohit},
  journal={arXiv preprint arXiv:2106.11250},
  year={2021}
}

@inproceedings{teeti2023temporal,
  title={Temporal DINO: A Self-supervised Video Strategy to Enhance Action Prediction},
  author={Teeti, Izzeddin and Bhargav, Rongali Sai and Singh, Vivek and Bradley, Andrew and Banerjee, Biplab and Cuzzolin, Fabio},
  booktitle={ICCVW},
  year={2023},
}

@inproceedings{li2023unmasked,
  title={Unmasked teacher: Towards training-efficient video foundation models},
  author={Li, Kunchang and Wang, Yali and Li, Yizhuo and Wang, Yi and He, Yinan and Wang, Limin and Qiao, Yu},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{lian2023av,
  title={Av-data2vec: Self-supervised learning of audio-visual speech representations with contextualized target representations},
  author={Lian, Jiachen and Baevski, Alexei and Hsu, Wei-Ning and Auli, Michael},
  booktitle={ASRUw},
  year={2023},
}

@inproceedings{vondrick2018tracking,
  title={Tracking emerges by colorizing videos},
  author={Vondrick, Carl and Shrivastava, Abhinav and Fathi, Alireza and Guadarrama, Sergio and Murphy, Kevin},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{zhang2023temporal,
  title={Temporal consistent automatic video colorization via semantic correspondence},
  author={Zhang, Yu and Chen, Siqi and Wang, Mingdao and Zhang, Xianlin and Zhu, Chuang and Zhang, Yue and Li, Xueming},
  booktitle={CVPR},
  year={2023}
}

@article{liu2024temporally,
  title={Temporally consistent video colorization with deep feature propagation and self-regularization learning},
  author={Liu, Yihao and Zhao, Hengyuan and Chan, Kelvin CK and Wang, Xintao and Loy, Chen Change and Qiao, Yu and Dong, Chao},
  journal={CVM},
  year={2024},
}

@inproceedings{ali2023task,
  title={Task Agnostic Restoration of Natural Video Dynamics},
  author={Ali, Muhammad Kashif and Kim, Dongjin and Kim, Tae Hyun},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{dhiman2023corf,
  title={Corf: Colorizing radiance fields using knowledge distillation},
  author={Dhiman, Ankit and Srinath, R and Sarkar, Srinjay and Boregowda, Lokesh R and Babu, R Venkatesh},
  booktitle={ICCVw},
  year={2023}
}

@inproceedings{wu2020memory,
  title={Memory selection network for video propagation},
  author={Wu, Ruizheng and Lin, Huaijia and Qi, Xiaojuan and Jia, Jiaya},
  booktitle={ECCV},
  year={2020}
}

@article{jabri2020space,
  title={Space-time correspondence as a contrastive random walk},
  author={Jabri, Allan and Owens, Andrew and Efros, Alexei},
  journal={NeurIPS},
  year={2020}
}

@inproceedings{benaim2020speednet,
  title={Speednet: Learning the speediness in videos},
  author={Benaim, Sagie and Ephrat, Ariel and Lang, Oran and Mosseri, Inbar and Freeman, William T and Rubinstein, Michael and Irani, Michal and Dekel, Tali},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{li2023svitt,
  title={Svitt: Temporal learning of sparse video-text transformers},
  author={Li, Yi and Min, Kyle and Tripathi, Subarna and Vasconcelos, Nuno},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{dong2023peco,
  title={Peco: Perceptual codebook for bert pre-training of vision transformers},
  author={Dong, Xiaoyi and Bao, Jianmin and Zhang, Ting and Chen, Dongdong and Zhang, Weiming and Yuan, Lu and Chen, Dong and Wen, Fang and Yu, Nenghai and Guo, Baining},
  booktitle={AAAI},
  year={2023}
}

@inproceedings{baevski2022data2vec,
  title={Data2vec: A general framework for self-supervised learning in speech, vision and language},
  author={Baevski, Alexei and Hsu, Wei-Ning and Xu, Qiantong and Babu, Arun and Gu, Jiatao and Auli, Michael},
  booktitle={ICML},
  year={2022}
}

@inproceedings{chen2022sdae,
  title={Sdae: Self-distillated masked autoencoder},
  author={Chen, Yabo and Liu, Yuchen and Jiang, Dongsheng and Zhang, Xiaopeng and Dai, Wenrui and Xiong, Hongkai and Tian, Qi},
  booktitle={ECCV},
  year={2022},
}

@article{tong2022videomae,
  title={Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training},
  author={Tong, Zhan and Song, Yibing and Wang, Jue and Wang, Limin},
  journal={NeurIPS},
  year={2022}
}

@article{peng2022beit,
  title={Beit v2: Masked image modeling with vector-quantized visual tokenizers},
  author={Peng, Zhiliang and Dong, Li and Bao, Hangbo and Ye, Qixiang and Wei, Furu},
  journal={arXiv:2208.06366},
  year={2022}
}

@inproceedings{wu2023dropmae,
  title={Dropmae: Masked autoencoders with spatial-attention dropout for tracking tasks},
  author={Wu, Qiangqiang and Yang, Tianyu and Liu, Ziquan and Wu, Baoyuan and Shan, Ying and Chan, Antoni B},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{girdhar2023omnimae,
  title={Omnimae: Single model masked pretraining on images and videos},
  author={Girdhar, Rohit and El-Nouby, Alaaeldin and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{liu2021hit,
  title={Hit: Hierarchical transformer with momentum contrast for video-text retrieval},
  author={Liu, Song and Fan, Haoqi and Qian, Shengsheng and Chen, Yiru and Ding, Wenkui and Wang, Zhongyuan},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{qian2021spatiotemporal,
  title={Spatiotemporal contrastive video representation learning},
  author={Qian, Rui and Meng, Tianjian and Gong, Boqing and Yang, Ming-Hsuan and Wang, Huisheng and Belongie, Serge and Cui, Yin},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{pan2021videomoco,
  title={Videomoco: Contrastive video representation learning with temporally adversarial examples},
  author={Pan, Tian and Song, Yibing and Yang, Tianyu and Jiang, Wenhao and Liu, Wei},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{feichtenhofer2021large,
  title={A large-scale study on unsupervised spatiotemporal representation learning},
  author={Feichtenhofer, Christoph and Fan, Haoqi and Xiong, Bo and Girshick, Ross and He, Kaiming},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{park2022probabilistic,
  title={Probabilistic representations for video contrastive learning},
  author={Park, Jungin and Lee, Jiyoung and Kim, Ig-Jae and Sohn, Kwanghoon},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{yao2021seco,
  title={Seco: Exploring sequence supervision for unsupervised representation learning},
  author={Yao, Ting and Zhang, Yiheng and Qiu, Zhaofan and Pan, Yingwei and Mei, Tao},
  booktitle={AAAI},
  year={2021}
}

@article{dave2022tclr,
  title={Tclr: Temporal contrastive learning for video representation},
  author={Dave, Ishan and Gupta, Rohit and Rizve, Mamshad Nayeem and Shah, Mubarak},
  journal={CVIU},
  year={2022},
}

@inproceedings{diba2021vi2clr,
  title={Vi2clr: Video and image for visual contrastive learning of representation},
  author={Diba, Ali and Sharma, Vivek and Safdari, Reza and Lotfi, Dariush and Sarfraz, Saquib and Stiefelhagen, Rainer and Van Gool, Luc},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{recasens2021broaden,
  title={Broaden your views for self-supervised video learning},
  author={Recasens, Adria and Luc, Pauline and Alayrac, Jean-Baptiste and Wang, Luyu and Strub, Florian and Tallec, Corentin and Malinowski, Mateusz and P{\u{a}}tr{\u{a}}ucean, Viorica and Altch{\'e}, Florent and Valko, Michal and others},
  booktitle={ICCV},
  year={2021}
}

@article{han2020self,
  title={Self-supervised co-training for video representation learning},
  author={Han, Tengda and Xie, Weidi and Zisserman, Andrew},
  journal={NeurIPS},
  year={2020}
}

@inproceedings{han2020memory,
  title={Memory-augmented dense predictive coding for video representation learning},
  author={Han, Tengda and Xie, Weidi and Zisserman, Andrew},
  booktitle={ECCV},
  year={2020},
}

@inproceedings{yang2021taco,
  title={Taco: Token-aware cascade contrastive learning for video-text alignment},
  author={Yang, Jianwei and Bisk, Yonatan and Gao, Jianfeng},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{li2021motion,
  title={Motion-focused contrastive learning of video representations},
  author={Li, Rui and Zhang, Yiheng and Qiu, Zhaofan and Yao, Ting and Liu, Dong and Mei, Tao},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{wang2022video,
  title={Video anomaly detection by solving decoupled spatio-temporal jigsaw puzzles},
  author={Wang, Guodong and Wang, Yunhong and Qin, Jie and Zhang, Dongming and Bao, Xiuguo and Huang, Di},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{liu2024solving,
  title={Solving Masked Jigsaw Puzzles with Diffusion Vision Transformers},
  author={Liu, Jinyang and Teshome, Wondmgezahu and Ghimire, Sandesh and Sznaier, Mario and Camps, Octavia},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{kim2019self,
  title={Self-supervised video representation learning with space-time cubic puzzles},
  author={Kim, Dahun and Cho, Donghyeon and Kweon, In So},
  booktitle={AAAI},
  year={2019}
}

@inproceedings{lee2017unsupervised,
  title={Unsupervised representation learning by sorting sequences},
  author={Lee, Hsin-Ying and Huang, Jia-Bin and Singh, Maneesh and Yang, Ming-Hsuan},
  booktitle={ICCV},
  year={2017}
}


@inproceedings{jenni2021time,
  title={Time-equivariant contrastive video representation learning},
  author={Jenni, Simon and Jin, Hailin},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{bagad2023test,
  title={Test of time: Instilling video-language models with a sense of time},
  author={Bagad, Piyush and Tapaswi, Makarand and Snoek, Cees G. M.},
  booktitle={CVPR},
  year={2023}
}

@article{parthasarathy2023self,
  title={Self-supervised video pretraining yields robust and more human-aligned visual representations},
  author={Parthasarathy, Nikhil and Eslami, SM and Carreira, Joao and Henaff, Olivier},
  journal={NeurIPS},
  year={2023}
}

@inproceedings{kuang2021video,
  title={Video contrastive learning with global context},
  author={Kuang, Haofei and Zhu, Yi and Zhang, Zhi and Li, Xinyu and Tighe, Joseph and Schwertfeger, S{\"o}ren and Stachniss, Cyrill and Li, Mu},
  booktitle={ICCVw},
  year={2021}
}

@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={CVPR},
  year={2022}
}

@article{feichtenhofer2022masked,
  title={Masked autoencoders as spatiotemporal learners},
  author={Feichtenhofer, Christoph and Li, Yanghao and He, Kaiming and others},
  journal={NeurIPS},
  year={2022}
}

@inproceedings{bandara2023adamae,
  title={AdaMAE: Adaptive Masking for Efficient Spatiotemporal Learning with Masked Autoencoders},
  author={Bandara, Wele Gedara Chaminda and Patel, Naman and Gholami, Ali and Nikkhah, Mehdi and Agrawal, Motilal and Patel, Vishal M},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{huang2023mgmae,
  title={Mgmae: Motion guided masking for video masked autoencoding},
  author={Huang, Bingkun and Zhao, Zhiyu and Zhang, Guozhen and Qiao, Yu and Wang, Limin},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{kim2024token,
  title={Token Fusion: Bridging the Gap between Token Pruning and Token Merging},
  author={Kim, Minchul and Gao, Shangqian and Hsu, Yen-Chang and Shen, Yilin and Jin, Hongxia},
  booktitle={WACV},
  year={2024}
}

@inproceedings{wang2023videomae,
  title={Videomae v2: Scaling video masked autoencoders with dual masking},
  author={Wang, Limin and Huang, Bingkun and Zhao, Zhiyu and Tong, Zhan and He, Yinan and Wang, Yi and Wang, Yali and Qiao, Yu},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{girshick2015fast,
  title={Fast r-cnn},
  author={Girshick, Ross},
  booktitle={ICCV},
  year={2015}
}

@inproceedings{girshick2014rich,
  title={Rich feature hierarchies for accurate object detection and semantic segmentation},
  author={Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  booktitle={CVPR},
  year={2014}
}

@inproceedings{soomro2015action,
  title={Action localization in videos through context walk},
  author={Soomro, Khurram and Idrees, Haroon and Shah, Mubarak},
  booktitle={ICCV},
  year={2015}
}

@inproceedings{arnab2021unified,
  title={Unified graph structured models for video understanding},
  author={Arnab, Anurag and Sun, Chen and Schmid, Cordelia},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{gritsenko2024end,
  title={End-to-end spatio-temporal action localisation with video transformers},
  author={Gritsenko, Alexey A and Xiong, Xuehan and Djolonga, Josip and Dehghani, Mostafa and Sun, Chen and Lucic, Mario and Schmid, Cordelia and Arnab, Anurag},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{xing2023svformer,
  title={Svformer: Semi-supervised video transformer for action recognition},
  author={Xing, Zhen and Dai, Qi and Hu, Han and Chen, Jingjing and Wu, Zuxuan and Jiang, Yu-Gang},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{wei2022masked,
  title={Masked feature prediction for self-supervised visual pre-training},
  author={Wei, Chen and Fan, Haoqi and Xie, Saining and Wu, Chao-Yuan and Yuille, Alan and Feichtenhofer, Christoph},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{piergiovanni2023rethinking,
  title={Rethinking video vits: Sparse video tubes for joint image and video learning},
  author={Piergiovanni, AJ and Kuo, Weicheng and Angelova, Anelia},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{goyal2017making,
  title={Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{antol2015vqa,
  title={Vqa: Visual question answering},
  author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
  booktitle={ICCV},
  year={2015}
}

@article{tu2014joint,
  title={Joint video and text parsing for understanding events and answering queries},
  author={Tu, Kewei and Meng, Meng and Lee, Mun Wai and Choe, Tae Eun and Zhu, Song-Chun},
  journal={IEEE MM},
  year={2014},
}

@inproceedings{tapaswi2016movieqa,
  title={Movieqa: Understanding stories in movies through question-answering},
  author={Tapaswi, Makarand and Zhu, Yukun and Stiefelhagen, Rainer and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{lin2022frozen,
  title={Frozen clip models are efficient video learners},
  author={Lin, Ziyi and Geng, Shijie and Zhang, Renrui and Gao, Peng and De Melo, Gerard and Wang, Xiaogang and Dai, Jifeng and Qiao, Yu and Li, Hongsheng},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{girdhar2023imagebind,
  title={Imagebind: One embedding space to bind them all},
  author={Girdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{wang2023masked,
  title={Masked video distillation: Rethinking masked feature modeling for self-supervised video representation learning},
  author={Wang, Rui and Chen, Dongdong and Wu, Zuxuan and Chen, Yinpeng and Dai, Xiyang and Liu, Mengchen and Yuan, Lu and Jiang, Yu-Gang},
  booktitle={CVPR},
  year={2023}
}

@article{mizrahi20234m,
  title={4m: Massively multimodal masked modeling},
  author={Mizrahi, David and Bachmann, Roman and Kar, Oguzhan and Yeo, Teresa and Gao, Mingfei and Dehghan, Afshin and Zamir, Amir},
  journal={NeurIPS},
  year={2023}
}

@inproceedings{zhao2024asymmetric,
  title={Asymmetric masked distillation for pre-training small foundation models},
  author={Zhao, Zhiyu and Huang, Bingkun and Xing, Sen and Wu, Gangshan and Qiao, Yu and Wang, Limin},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{woo2023towards,
  title={Towards good practices for missing modality robust action recognition},
  author={Woo, Sangmin and Lee, Sumin and Park, Yeonju and Nugroho, Muhammad Adi and Kim, Changick},
  booktitle={AAAI},
  year={2023}
}

@inproceedings{lin2023smaug,
  title={Smaug: Sparse masked autoencoder for efficient video-language pre-training},
  author={Lin, Yuanze and Wei, Chen and Wang, Huiyu and Yuille, Alan and Xie, Cihang},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{pei2024videomac,
  title={VideoMAC: Video Masked Autoencoders Meet ConvNets},
  author={Pei, Gensheng and Chen, Tao and Jiang, Xiruo and Liu, Huafeng and Sun, Zeren and Yao, Yazhou},
  booktitle={CVPR},
  year={2024}
}


@inproceedings{thurau2008pose,
  title={Pose primitive based human action recognition in videos or still images},
  author={Thurau, Christian and Hlav{\'a}c, V{\'a}clav},
  booktitle={CVPR},
  year={2008}
}


@inproceedings{curto2021dyadformer,
  title={Dyadformer: A multi-modal transformer for long-range modeling of dyadic interactions},
  author={Curto, David and Clap{\'e}s, Albert and Selva, Javier and Smeureanu, Sorina and Junior, Julio and Jacques, CS and Gallardo-Pujol, David and Guilera, Georgina and Leiva, David and Moeslund, Thomas B and others},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{mikolajczyk2008action,
  title={Action recognition with motion-appearance vocabulary forest},
  author={Mikolajczyk, Krystian and Uemura, Hirofumi},
  booktitle={CVPR},
  year={2008}
}


@article{wang2024internvideo2,
  title={Internvideo2: Scaling video foundation models for multimodal video understanding},
  author={Wang, Yi and Li, Kunchang and Li, Xinhao and Yu, Jiashuo and He, Yinan and Chen, Guo and Pei, Baoqi and Zheng, Rongkun and Xu, Jilan and Wang, Zun and others},
  journal={arXiv:2403.15377},
  year={2024}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={NeurIPS},
  year={2022}
}


@article{maaz2023video,
  title={Video-chatgpt: Towards detailed video understanding via large vision and language models},
  author={Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Khan, Fahad Shahbaz},
  journal={arXiv:2306.05424},
  year={2023}
}


@inproceedings{piergiovanni2024mirasol3b,
  title={Mirasol3b: A multimodal autoregressive model for time-aligned and contextual modalities},
  author={Piergiovanni, AJ and Noble, Isaac and Kim, Dahun and Ryoo, Michael S and Gomes, Victor and Angelova, Anelia},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{xu2023mplug,
  title={mplug-2: A modularized multi-modal foundation model across text, image and video},
  author={Xu, Haiyang and Ye, Qinghao and Yan, Ming and Shi, Yaya and Ye, Jiabo and Xu, Yuanhong and Li, Chenliang and Bi, Bin and Qian, Qi and Wang, Wei and others},
  booktitle={ICML},
  year={2023}
}

@article{zellers2021merlot,
  title={Merlot: Multimodal neural script knowledge models},
  author={Zellers, Rowan and Lu, Ximing and Hessel, Jack and Yu, Youngjae and Park, Jae Sung and Cao, Jize and Farhadi, Ali and Choi, Yejin},
  journal={NeurIPS},
  year={2021}
}

@article{zhao2024videoprism,
  title={VideoPrism: A Foundational Visual Encoder for Video Understanding},
  author={Zhao, Long and Gundavarapu, Nitesh B and Yuan, Liangzhe and Zhou, Hao and Yan, Shen and Sun, Jennifer J and Friedman, Luke and Qian, Rui and Weyand, Tobias and Zhao, Yue and others},
  journal={ICML},
  year={2024}
}

@inproceedings{zellers2022merlot,
  title={Merlot reserve: Neural script knowledge through vision and language and sound},
  author={Zellers, Rowan and Lu, Jiasen and Lu, Ximing and Yu, Youngjae and Zhao, Yanpeng and Salehi, Mohammadreza and Kusupati, Aditya and Hessel, Jack and Farhadi, Ali and Choi, Yejin},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{cao2022locvtp,
  title={Locvtp: Video-text pre-training for temporal localization},
  author={Cao, Meng and Yang, Tianyu and Weng, Junwu and Zhang, Can and Wang, Jue and Zou, Yuexian},
  booktitle={ECCV},
  year={2022},
}

@article{dwibedi2024ovr,
  title={OVR: A Dataset for Open Vocabulary Temporal Repetition Counting in Videos},
  author={Dwibedi, Debidatta and Aytar, Yusuf and Tompson, Jonathan and Zisserman, Andrew},
  journal={arXiv:2407.17085},
  year={2024}
}

@article{lei2021detecting,
  title={Detecting moments and highlights in videos via natural language queries},
  author={Lei, Jie and Berg, Tamara L and Bansal, Mohit},
  journal={NeurIPS},
  year={2021}
}

@article{escorcia2019temporal,
  title={Temporal localization of moments in video collections with natural language},
  author={Escorcia, Victor and Soldan, Mattia and Sivic, Josef and Ghanem, Bernard and Russell, Bryan},
  year={2019},
  journal={arXiv:1907.12763}
}

@inproceedings{buch2017sst,
  title={Sst: Single-stream temporal action proposals},
  author={Buch, Shyamal and Escorcia, Victor and Shen, Chuanqi and Ghanem, Bernard and Carlos Niebles, Juan},
  booktitle={CVPR},
  year={2017}
}

@article{lin2022egocentric,
  title={Egocentric video-language pretraining},
  author={Lin, Kevin Qinghong and Wang, Jinpeng and Soldan, Mattia and Wray, Michael and Yan, Rui and Xu, Eric Z and Gao, Difei and Tu, Rong-Cheng and Zhao, Wenzhe and Kong, Weijie and others},
  journal={NeurIPS},
  year={2022}
}

@article{wang2022contrastive,
  title={Contrastive video-language learning with fine-grained frame sampling},
  author={Wang, Zixu and Zhong, Yujie and Miao, Yishu and Ma, Lin and Specia, Lucia},
  journal={arXiv preprint arXiv:2210.05039},
  year={2022}
}


@article{yu2022coca,
  title={Coca: Contrastive captioners are image-text foundation models},
  author={Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
  journal={arXiv:2205.01917},
  year={2022}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={NeurIPS},
  year={2020}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv:2302.13971},
  year={2023}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={ICML},
  year={2023},
}

@article{liu2024visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={NeurIPS},
  year={2024}
}


@inproceedings{grauman2022ego4d,
  title={Ego4d: Around the world in 3,000 hours of egocentric video},
  author={Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and others},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{fathi2012social,
  title={Social interactions: A first-person perspective},
  author={Fathi, Alircza and Hodgins, Jessica K and Rehg, James M},
  booktitle={CVPR},
  year={2012}
}


@inproceedings{xu2016msr,
  title={Msr-vtt: A large video description dataset for bridging video and language},
  author={Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{xiao2021next,
  title={Next-qa: Next phase of question-answering to explaining temporal actions},
  author={Xiao, Junbin and Shang, Xindi and Yao, Angela and Chua, Tat-Seng},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{miech2019howto100m,
  title={Howto100m: Learning a text-video embedding by watching hundred million narrated video clips},
  author={Miech, Antoine and Zhukov, Dimitri and Alayrac, Jean-Baptiste and Tapaswi, Makarand and Laptev, Ivan and Sivic, Josef},
  booktitle={CVPR},
  year={2019}
}

@article{lei2018tvqa,
  title={Tvqa: Localized, compositional video question answering},
  author={Lei, Jie and Yu, Licheng and Bansal, Mohit and Berg, Tamara L},
  journal={arXiv:1809.01696},
  year={2018}
}

@inproceedings{li2024mvbench,
  title={Mvbench: A comprehensive multi-modal video understanding benchmark},
  author={Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and others},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{cherian20222,
  title={(2.5+ 1) d spatio-temporal scene graphs for video question answering},
  author={Cherian, Anoop and Hori, Chiori and Marks, Tim K and Le Roux, Jonathan},
  booktitle={AAAI},
  year={2022}
}

@article{li2024llava,
  title={Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models},
  author={Li, Feng and Zhang, Renrui and Zhang, Hao and Zhang, Yuanhan and Li, Bo and Li, Wei and Ma, Zejun and Li, Chunyuan},
  journal={arXiv:2407.07895},
  year={2024}
}

@article{fu2024video,
  title={Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis},
  author={Fu, Chaoyou and Dai, Yuhan and Luo, Yongdong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others},
  journal={arXiv:2405.21075},
  year={2024}
}

@article{ying2024mmt,
  title={Mmt-bench: A comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi},
  author={Ying, Kaining and Meng, Fanqing and Wang, Jin and Li, Zhiqian and Lin, Han and Yang, Yue and Zhang, Hao and Zhang, Wenbo and Lin, Yuqi and Liu, Shuo and others},
  journal={arXiv:2404.16006},
  year={2024}
}

@article{rawal2024cinepile,
  title={Cinepile: A long video question answering dataset and benchmark},
  author={Rawal, Ruchit and Saifullah, Khalid and Farr{\'e}, Miquel and Basri, Ronen and Jacobs, David and Somepalli, Gowthami and Goldstein, Tom},
  journal={arXiv:2405.08813},
  year={2024}
}

@inproceedings{fei2024video,
  title={Video-of-thought: Step-by-step video reasoning from perception to cognition},
  author={Fei, Hao and Wu, Shengqiong and Ji, Wei and Zhang, Hanwang and Zhang, Meishan and Lee, Mong-Li and Hsu, Wynne},
  booktitle={ICML},
  year={2024}
}

@inproceedings{geng2021dynamic,
  title={Dynamic graph representation learning for video dialog via multi-modal shuffled transformers},
  author={Geng, Shijie and Gao, Peng and Chatterjee, Moitreya and Hori, Chiori and Le Roux, Jonathan and Zhang, Yongfeng and Li, Hongsheng and Cherian, Anoop},
  booktitle={AAAI},
  year={2021}
}

@inproceedings{chen2011collecting,
  title={Collecting highly parallel data for paraphrase evaluation},
  author={Chen, David and Dolan, William B},
  booktitle={ACL},
  year={2011}
}

 @inproceedings{zhou2018towards,
    author={Zhou, Luowei and Xu, Chenliang and Corso, Jason J},
    title = {Towards Automatic Learning of Procedures From Web Instructional Videos},
    booktitle = {AAAI},
    year = {2018},
  }
      
@inproceedings{wang2019vatex,
  title={Vatex: A large-scale, high-quality multilingual dataset for video-and-language research},
  author={Wang, Xin and Wu, Jiawei and Chen, Junkun and Li, Lei and Wang, Yuan-Fang and Wang, William Yang},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{gemmeke2017audio,
  title={Audio set: An ontology and human-labeled dataset for audio events},
  author={Gemmeke, Jort F and Ellis, Daniel PW and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R Channing and Plakal, Manoj and Ritter, Marvin},
  booktitle={ICASSP},
  year={2017}
}

@inproceedings{chen2020vggsound,
  title={Vggsound: A large-scale audio-visual dataset},
  author={Chen, Honglie and Xie, Weidi and Vedaldi, Andrea and Zisserman, Andrew},
  booktitle={ICASSP},
  year={2020}
}

@inproceedings{owens2016visually,
  title={Visually indicated sounds},
  author={Owens, Andrew and Isola, Phillip and McDermott, Josh and Torralba, Antonio and Adelson, Edward H and Freeman, William T},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{huh2023epic,
  title={Epic-sounds: A large-scale dataset of actions that sound},
  author={Huh, Jaesung and Chalk, Jacob and Kazakos, Evangelos and Damen, Dima and Zisserman, Andrew},
  booktitle={ICASSP},
  year={2023}
}

@inproceedings{zhou2022audio,
  title={Audio--visual segmentation},
  author={Zhou, Jinxing and Wang, Jianyuan and Zhang, Jiayi and Sun, Weixuan and Zhang, Jing and Birchfield, Stan and Guo, Dan and Kong, Lingpeng and Wang, Meng and Zhong, Yiran},
  booktitle={ECCV},
  year={2022},
}

@inproceedings{tian2020unified,
  title={Unified multisensory perception: Weakly-supervised audio-visual video parsing},
  author={Tian, Yapeng and Li, Dingzeyu and Xu, Chenliang},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{van2022exploring,
  title={Exploring fine-grained audiovisual categorization with the ssw60 dataset},
  author={Van Horn, Grant and Qian, Rui and Wilber, Kimberly and Adam, Hartwig and Mac Aodha, Oisin and Belongie, Serge},
  booktitle={ECCV},
  year={2022}
}

@article{sigal2010humaneva,
  title={Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion},
  author={Sigal, Leonid and Balan, Alexandru O and Black, Michael J},
  journal={IJCV},
  year={2010}
}

@article{joo2017panoptic,
  title={Panoptic Studio: A Massively Multiview System for Social Interaction Capture},
  author={Joo, Hanbyul and Simon, Tomas and Li, Xulong and Liu, Hao and Tan, Lei and Gui, Lin and Banerjee, Sean and Godisart, Timothy Scott and Nabbe, Bart and Matthews, Iain and Kanade, Takeo and Nobuhara, Shohei and Sheikh, Yaser},
  journal={IEEE TPAMI},
  year={2017}
}


@article{ionescu2013human3,
  title={Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments},
  author={Ionescu, Catalin and Papava, Dragos and Olaru, Vlad and Sminchisescu, Cristian},
  journal={IEEE TPAMI},
  year={2013}
}

@inproceedings{tsuchida2019aist,
  title={AIST Dance Video Database: Multi-Genre, Multi-Dancer, and Multi-Camera Database for Dance Information Processing.},
  author={Tsuchida, Shuhei and Fukayama, Satoru and Hamasaki, Masahiro and Goto, Masataka},
  booktitle={ISMIR},
  year={2019}
}

@inproceedings{peng2021neural,
  title={Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans},
  author={Peng, Sida and Zhang, Yuanqing and Xu, Yinghao and Wang, Qianqian and Shuai, Qing and Bao, Hujun and Zhou, Xiaowei},
  booktitle={CVPR},
  year={2021}
}

@article{broxton2020immersive,
  title={Immersive light field video with a layered mesh representation},
  author={Broxton, Michael and Flynn, John and Overbeck, Ryan and Erickson, Daniel and Hedman, Peter and Duvall, Matthew and Dourgarian, Jason and Busch, Jay and Whalen, Matt and Debevec, Paul},
  journal={ACM TOG},
  year={2020}
}

@article{lai2024human,
      title={Human Action Anticipation: A Survey}, 
      author={Bolin Lai and Sam Toyer and Tushar Nagarajan and Rohit Girdhar and Shengxin Zha and James M. Rehg and Kris Kitani and Kristen Grauman and Ruta Desai and Miao Liu},
      year={2024},
      journal={axiv},
}

@inproceedings{xie2024autoad,
  title={Autoad-zero: A training-free framework for zero-shot audio description},
  author={Xie, Junyu and Han, Tengda and Bain, Max and Nagrani, Arsha and Varol, G{\"u}l and Xie, Weidi and Zisserman, Andrew},
  booktitle={ACCV},
  year={2024}
}

@inproceedings{yoon2020novel,
  title={Novel view synthesis of dynamic scenes with globally coherent depths from a monocular camera},
  author={Yoon, Jae Shin and Kim, Kihwan and Gallo, Orazio and Park, Hyun Soo and Kautz, Jan},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{lin2021deep,
  title={Deep 3d mask volume for view synthesis of dynamic scenes},
  author={Lin, Kai-En and Xiao, Lei and Liu, Feng and Yang, Guowei and Ramamoorthi, Ravi},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{li2022neural,
  title={Neural 3d video synthesis from multi-view video},
  author={Li, Tianye and Slavcheva, Mira and Zollhoefer, Michael and Green, Simon and Lassner, Christoph and Kim, Changil and Schmidt, Tanner and Lovegrove, Steven and Goesele, Michael and Newcombe, Richard and others},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{alayrac2017joint,
  title={Joint discovery of object states and manipulation actions},
  author={Alayrac, Jean-Baptiste and Laptev, Ivan and Sivic, Josef and Lacoste-Julien, Simon},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{liu2017jointly,
  title={Jointly recognizing object fluents and tasks in egocentric videos},
  author={Liu, Yang and Wei, Ping and Zhu, Song-Chun},
  booktitle={ICCV},
  year={2017}
}

@article{tschernezki2024epic,
  title={Epic fields: Marrying 3d geometry and video understanding},
  author={Tschernezki, Vadim and Darkhalil, Ahmad and Zhu, Zhifan and Fouhey, David and Laina, Iro and Larlus, Diane and Damen, Dima and Vedaldi, Andrea},
  journal={NeurIPS},
  year={2024}
}


@inproceedings{yeung2016end,
  title={End-to-end learning of action detection from frame glimpses in videos},
  author={Yeung, Serena and Russakovsky, Olga and Mori, Greg and Fei-Fei, Li},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{stergiou2023play,
  title={Play it back: Iterative attention for audio recognition},
  author={Stergiou, Alexandros and Damen, Dima},
  booktitle={ICASSP},
  year={2023},
}

@inproceedings{kazakos2021slow,
  title={Slow-fast auditory streams for audio recognition},
  author={Kazakos, Evangelos and Nagrani, Arsha and Zisserman, Andrew and Damen, Dima},
  booktitle={ICASSP},
  year={2021},
}

@article{gong2021psla,
  title={Psla: Improving audio tagging with pretraining, sampling, labeling, and aggregation},
  author={Gong, Yuan and Chung, Yu-An and Glass, James},
  journal={IEEE/ACM TASLP},
  year={2021},
}

@inproceedings{baade2022mae,
  title={Mae-ast: Masked autoencoding audio spectrogram transformer},
  author={Baade, Alan and Peng, Puyuan and Harwath, David},
  booktitle={Interspeech},
  year={2022}
}

@inproceedings{koutini2021efficient,
  title={Efficient training of audio transformers with patchout},
  author={Koutini, Khaled and Schl{\"u}ter, Jan and Eghbal-Zadeh, Hamid and Widmer, Gerhard},
  booktitle={Interspeech},
  year={2022}
}

@article{kong2020panns,
  title={Panns: Large-scale pretrained audio neural networks for audio pattern recognition},
  author={Kong, Qiuqiang and Cao, Yin and Iqbal, Turab and Wang, Yuxuan and Wang, Wenwu and Plumbley, Mark D},
  journal={IEEE/ACM TASLP},
  year={2020},
}

@article{wu2020dynamic,
  title={A dynamic frame selection framework for fast video recognition},
  author={Wu, Zuxuan and Li, Hengduo and Xiong, Caiming and Jiang, Yu-Gang and Davis, Larry S},
  journal={IEEE TPAMI},
  year={2020},
}

@inproceedings{kim2021efficient,
  title={Efficient action recognition via dynamic knowledge propagation},
  author={Kim, Hanul and Jain, Mihir and Lee, Jun-Tae and Yun, Sungrack and Porikli, Fatih},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{sun2021dynamic,
  title={Dynamic network quantization for efficient video inference},
  author={Sun, Ximeng and Panda, Rameswar and Chen, Chun-Fu Richard and Oliva, Aude and Feris, Rogerio and Saenko, Kate},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{abati2023resq,
  title={ResQ: Residual Quantization for Video Perception},
  author={Abati, Davide and Ben Yahia, Haitam and Nagel, Markus and Habibian, Amirhossein},
  booktitle={ICCV},
  year={2023}
}

@article{ma2022rethinking,
  title={Rethinking resolution in the context of efficient video recognition},
  author={Ma, Chuofan and Guo, Qiushan and Jiang, Yi and Luo, Ping and Yuan, Zehuan and Qi, Xiaojuan},
  journal={NeurIPS},
  year={2022}
}

@article{zhang2022look,
  title={Look more but care less in video recognition},
  author={Zhang, Yitian and Bai, Yue and Wang, Huan and Xu, Yi and Fu, Yun},
  journal={NeurIPS},
  year={2022}
}

@inproceedings{wu2024haltingvt,
  title={HaltingVT: Adaptive Token Halting Transformer for Efficient Video Recognition},
  author={Wu, Qian and Cui, Ruoxuan and Li, Yuke and Zhu, Haoqi},
  booktitle={ICASSP},
  year={2024},
}

@article{sandvine2024global,
  title={Global internet phenomena report},
  author={Sandvine, I},
  journal={North America and Latin America},
  year={2024}
}

@inproceedings{zhao2018sound,
  title={The sound of pixels},
  author={Zhao, Hang and Gan, Chuang and Rouditchenko, Andrew and Vondrick, Carl and McDermott, Josh and Torralba, Antonio},
  booktitle={ECCV},
  year={2018}
}

@article{gong2022uavm,
  title={Uavm: Towards unifying audio and visual models},
  author={Gong, Yuan and Liu, Alexander H and Rouditchenko, Andrew and Glass, James},
  journal={IEEE SPL},
  year={2022},
}

@inproceedings{hu2019deep,
  title={Deep multimodal clustering for unsupervised audiovisual learning},
  author={Hu, Di and Nie, Feiping and Li, Xuelong},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{amrani2021noise,
  title={Noise estimation using density estimation for self-supervised multimodal learning},
  author={Amrani, Elad and Ben-Ari, Rami and Rotman, Daniel and Bronstein, Alex},
  booktitle={AAAI},
  year={2021}
}


@inproceedings{hu2022mix,
  title={Mix and localize: Localizing sound sources in mixtures},
  author={Hu, Xixi and Chen, Ziyang and Owens, Andrew},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{mo2023unified,
  title={A unified audio-visual learning framework for localization, separation, and recognition},
  author={Mo, Shentong and Morgado, Pedro},
  booktitle={ICML},
  year={2023},
}

@inproceedings{wu2021exploring,
  title={Exploring heterogeneous clues for weakly-supervised audio-visual video parsing},
  author={Wu, Yu and Yang, Yi},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{ma2021active,
  title={Active Contrastive Learning of Audio-Visual Video Representations},
  author={Ma, Shuang and Zeng, Zhaoyang and McDuff, Daniel and Song, Yale},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{morgado2021audio,
  title={Audio-visual instance discrimination with cross-modal agreement},
  author={Morgado, Pedro and Vasconcelos, Nuno and Misra, Ishan},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{singh2024looking,
  title={Looking similar sounding different: Leveraging counterfactual cross-modal pairs for audiovisual representation learning},
  author={Singh, Nikhil and Wu, Chih-Wei and Orife, Iroro and Kalayeh, Mahdi},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{gong2023contrastive,
  title={Contrastive audio-visual masked autoencoder},
  author={Gong, Yuan and Rouditchenko, Andrew and Liu, Alexander H and Harwath, David and Karlinsky, Leonid and Kuehne, Hilde and Glass, James},
  booktitle={ICLR},
  year={2023}
}

@inproceedings{huang2023mavil,
  title={Mavil: Masked audio-video learners},
  author={Huang, Po-Yao and Sharma, Vasu and Xu, Hu and Ryali, Chaitanya and Li, Yanghao and Li, Shang-Wen and Ghosh, Gargi and Malik, Jitendra and Feichtenhofer, Christoph and others},
  booktitle={NeurIPS},
  year={2023}
}

@inproceedings{pian2023audio,
  title={Audio-visual class-incremental learning},
  author={Pian, Weiguo and Mo, Shentong and Guo, Yunhui and Tian, Yapeng},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{georgescu2023audiovisual,
  title={Audiovisual masked autoencoders},
  author={Georgescu, Mariana-Iuliana and Fonseca, Eduardo and Ionescu, Radu Tudor and Lucic, Mario and Schmid, Cordelia and Arnab, Anurag},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{zhang2024multimodal,
  title={Multimodal representation learning by alternating unimodal adaptation},
  author={Zhang, Xiaohui and Yoon, Jaehong and Bansal, Mohit and Yao, Huaxiu},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{guo2024crossmae,
  title={CrossMAE: Cross-Modality Masked Autoencoders for Region-Aware Audio-Visual Pre-Training},
  author={Guo, Yuxin and Sun, Siyang and Ma, Shuailei and Zheng, Kecheng and Bao, Xiaoyi and Ma, Shijie and Zou, Wei and Zheng, Yun},
  booktitle={CVPR},
  year={2024}
}

@article{liang2022foundations,
  title={Foundations and Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions},
  author={Liang, Paul Pu and Zadeh, Amir and Morency, Louis-Philippe},
  journal={arXiv:2209.03430},
  year={2022}
}

@inproceedings{zhang2018unreasonable,
  title={The unreasonable effectiveness of deep features as a perceptual metric},
  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
  booktitle={CVPR},
  year={2018}
}

@article{ding2020image,
  title={Image quality assessment: Unifying structure and texture similarity},
  author={Ding, Keyan and Ma, Kede and Wang, Shiqi and Simoncelli, Eero P},
  journal={IEEE TPAMI},
  year={2020},
}

@article{czolbe2020loss,
  title={A loss function for generative neural networks based on watson’s perceptual model},
  author={Czolbe, Steffen and Krause, Oswin and Cox, Ingemar and Igel, Christian},
  journal={NeurIPS},
  year={2020}
}

@inproceedings{li2019quality,
  title={Quality assessment of in-the-wild videos},
  author={Li, Dingquan and Jiang, Tingting and Jiang, Ming},
  booktitle={MM},
  year={2019}
}

@inproceedings{hou2022perceptual,
  title={A perceptual quality metric for video frame interpolation},
  author={Hou, Qiqi and Ghildyal, Abhijay and Liu, Feng},
  booktitle={ECCV},
  year={2022}
}

@article{ming2024survey,
  title={A Survey on Video Prediction: From Deterministic to Generative Approaches},
  author={Ming, Ruibo and Huang, Zhewei and Ju, Zhuoxuan and Hu, Jianming and Peng, Lihui and Zhou, Shuchang},
  journal={arXiv:2401.14718},
  year={2024}
}

@article{gat2021perceptual,
  title={Perceptual score: What data modalities does your model perceive?},
  author={Gat, Itai and Schwartz, Idan and Schwing, Alex},
  journal={NeurIPS},
  year={2021}
}

@article{stergiou2024lavib,
  title={LAVIB: A Large-scale Video Interpolation Benchmark},
  author={Stergiou, Alexandros},
  journal={NeurIPS},
  year={2024}
}


@article{tewari2023diffusion,
  title={Diffusion with forward models: Solving stochastic inverse problems without direct supervision},
  author={Tewari, Ayush and Yin, Tianwei and Cazenavette, George and Rezchikov, Semon and Tenenbaum, Josh and Durand, Fr{\'e}do and Freeman, Bill and Sitzmann, Vincent},
  journal={NeurIPS},
  year={2023}
}

@article{spielberg2023differentiable,
  title={Differentiable visual computing for inverse problems and machine learning},
  author={Spielberg, Andrew and Zhong, Fangcheng and Rematas, Konstantinos and Jatavallabhula, Krishna Murthy and Oztireli, Cengiz and Li, Tzu-Mao and Nowrouzezahrai, Derek},
  journal={Nature Machine Intelligence},
  year={2023}
}

@inproceedings{chen2024soundingactions,
  title={Soundingactions: Learning how actions sound from narrated egocentric videos},
  author={Chen, Changan and Ashutosh, Kumar and Girdhar, Rohit and Harwath, David and Grauman, Kristen},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{ma2022multimodal,
  title={Are multimodal transformers robust to missing modality?},
  author={Ma, Mengmeng and Ren, Jian and Zhao, Long and Testuggine, Davide and Peng, Xi},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{wang2023distribution,
  title={Distribution-consistent modal recovering for incomplete multimodal learning},
  author={Wang, Yuanzhi and Cui, Zhen and Li, Yong},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{zhang2023learning,
  title={Learning unseen modality interaction},
  author={Zhang, Yunhua and Doughty, Hazel and Snoek, Cees G. M.},
  booktitle={NeurIPS},
  year={2023}
}

@article{konevcny2016federated,
  title={Federated optimization: Distributed machine learning for on-device intelligence},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Ramage, Daniel and Richt{\'a}rik, Peter},
  journal={arXiv:1610.02527},
  year={2016}
}

@inproceedings{schlimmer1986case,
  title={A case study of incremental concept induction},
  author={Schlimmer, Jeffrey C and Fisher, Douglas},
  booktitle={AAAI},
  year={1986}
}

@article{utgoff1989incremental,
  title={Incremental induction of decision trees},
  author={Utgoff, Paul E},
  journal={Machine learning},
  year={1989},
}

@article{bottou1998online,
  title={Online algorithms and stochastic approximations},
  author={Bottou, L{\'e}on},
  journal={Online learning in neural networks},
  year={1998}
}

@inproceedings{kim2024missing,
  title={Missing Modality Prediction for Unpaired Multimodal Learning via Joint Embedding of Unimodal Models},
  author={Kim, Donggeun and Kim, Taesup},
  booktitle={ECCV},
  year={2024}
}

@article{zhu2024languagebind,
  title={Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment},
  author={Zhu, Bin and Lin, Bin and Ning, Munan and Yan, Yang and Cui, Jiaxi and Wang, HongFa and Pang, Yatian and Jiang, Wenhao and Zhang, Junwu and Li, Zongwei and others},
  journal={ICLR 2024},
  year={2024}
}

@article{lin2024siamese,
  title={Siamese vision transformers are scalable audio-visual learners},
  author={Lin, Yan-Bo and Bertasius, Gedas},
  journal={arXiv:2403.19638},
  year={2024}
}

@inproceedings{nagrani2021attention,
  title={Attention bottlenecks for multimodal fusion},
  author={Nagrani, Arsha and Yang, Shan and Arnab, Anurag and Jansen, Aren and Schmid, Cordelia and Sun, Chen},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{fayek2020large,
  title={Large scale audiovisual learning of sounds with weakly labeled data},
  author={Fayek, Haytham M and Kumar, Anurag},
  booktitle={IJCAI},
  year={2020}
}

@inproceedings{wang2020makes,
  title={What makes training multi-modal classification networks hard?},
  author={Wang, Weiyao and Tran, Du and Feiszli, Matt},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{lin2023vision,
  title={Vision transformers are parameter-efficient audio-visual learners},
  author={Lin, Yan-Bo and Sung, Yi-Lin and Lei, Jie and Bansal, Mohit and Bertasius, Gedas},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{jaegle2021perceiver,
  title={Perceiver: General perception with iterative attention},
  author={Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
  booktitle={ICML},
  year={2021},
}

@inproceedings{shahroudy2016ntu,
  title={Ntu rgb+ d: A large scale dataset for 3d human activity analysis},
  author={Shahroudy, Amir and Liu, Jun and Ng, Tian-Tsong and Wang, Gang},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{zhang2013actemes,
  title={From actemes to action: A strongly-supervised representation for detailed action understanding},
  author={Zhang, Weiyu and Zhu, Menglong and Derpanis, Konstantinos G},
  booktitle={ICCV},
  year={2013}
}

@inproceedings{tang2019coin,
  title={Coin: A large-scale dataset for comprehensive instructional video analysis},
  author={Tang, Yansong and Ding, Dajun and Rao, Yongming and Zheng, Yu and Zhang, Danyang and Zhao, Lili and Lu, Jiwen and Zhou, Jie},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{gu2018ava,
  title={Ava: A video dataset of spatio-temporally localized atomic visual actions},
  author={Gu, Chunhui and Sun, Chen and Ross, David A and Vondrick, Carl and Pantofaru, Caroline and Li, Yeqing and Vijayanarasimhan, Sudheendra and Toderici, George and Ricco, Susanna and Sukthankar, Rahul and others},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{ben2021ikea,
  title={The ikea asm dataset: Understanding people assembling furniture through actions, objects and pose},
  author={Ben-Shabat, Yizhak and Yu, Xin and Saleh, Fatemeh and Campbell, Dylan and Rodriguez-Opazo, Cristian and Li, Hongdong and Gould, Stephen},
  booktitle={WACV},
  year={2021}
}

@inproceedings{stergiou2023leaping,
  title={Leaping Into Memories: Space-Time Deep Feature Synthesis},
  author={Stergiou, Alexandros and Deligiannis, Nikos},
  booktitle={ICCV},
  year={2023}
}

@article{kuo2023mammut,
  title={Mammut: A simple architecture for joint learning for multimodal tasks},
  author={Kuo, Weicheng and Piergiovanni, AJ and Kim, Dahun and Luo, Xiyang and Caine, Ben and Li, Wei and Ogale, Abhijit and Zhou, Luowei and Dai, Andrew and Chen, Zhifeng and others},
  journal={TMLR},
  year={2023}
}

@inproceedings{islam2024video,
  title={Video ReCap: Recursive Captioning of Hour-Long Videos},
  author={Islam, Md Mohaiminul and Ho, Ngan and Yang, Xitong and Nagarajan, Tushar and Torresani, Lorenzo and Bertasius, Gedas},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{min2024morevqa,
  title={MoReVQA: Exploring Modular Reasoning Models for Video Question Answering},
  author={Min, Juhong and Buch, Shyamal and Nagrani, Arsha and Cho, Minsu and Schmid, Cordelia},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{xu2017video,
  title={Video question answering via gradually refined attention over appearance and motion},
  author={Xu, Dejing and Zhao, Zhou and Xiao, Jun and Wu, Fei and Zhang, Hanwang and He, Xiangnan and Zhuang, Yueting},
  booktitle={MM},
  year={2017}
}

@article{yu2023self,
  title={Self-chained image-language model for video localization and question answering},
  author={Yu, Shoubin and Cho, Jaemin and Yadav, Prateek and Bansal, Mohit},
  journal={NeurIPS},
  year={2023}
}

@inproceedings{xiao2024can,
  title={Can i trust your answer? visually grounded video question answering},
  author={Xiao, Junbin and Yao, Angela and Li, Yicong and Chua, Tat-Seng},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{gao2023mist,
  title={Mist: Multi-modal iterative spatial-temporal transformer for long-form video question answering},
  author={Gao, Difei and Zhou, Luowei and Ji, Lei and Zhu, Linchao and Yang, Yi and Shou, Mike Zheng},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{xiao2022video,
  title={Video graph transformer for video question answering},
  author={Xiao, Junbin and Zhou, Pan and Chua, Tat-Seng and Yan, Shuicheng},
  booktitle={ECCV},
  year={2022}
}

@article{xiao2023contrastive,
  title={Contrastive video question answering via video graph transformer},
  author={Xiao, Junbin and Zhou, Pan and Yao, Angela and Li, Yicong and Hong, Richang and Yan, Shuicheng and Chua, Tat-Seng},
  journal={IEEE TPAMI},
  year={2023},
}

@inproceedings{patrick2020support,
  title={Support-set bottlenecks for video-text representation learning},
  author={Patrick, Mandela and Huang, Po-Yao and Asano, Yuki and Metze, Florian and Hauptmann, Alexander and Henriques, Joao and Vedaldi, Andrea},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{chun2021probabilistic,
  title={Probabilistic embeddings for cross-modal retrieval},
  author={Chun, Sanghyuk and Oh, Seong Joon and De Rezende, Rafael Sampaio and Kalantidis, Yannis and Larlus, Diane},
  booktitle={CVPR},
  year={2021}
}

@article{hao2024uncertainty,
  title={Uncertainty-aware alignment network for cross-domain video-text retrieval},
  author={Hao, Xiaoshuai and Zhang, Wanqian},
  journal={NeurIPS},
  year={2024}
}

@inproceedings{li2023progressive,
  title={Progressive spatio-temporal prototype matching for text-video retrieval},
  author={Li, Pandeng and Xie, Chen-Wei and Zhao, Liming and Xie, Hongtao and Ge, Jiannan and Zheng, Yun and Zhao, Deli and Zhang, Yongdong},
  booktitle={ICCV},
  year={2023}
}


@inproceedings{jang2017tgif,
  title={Tgif-qa: Toward spatio-temporal reasoning in visual question answering},
  author={Jang, Yunseok and Song, Yale and Yu, Youngjae and Kim, Youngjin and Kim, Gunhee},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{fan2019heterogeneous,
  title={Heterogeneous memory enhanced multimodal attention model for video question answering},
  author={Fan, Chenyou and Zhang, Xiaofan and Zhang, Shu and Wang, Wensheng and Zhang, Chi and Huang, Heng},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{ye2017video,
  title={Video question answering via attribute-augmented attention network learning},
  author={Ye, Yunan and Zhao, Zhou and Li, Yimeng and Chen, Long and Xiao, Jun and Zhuang, Yueting},
  booktitle={SIGIR},
  year={2017}
}

@inproceedings{yu2017end,
  title={End-to-end concept word detection for video captioning, retrieval, and question answering},
  author={Yu, Youngjae and Ko, Hyungjin and Choi, Jongwook and Kim, Gunhee},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{zeng2017leveraging,
  title={Leveraging video descriptions to learn video question answering},
  author={Zeng, Kuo-Hao and Chen, Tseng-Hung and Chuang, Ching-Yao and Liao, Yuan-Hong and Niebles, Juan Carlos and Sun, Min},
  booktitle={AAAI},
  year={2017}
}

@inproceedings{gao2018motion,
  title={Motion-appearance co-memory networks for video question answering},
  author={Gao, Jiyang and Ge, Runzhou and Chen, Kan and Nevatia, Ram},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{huang2020location,
  title={Location-aware graph convolutional networks for video question answering},
  author={Huang, Deng and Chen, Peihao and Zeng, Runhao and Du, Qing and Tan, Mingkui and Gan, Chuang},
  booktitle={AAAI},
  year={2020}
}

@inproceedings{li2019beyond,
  title={Beyond rnns: Positional self-attention with co-attention for video question answering},
  author={Li, Xiangpeng and Song, Jingkuan and Gao, Lianli and Liu, Xianglong and Huang, Wenbing and He, Xiangnan and Gan, Chuang},
  booktitle={AAAI},
  year={2019}
}

@inproceedings{mavroudi2023learning,
  title={Learning to ground instructional articles in videos through narrations},
  author={Mavroudi, Effrosyni and Afouras, Triantafyllos and Torresani, Lorenzo},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{li2022equivariant,
  title={Equivariant and invariant grounding for video question answering},
  author={Li, Yicong and Wang, Xiang and Xiao, Junbin and Chua, Tat-Seng},
  booktitle={MM},
  year={2022}
}

@inproceedings{li2023discovering,
  title={Discovering spatio-temporal rationales for video question answering},
  author={Li, Yicong and Xiao, Junbin and Feng, Chun and Wang, Xiang and Chua, Tat-Seng},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{jiang2020reasoning,
  title={Reasoning with heterogeneous graph alignment for video question answering},
  author={Jiang, Pin and Han, Yahong},
  booktitle={AAAI},
  year={2020}
}

@inproceedings{park2021bridge,
  title={Bridge to answer: Structure-aware graph interaction network for video question answering},
  author={Park, Jungin and Lee, Jiyoung and Sohn, Kwanghoon},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{guo2021multi,
  title={Multi-scale progressive attention network for video question answering},
  author={Guo, Zhicheng and Zhao, Jiaxuan and Jiao, Licheng and Liu, Xu and Li, Lingling},
  booktitle={ACL},
  year={2021}
}

@inproceedings{flanagan2023learning,
  title={Learning temporal sentence grounding from narrated egovideos},
  author={Flanagan, Kevin and Damen, Dima and Wray, Michael},
  booktitle={BMVC},
  year={2023}
}

@inproceedings{rohrbach2015dataset,
  title={A dataset for movie description},
  author={Rohrbach, Anna and Rohrbach, Marcus and Tandon, Niket and Schiele, Bernt},
  booktitle={CVPR},
  year={2015}
}

@article{tang2023video,
  title={Video understanding with large language models: A survey},
  author={Tang, Yunlong and Bi, Jing and Xu, Siting and Song, Luchuan and Liang, Susan and Wang, Teng and Zhang, Daoan and An, Jie and Lin, Jingyang and Zhu, Rongyi and others},
  journal={arXiv:2312.17432},
  year={2023}
}

@inproceedings{anne2017localizing,
  title={Localizing moments in video with natural language},
  author={Hendricks, Lisa Anne and Wang, Oliver and Shechtman, Eli and Sivic, Josef and Darrell, Trevor and Russell, Bryan},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{gao2017tall,
  title={Tall: Temporal activity localization via language query},
  author={Gao, Jiyang and Sun, Chen and Yang, Zhenheng and Nevatia, Ram},
  booktitle={ICCV},
  year={2017}
}

@article{regneri2013grounding,
  title={Grounding action descriptions in videos},
  author={Regneri, Michaela and Rohrbach, Marcus and Wetzel, Dominikus and Thater, Stefan and Schiele, Bernt and Pinkal, Manfred},
  journal={TACL},
  year={2013},
}

@inproceedings{zhang2023video,
  title={Video-llama: An instruction-tuned audio-visual language model for video understanding},
  author={Zhang, Hang and Li, Xin and Bing, Lidong},
  booktitle={EMNLP},
  year={2023}
}

@inproceedings{ngiam2011multimodal,
  title={Multimodal deep learning},
  author={Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y},
  booktitle={ICML},
  year={2011}
}

@inproceedings{paredes2012exploiting,
  title={Exploiting unrelated tasks in multi-task learning},
  author={Paredes, Bernardino Romera and Argyriou, Andreas and Berthouze, Nadia and Pontil, Massimiliano},
  booktitle={AISTATS},
  year={2012}
}

@inproceedings{cao2021pursuit,
  title={On pursuit of designing multi-modal transformer for video grounding},
  author={Cao, Meng and Chen, Long and Shou, Mike Zheng and Zhang, Can and Zou, Yuexian},
  booktitle={EMNLP},
  year={2021}
}

@inproceedings{qian2024momentor,
  title={Momentor: Advancing video large language model with fine-grained temporal reasoning},
  author={Qian, Long and Li, Juncheng and Wu, Yu and Ye, Yaobo and Fei, Hao and Chua, Tat-Seng and Zhuang, Yueting and Tang, Siliang},
  booktitle={ICML},
  year={2024}
}

@inproceedings{gu2024context,
  title={Context-Guided Spatio-Temporal Video Grounding},
  author={Gu, Xin and Fan, Heng and Huang, Yan and Luo, Tiejian and Zhang, Libo},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{kim2024you,
  title={Do You Remember? Dense Video Captioning with Cross-Modal Memory Retrieval},
  author={Kim, Minkuk and Kim, Hyeon Bae and Moon, Jinyoung and Choi, Jinwoo and Kim, Seong Tae},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{wray2019fine,
  title={Fine-grained action retrieval through multiple parts-of-speech embeddings},
  author={Wray, Michael and Larlus, Diane and Csurka, Gabriela and Damen, Dima},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{gabeur2020multi,
  title={Multi-modal transformer for video retrieval},
  author={Gabeur, Valentin and Sun, Chen and Alahari, Karteek and Schmid, Cordelia},
  booktitle={ECCV},
  year={2020}
}

@article{albanie2020end,
  title={The end-of-end-to-end: A video understanding pentathlon challenge (2020)},
  author={Albanie, Samuel and Liu, Yang and Nagrani, Arsha and Miech, Antoine and Coto, Ernesto and Laptev, Ivan and Sukthankar, Rahul and Ghanem, Bernard and Zisserman, Andrew and Gabeur, Valentin and others},
  journal={arXiv:2008.00744},
  year={2020}
}

@inproceedings{alwassel2018diagnosing,
  title={Diagnosing error in temporal action detectors},
  author={Alwassel, Humam and Heilbron, Fabian Caba and Escorcia, Victor and Ghanem, Bernard},
  booktitle={ECCV},
  year={2018}
}

@article{eagleman2010does,
  title={How does the timing of neural signals map onto the timing of perception},
  author={Eagleman, David M},
  journal={Space and time in perception and action},
  year={2010},
}

@article{wang2012life,
  title={Life motion signals lengthen perceived temporal duration},
  author={Wang, Li and Jiang, Yi},
  journal={National Academy of Sciences},
  year={2012}
}

@article{morrone2005saccadic,
  title={Saccadic eye movements cause compression of time as well as space},
  author={Morrone, M Concetta and Ross, John and Burr, David},
  journal={Nature Neuroscience},
  year={2005}
}

@inproceedings{anderson2018vision,
  title={Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments},
  author={Anderson, Peter and Wu, Qi and Teney, Damien and Bruce, Jake and Johnson, Mark and S{\"u}nderhauf, Niko and Reid, Ian and Gould, Stephen and Van Den Hengel, Anton},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{liu2019use,
  title={Use what you have: Video retrieval using representations from collaborative experts},
  author={Liu, Yang and Albanie, Samuel and Nagrani, Arsha and Zisserman, Andrew},
  booktitle={BMVC},
  year={2019}
}

@inproceedings{mithun2018learning,
  title={Learning joint embedding with multimodal cues for cross-modal video-text retrieval},
  author={Mithun, Niluthpol Chowdhury and Li, Juncheng and Metze, Florian and Roy-Chowdhury, Amit K},
  booktitle={ICMR},
  year={2018}
}

@inproceedings{gordo2017beyond,
  title={Beyond instance-level image retrieval: Leveraging captions to learn a global visual representation for semantic retrieval},
  author={Gordo, Albert and Larlus, Diane},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{kingma2013auto,
  title={Auto-encoding variational bayes. arXiv e-prints},
  author={Kingma, Diederik P and Welling, Max},
  booktitle={ICLR},
  year={2013}
}

@inproceedings{wang2016learning,
  title={Learning deep structure-preserving image-text embeddings},
  author={Wang, Liwei and Li, Yin and Lazebnik, Svetlana},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{wray2021semantic,
  title={On semantic similarity in video retrieval},
  author={Wray, Michael and Doughty, Hazel and Damen, Dima},
  booktitle={CVPR},
  year={2021}
}

@article{dong2018predicting,
  title={Predicting visual features from text for image and video caption retrieval},
  author={Dong, Jianfeng and Li, Xirong and Snoek, Cees G. M.},
  journal={IEEE TM},
  year={2018}
}

@inproceedings{otani2016learning,
  title={Learning joint representations of videos and sentences with web image search},
  author={Otani, Mayu and Nakashima, Yuta and Rahtu, Esa and Heikkil{\"a}, Janne and Yokoya, Naokazu},
  booktitle={ECCVw},
  year={2016}
}

@article{torabi2016learning,
  title={Learning language-visual embedding for movie understanding with natural-language},
  author={Torabi, Atousa and Tandon, Niket and Sigal, Leonid},
  journal={arXiv:1609.08124},
  year={2016}
}

@inproceedings{xu2015jointly,
  title={Jointly modeling deep video and compositional text to bridge vision and language in a unified framework},
  author={Xu, Ran and Xiong, Caiming and Chen, Wei and Corso, Jason},
  booktitle={AAAI},
  year={2015}
}

@inproceedings{oncescu2021queryd,
  title={Queryd: A video dataset with high-quality text and audio narrations},
  author={Oncescu, Andreea-Maria and Henriques, Joao F and Liu, Yang and Zisserman, Andrew and Albanie, Samuel},
  booktitle={ICASSP},
  year={2021}
}

@article{yang2022zero,
  title={Zero-shot video question answering via frozen bidirectional language models},
  author={Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
  journal={NeurIPS},
  year={2022}
}

@inproceedings{iashin2020better,
  title={A better use of audio-visual cues: Dense video captioning with bi-modal transformer},
  author={Iashin, Vladimir and Rahtu, Esa},
  booktitle={BMVC},
  year={2020}
}

@inproceedings{iashin2020multi,
  title={Multi-modal dense video captioning},
  author={Iashin, Vladimir and Rahtu, Esa},
  booktitle={CVPRw},
  year={2020}
}

@inproceedings{krishna2017dense,
  title={Dense-captioning events in videos},
  author={Krishna, Ranjay and Hata, Kenji and Ren, Frederic and Fei-Fei, Li and Carlos Niebles, Juan},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{wang2018bidirectional,
  title={Bidirectional attentive fusion with context gating for dense video captioning},
  author={Wang, Jingwen and Jiang, Wenhao and Ma, Lin and Liu, Wei and Xu, Yong},
  booktitle={CVPR},
  year={2018}
}

@article{wang2020event,
  title={Event-centric hierarchical representation for dense video captioning},
  author={Wang, Teng and Zheng, Huicheng and Yu, Mingjing and Tian, Qian and Hu, Haifeng},
  journal={IEEE TCSVT},
  year={2020},
}

@inproceedings{liu2021hair,
  title={Hair: Hierarchical visual-semantic relational reasoning for video question answering},
  author={Liu, Fei and Liu, Jing and Wang, Weining and Lu, Hanqing},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{dang2021hierarchical,
  title={Hierarchical Object-oriented Spatio-Temporal Reasoning for Video Question Answering},
  author={Dang, Long Hoang and Le, Thao Minh and Le, Vuong and Tran, Truyen},
  booktitle={IJCAI},
  year={2021}
}

@inproceedings{zhu2020actbert,
  title={Actbert: Learning global-local video-text representations},
  author={Zhu, Linchao and Yang, Yi},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{chadha2021iperceive,
  title={iPerceive: Applying common-sense reasoning to multi-modal dense video captioning and video question answering},
  author={Chadha, Aman and Arora, Gurneet and Kaloty, Navpreet},
  booktitle={WACV},
  year={2021}
}

@inproceedings{alayrac2016unsupervised,
  title={Unsupervised learning from narrated instruction videos},
  author={Alayrac, Jean-Baptiste and Bojanowski, Piotr and Agrawal, Nishant and Sivic, Josef and Laptev, Ivan and Lacoste-Julien, Simon},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{chen2021towards,
  title={Towards bridging event captioner and sentence localizer for weakly supervised dense event captioning},
  author={Chen, Shaoxiang and Jiang, Yu-Gang},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{dwibedi2020counting,
  title={Counting out time: Class agnostic video repetition counting in the wild},
  author={Dwibedi, Debidatta and Aytar, Yusuf and Tompson, Jonathan and Sermanet, Pierre and Zisserman, Andrew},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{deng2021sketch,
  title={Sketch, ground, and refine: Top-down dense video captioning},
  author={Deng, Chaorui and Chen, Shizhe and Chen, Da and He, Yuan and Wu, Qi},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{li2018jointly,
  title={Jointly localizing and describing events for dense video captioning},
  author={Li, Yehao and Yao, Ting and Pan, Yingwei and Chao, Hongyang and Mei, Tao},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{chen2017generating,
  title={Generating video descriptions with topic guidance},
  author={Chen, Shizhe and Chen, Jia and Jin, Qin},
  booktitle={ICMR},
  year={2017}
}

@inproceedings{bain2020condensed,
  title={Condensed movies: Story based retrieval with contextual embeddings},
  author={Bain, Max and Nagrani, Arsha and Brown, Andrew and Zisserman, Andrew},
  booktitle={ACCV},
  year={2020}
}

@inproceedings{xiao2022hierarchical,
  title={Hierarchical self-supervised representation learning for movie understanding},
  author={Xiao, Fanyi and Kundu, Kaustav and Tighe, Joseph and Modolo, Davide},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{yu2021transitional,
  title={Transitional adaptation of pretrained models for visual storytelling},
  author={Yu, Youngjae and Chung, Jiwan and Yun, Heeseung and Kim, Jongseok and Kim, Gunhee},
  booktitle={CVPR},
  year={2021}
}

@article{li2019video,
  title={Video storytelling: Textual summaries for events},
  author={Li, Junnan and Wong, Yongkang and Zhao, Qi and Kankanhalli, Mohan S},
  journal={IEEE T-M},
  year={2019}
}

@inproceedings{guadarrama2013youtube2text,
  title={Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition},
  author={Guadarrama, Sergio and Krishnamoorthy, Niveda and Malkarnenkar, Girish and Venugopalan, Subhashini and Mooney, Raymond and Darrell, Trevor and Saenko, Kate},
  booktitle={ICCV},
  year={2013}
}

@article{tian2024videotetris,
  title={VideoTetris: Towards Compositional Text-to-Video Generation},
  author={Tian, Ye and Yang, Ling and Yang, Haotian and Gao, Yuan and Deng, Yufan and Chen, Jingmin and Wang, Xintao and Yu, Zhaochen and Tao, Xin and Wan, Pengfei and others},
  journal={arXiv:2406.04277},
  year={2024}
}

@inproceedings{deng2019irc,
  title={IRC-GAN: Introspective Recurrent Convolutional GAN for Text-to-video Generation.},
  author={Deng, Kangle and Fei, Tianyi and Huang, Xin and Peng, Yuxin},
  booktitle={IJCAI},
  year={2019}
}

@inproceedings{gupta2022rv,
  title={Rv-gan: Recurrent gan for unconditional video generation},
  author={Gupta, Sonam and Keshari, Arti and Das, Sukhendu},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{tian2021good,
  title={A good image generator is what you need for high-resolution video synthesis},
  author={Tian, Yu and Ren, Jian and Chai, Menglei and Olszewski, Kyle and Peng, Xi and Metaxas, Dimitris N and Tulyakov, Sergey},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{munoz2021temporal,
  title={Temporal shift GAN for large scale video generation},
  author={Munoz, Andres and Zolfaghari, Mohammadreza and Argus, Max and Brox, Thomas},
  booktitle={WACV},
  year={2021}
}

@article{an2023latent,
  title={Latent-shift: Latent diffusion with temporal shift for efficient text-to-video generation},
  author={An, Jie and Zhang, Songyang and Yang, Harry and Gupta, Sonal and Huang, Jia-Bin and Luo, Jiebo and Yin, Xi},
  journal={arXiv:2304.08477},
  year={2023}
}

@inproceedings{lee2024grid,
  title={Grid Diffusion Models for Text-to-Video Generation},
  author={Lee, Taegyeong and Kwon, Soyeong and Kim, Taehwan},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{wang2024recipe,
  title={A recipe for scaling up text-to-video generation with text-free videos},
  author={Wang, Xiang and Zhang, Shiwei and Yuan, Hangjie and Qing, Zhiwu and Gong, Biao and Zhang, Yingya and Shen, Yujun and Gao, Changxin and Sang, Nong},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{wei2024dreamvideo,
  title={Dreamvideo: Composing your dream videos with customized subject and motion},
  author={Wei, Yujie and Zhang, Shiwei and Qing, Zhiwu and Yuan, Hangjie and Liu, Zhiheng and Liu, Yu and Zhang, Yingya and Zhou, Jingren and Shan, Hongming},
  booktitle={CVPR},
  year={2024}
}

@article{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={NeurIPS},
  year={2014}
}

@inproceedings{wang2023styleinv,
  title={Styleinv: A temporal style modulated inversion network for unconditional video generation},
  author={Wang, Yuhan and Jiang, Liming and Loy, Chen Change},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{fei2024dysen,
  title={Dysen-VDM: Empowering Dynamics-aware Text-to-Video Diffusion with LLMs},
  author={Fei, Hao and Wu, Shengqiong and Ji, Wei and Zhang, Hanwang and Chua, Tat-Seng},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{pan2017video,
  title={Video captioning with transferred semantic attributes},
  author={Pan, Yingwei and Yao, Ting and Li, Houqiang and Mei, Tao},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{aafaq2019spatio,
  title={Spatio-temporal dynamics and semantic attribute enriched visual encoding for video captioning},
  author={Aafaq, Nayyer and Akhtar, Naveed and Liu, Wei and Gilani, Syed Zulqarnain and Mian, Ajmal},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{wang2018reconstruction,
  title={Reconstruction network for video captioning},
  author={Wang, Bairui and Ma, Lin and Zhang, Wei and Liu, Wei},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{zheng2020syntax,
  title={Syntax-aware action targeting for video captioning},
  author={Zheng, Qi and Wang, Chaoyue and Tao, Dacheng},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{lin2022swinbert,
  title={Swinbert: End-to-end transformers with sparse attention for video captioning},
  author={Lin, Kevin and Li, Linjie and Lin, Chung-Ching and Ahmed, Faisal and Gan, Zhe and Liu, Zicheng and Lu, Yumao and Wang, Lijuan},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{ye2022hierarchical,
  title={Hierarchical modular network for video captioning},
  author={Ye, Hanhua and Li, Guorong and Qi, Yuankai and Wang, Shuhui and Huang, Qingming and Yang, Ming-Hsuan},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{shen2023accurate,
  title={Accurate and fast compressed video captioning},
  author={Shen, Yaojie and Gu, Xin and Xu, Kai and Fan, Heng and Wen, Longyin and Zhang, Libo},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{yan2023prompt,
  title={Prompt Learns Prompt: Exploring Knowledge-Aware Generative Prompt Collaboration For Video Captioning.},
  author={Yan, Liqi and Han, Cheng and Xu, Zenglin and Liu, Dongfang and Wang, Qifan},
  booktitle={IJCAI},
  year={2023}
}

@article{barnard2003matching,
  title={Matching words and pictures},
  author={Barnard, Kobus and Duygulu, Pinar and Forsyth, David and De Freitas, Nando and Blei, David M and Jordan, Michael I},
  journal={JMLR},
  year={2003}
}

@inproceedings{barnard2001learning,
  title={Learning the semantics of words and pictures},
  author={Barnard, Kobus and Forsyth, David},
  booktitle={ICCV},
  year={2001}
}

@article{mokady2021clipcap,
  title={Clipcap: Clip prefix for image captioning},
  author={Mokady, Ron and Hertz, Amir and Bermano, Amit H},
  journal={arXiv:2111.09734},
  year={2021}
}

@inproceedings{gan2017semantic,
  title={Semantic compositional networks for visual captioning},
  author={Gan, Zhe and Gan, Chuang and He, Xiaodong and Pu, Yunchen and Tran, Kenneth and Gao, Jianfeng and Carin, Lawrence and Deng, Li},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{mun2019streamlined,
  title={Streamlined dense video captioning},
  author={Mun, Jonghwan and Yang, Linjie and Ren, Zhou and Xu, Ning and Han, Bohyung},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{rahman2019watch,
  title={Watch, listen and tell: Multi-modal weakly supervised dense event captioning},
  author={Rahman, Tanzila and Xu, Bicheng and Sigal, Leonid},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{shen2017weakly,
  title={Weakly supervised dense video captioning},
  author={Shen, Zhiqiang and Li, Jianguo and Su, Zhou and Li, Minjun and Chen, Yurong and Jiang, Yu-Gang and Xue, Xiangyang},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{shi2019dense,
  title={Dense procedure captioning in narrated instructional videos},
  author={Shi, Botian and Ji, Lei and Liang, Yaobo and Duan, Nan and Chen, Peng and Niu, Zhendong and Zhou, Ming},
  booktitle={ACL},
  year={2019}
}

@inproceedings{aytar2016soundnet,
  title={Soundnet: Learning sound representations from unlabeled video},
  author={Aytar, Yusuf and Vondrick, Carl and Torralba, Antonio},
  booktitle={NeurIPS},
  year={2016}
}

@inproceedings{shang2017video,
  title={Video visual relation detection},
  author={Shang, Xindi and Ren, Tongwei and Guo, Jingfan and Zhang, Hanwang and Chua, Tat-Seng},
  booktitle={MM},
  year={2017}
}

@inproceedings{xu2015learning,
  title={Learning deep representations of appearance and motion for anomalous event detection},
  author={Xu, Dan and Ricci, Elisa and Yan, Yan and Song, Jingkuan and Sebe, Nicu},
  booktitle={BMVC},
  year={2015}
}

@inproceedings{wang2021end,
  title={End-to-end dense video captioning with parallel decoding},
  author={Wang, Teng and Zhang, Ruimao and Lu, Zhichao and Zheng, Feng and Cheng, Ran and Luo, Ping},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{zhou2018end,
  title={End-to-end dense video captioning with masked transformer},
  author={Zhou, Luowei and Zhou, Yingbo and Corso, Jason J and Socher, Richard and Xiong, Caiming},
  booktitle={CVPR},
  year={2018}
}

@article{akbari2021vatt,
  title={Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text},
  author={Akbari, Hassan and Yuan, Liangzhe and Qian, Rui and Chuang, Wei-Hong and Chang, Shih-Fu and Cui, Yin and Gong, Boqing},
  journal={NeurIPS},
  year={2021}
}

@inproceedings{bain2021frozen,
  title={Frozen in time: A joint video and image encoder for end-to-end retrieval},
  author={Bain, Max and Nagrani, Arsha and Varol, G{\"u}l and Zisserman, Andrew},
  booktitle={ICCV},
  year={2021}
}

@article{fu2021violet,
  title={Violet: End-to-end video-language transformers with masked visual-token modeling},
  author={Fu, Tsu-Jui and Li, Linjie and Gan, Zhe and Lin, Kevin and Wang, William Yang and Wang, Lijuan and Liu, Zicheng},
  journal={arXiv:2111.12681},
  year={2021}
}

@inproceedings{ge2022bridging,
  title={Bridging video-text retrieval with multiple choice questions},
  author={Ge, Yuying and Ge, Yixiao and Liu, Xihui and Li, Dian and Shan, Ying and Qie, Xiaohu and Luo, Ping},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{han2022temporal,
  title={Temporal alignment networks for long-term video},
  author={Han, Tengda and Xie, Weidi and Zisserman, Andrew},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{ko2022video,
  title={Video-text representation learning via differentiable weak temporal alignment},
  author={Ko, Dohwan and Choi, Joonmyung and Ko, Juyeon and Noh, Shinyeong and On, Kyoung-Woon and Kim, Eun-Sol and Kim, Hyunwoo J},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{wang2022object,
  title={Object-aware video-language pre-training for retrieval},
  author={Wang, Jinpeng and Ge, Yixiao and Cai, Guanyu and Yan, Rui and Lin, Xudong and Shan, Ying and Qie, Xiaohu and Shou, Mike Zheng},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{xue2022advancing,
  title={Advancing high-resolution video-language representation with large-scale video transcriptions},
  author={Xue, Hongwei and Hang, Tiankai and Zeng, Yanhong and Sun, Yuchong and Liu, Bei and Yang, Huan and Fu, Jianlong and Guo, Baining},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{yang2021just,
  title={Just ask: Learning to answer questions from millions of narrated videos},
  author={Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{li2022align,
  title={Align and prompt: Video-and-language pre-training with entity prompts},
  author={Li, Dongxu and Li, Junnan and Li, Hongdong and Niebles, Juan Carlos and Hoi, Steven CH},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{li2020hero,
  title={Hero: Hierarchical encoder for video+ language omni-representation pre-training},
  author={Li, Linjie and Chen, Yen-Chun and Cheng, Yu and Gan, Zhe and Yu, Licheng and Liu, Jingjing},
  booktitle={EMNLP},
  year={2020}
}

@inproceedings{miech2020end,
  title={End-to-end learning of visual representations from uncurated instructional videos},
  author={Miech, Antoine and Alayrac, Jean-Baptiste and Smaira, Lucas and Laptev, Ivan and Sivic, Josef and Zisserman, Andrew},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{seo2022end,
  title={End-to-end generative pretraining for multimodal video captioning},
  author={Seo, Paul Hongsuck and Nagrani, Arsha and Arnab, Anurag and Schmid, Cordelia},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{seo2021look,
  title={Look before you speak: Visually contextualized utterances},
  author={Seo, Paul Hongsuck and Nagrani, Arsha and Schmid, Cordelia},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{sun2019videobert,
  title={Videobert: A joint model for video and language representation learning},
  author={Sun, Chen and Myers, Austin and Vondrick, Carl and Murphy, Kevin and Schmid, Cordelia},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{wang2023all,
  title={All in one: Exploring unified video-language pre-training},
  author={Wang, Jinpeng and Ge, Yixiao and Yan, Rui and Ge, Yuying and Lin, Kevin Qinghong and Tsutsui, Satoshi and Lin, Xudong and Cai, Guanyu and Wu, Jianping and Shan, Ying and others},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{hu2022transrac,
  title={Transrac: Encoding multi-scale temporal correlation with transformers for repetitive action counting},
  author={Hu, Huazhang and Dong, Sixun and Zhao, Yiqun and Lian, Dongze and Li, Zhengxin and Gao, Shenghua},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{zhang2020context,
  title={Context-aware and scale-insensitive temporal repetition counting},
  author={Zhang, Huaidong and Xu, Xuemiao and Han, Guoqiang and He, Shengfeng},
  booktitle={CVPR},
  year={2020}
}


@article{lu2004repetitive,
  title={{Repetitive Motion Analysis: Segmentation and Event Classification}},
  author={Lu, ChunMei and Ferrier, Nicola J},
  journal={IEEE TPAMI},
  year={2004},
}

@inproceedings{panagiotakis2018unsupervised,
  title={{Unsupervised Detection of Periodic Segments in Videos}},
  author={Panagiotakis, Costas and Karvounas, Giorgos and Argyros, Antonis},
  booktitle={ICIP},
  year={2018}
}

@inproceedings{thangali2005periodic,
  title={Periodic motion detection and estimation via space-time sampling},
  author={Thangali, Ashwin and Sclaroff, Stan},
  booktitle={WACV},
  year={2005}
}

@article{junejo2010view,
  title={View-independent action recognition from temporal self-similarities},
  author={Junejo, Imran N and Dexter, Emilie and Laptev, Ivan and Perez, Patrick},
  journal={IEEE TPAMI},
  year={2010}
}

@inproceedings{korner2013temporal,
  title={Temporal self-similarity for appearance-based action recognition in multi-view setups},
  author={K{\"o}rner, Marco and Denzler, Joachim},
  booktitle={CAIP},
  year={2013},
}

@article{benabdelkader2004gait,
  title={Gait recognition using image self-similarity},
  author={BenAbdelkader, Chiraz and Cutler, Ross G and Davis, Larry S},
  journal={EURASIP},
  year={2004},
}

@inproceedings{runia2018real,
  title={{Real-World Repetition Estimation by Div, Grad and Curl}},
  author={Runia, Tom F. H. and Snoek, Cees G. M. and Smeulders, Arnold W.M.},
  booktitle={CVPR},
  year={2018}
}

@article{ferreira2021deep,
  title={{Deep Learning Approaches for Workout Repetition Counting and Validation}},
  author={Ferreira, Bruno and Ferreira, Pedro M and Pinheiro, Gil and Figueiredo, Nelson and Carvalho, Filipe and Menezes, Paulo and Batista, Jorge},
  journal={PRL},
  year={2021}
}

@article{yao2023poserac,
  title={{PoseRAC: Pose Saliency Transformer for Repetitive Action Counting}},
  author={Yao, Ziyu and Cheng, Xuxin and Zou, Yuexian},
  journal={arXiv:2303.08450},
  year={2023}
}

@article{nelson1980factors,
  title={Factors influencing young children's use of motives and outcomes as moral criteria},
  author={Nelson, Sharon A},
  journal={Child Development},
  year={1980}
}

@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

@inproceedings{poli2023hyena,
  title={Hyena hierarchy: Towards larger convolutional language models},
  author={Poli, Michael and Massaroli, Stefano and Nguyen, Eric and Fu, Daniel Y and Dao, Tri and Baccus, Stephen and Bengio, Yoshua and Ermon, Stefano and R{\'e}, Christopher},
  booktitle={ICLR},
  year={2023},
}

@article{kohler1967gestalt,
  title={Gestalt psychology},
  author={K{\"o}hler, Wolfgang},
  journal={Psychologische forschung},
  year={1967}
}

@book{koffka2013principles,
  title={Principles of Gestalt psychology},
  author={Koffka, Kurt},
  journal={Psychologische forschung},
  year={2013},
  publisher={routledge}
}

@inproceedings{khattak2023maple,
  title={Maple: Multi-modal prompt learning},
  author={Khattak, Muhammad Uzair and Rasheed, Hanoona and Maaz, Muhammad and Khan, Salman and Khan, Fahad Shahbaz},
  booktitle={CVPR},
  year={2023}
}

@article{mettes2024hyperbolic,
  title={Hyperbolic deep learning in computer vision: A survey},
  author={Mettes, Pascal and Ghadimi Atigh, Mina and Keller-Ressel, Martin and Gu, Jeffrey and Yeung, Serena},
  journal={IJCV},
  year={2024}
}

@inproceedings{li2024deal,
  title={DEAL: Disentangle and Localize Concept-level Explanations for VLMs},
  author={Li, Tang and Ma, Mengmeng and Peng, Xi},
  booktitle={ECCV},
  year={2024},
}

@inproceedings{kim2023exposing,
  title={Exposing and mitigating spurious correlations for cross-modal retrieval},
  author={Kim, Jae Myung and Koepke, A and Schmid, Cordelia and Akata, Zeynep},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{chen2020counterfactual,
  title={Counterfactual samples synthesizing for robust visual question answering},
  author={Chen, Long and Yan, Xin and Xiao, Jun and Zhang, Hanwang and Pu, Shiliang and Zhuang, Yueting},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{tian2024argue,
  title={ArGue: Attribute-Guided Prompt Tuning for Vision-Language Models},
  author={Tian, Xinyu and Zou, Shu and Yang, Zhaoyuan and Zhang, Jing},
  booktitle={CVPR},
  year={2024}
}

@article{zhang2024rethinking,
  title={Rethinking Misalignment in Vision-Language Model Adaptation from a Causal Perspective},
  author={Zhang, Yanan and Li, Jiangmeng and Liu, Lixiang and Qiang, Wenwen},
  journal={NeurIPS},
  year={2024}
}

@inproceedings{alper2024emergent,
  title={Emergent visual-semantic hierarchies in image-text representations},
  author={Alper, Morris and Averbuch-Elor, Hadar},
  booktitle={ECCV},
  year={2024}
}

@article{li2024scene,
  title={Scene graph generation: A comprehensive survey},
  author={Li, Hongsheng and Zhu, Guangming and Zhang, Liang and Jiang, Youliang and Dang, Yixuan and Hou, Haoran and Shen, Peiyi and Zhao, Xia and Shah, Syed Afaq Ali and Bennamoun, Mohammed},
  journal={Neurocomputing},
  year={2024}
}

@article{li2024fmm,
  title={Fmm-attack: A flow-based multi-modal adversarial attack on video-based llms},
  author={Li, Jinmin and Gao, Kuofeng and Bai, Yang and Zhang, Jingyun and Xia, Shu-tao and Wang, Yisen},
  journal={arXiv:2403.13507},
  year={2024}
}

@article{shen2024tag,
  title={Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains},
  author={Shen, Junhong and Tenenholtz, Neil and Hall, James Brian and Alvarez-Melis, David and Fusi, Nicolo},
  journal={arXiv:2402.05140},
  year={2024}
}

@article{bai2024generalist,
  title={From Generalist to Specialist: Adapting Vision Language Models via Task-Specific Visual Instruction Tuning},
  author={Bai, Yang and Zhou, Yang and Zhou, Jun and Goh, Rick Siow Mong and Ting, Daniel Shu Wei and Liu, Yong},
  journal={arXiv:2410.06456},
  year={2024}
}

@article{luo2024mmevol,
  title={Mmevol: Empowering multimodal large language models with evol-instruct},
  author={Luo, Run and Zhang, Haonan and Chen, Longze and Lin, Ting-En and Liu, Xiong and Wu, Yuchuan and Yang, Min and Wang, Minzheng and Zeng, Pengpeng and Gao, Lianli and others},
  journal={arXiv:2409.05840},
  year={2024}
}

@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={NeurIPS},
  year={2022}
}

@article{genovese2012minimax,
  title={Minimax manifold estimation},
  author={Genovese, Christopher R and Perone Pacifico, Marco and Verdinelli, Isabella and Wasserman, Larry and others},
  journal={JMLR},
  year={2012}
}

@article{jiang2018trust,
  title={To trust or not to trust a classifier},
  author={Jiang, Heinrich and Kim, Been and Guan, Melody and Gupta, Maya},
  journal={NeurIPS},
  year={2018}
}

@article{he2023manifold,
  title={Manifold preserving guided diffusion},
  author={He, Yutong and Murata, Naoki and Lai, Chieh-Hsin and Takida, Yuhta and Uesaka, Toshimitsu and Kim, Dongjun and Liao, Wei-Hsiang and Mitsufuji, Yuki and Kolter, J Zico and Salakhutdinov, Ruslan and others},
  journal={NeurIPS},
  year={2023}
}

@inproceedings{bordt2023manifold,
  title={The manifold hypothesis for gradient-based explanations},
  author={Bordt, Sebastian and Upadhyay, Uddeshya and Akata, Zeynep and von Luxburg, Ulrike},
  booktitle={CVPRw},
  year={2023}
}

@inproceedings{chen2022vita,
  title={VITA: A multi-source vicinal transfer augmentation method for out-of-distribution generalization},
  author={Chen, Minghui and Wen, Cheng and Zheng, Feng and He, Fengxiang and Shao, Ling},
  booktitle={AAAI},
  year={2022}
}

@inproceedings{shin2023anomaly,
  title={Anomaly Detection using Score-based Perturbation Resilience},
  author={Shin, Woosang and Lee, Jonghyeon and Lee, Taehan and Lee, Sangmoon and Yun, Jong Pil},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{perrett2023use,
  title={Use your head: Improving long-tail video recognition},
  author={Perrett, Toby and Sinha, Saptarshi and Burghardt, Tilo and Mirmehdi, Majid and Damen, Dima},
  booktitle={CVPR},
  year={2023}
}

@article{bleeker2024demonstrating,
  title={Demonstrating and Reducing Shortcuts in Vision-Language Representation Learning},
  author={Bleeker, Maurits and Hendriksen, Mariya and Yates, Andrew and de Rijke, Maarten},
  journal={TMLR},
  year={2024}
}

@article{robinson2021can,
  title={Can contrastive learning avoid shortcut solutions?},
  author={Robinson, Joshua and Sun, Li and Yu, Ke and Batmanghelich, Kayhan and Jegelka, Stefanie and Sra, Suvrit},
  journal={NeurIPs},
  year={2021}
}

@inproceedings{li2023addressing,
  title={Addressing feature suppression in unsupervised visual representations},
  author={Li, Tianhong and Fan, Lijie and Yuan, Yuan and He, Hao and Tian, Yonglong and Feris, Rogerio and Indyk, Piotr and Katabi, Dina},
  booktitle={WACV},
  year={2023}
}

@inproceedings{adnan2022monitoring,
  title={Monitoring shortcut learning using mutual information},
  author={Adnan, Mohammed and Ioannou, Yani and Tsai, Chuan-Yung and Galloway, Angus and Tizhoosh, Hamid R and Taylor, Graham W},
  booktitle={ICMLw},
  year={2022}
}

@inproceedings{bansal2023rethinking,
  title={Rethinking the role of scale for in-context learning: An interpretability-based case study at 66 billion scale},
  author={Bansal, Hritik and Gopalakrishnan, Karthik and Dingliwal, Saket and Bodapati, Sravan and Kirchhoff, Katrin and Roth, Dan},
  booktitle={ACL},
  year={2023}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={NeurIPS},
  year={2022}
}


@article{hoffmann2022empirical,
  title={An empirical analysis of compute-optimal large language model training},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={NeurIPS},
  year={2022}
}

@inproceedings{chen2024understanding,
  title={Understanding and Improving In-Context Learning on Vision-language Models},
  author={Chen, Shuo and Han, Zhen and He, Bailan and Buckley, Mark and Torr, Philip and Tresp, Volker and Gu, Jindong},
  booktitle={ICLRw},
  year={2024}
}

@inproceedings{baldassini2024makes,
  title={What Makes Multimodal In-Context Learning Work?},
  author={Baldassini, Folco Bertini and Shukor, Mustafa and Cord, Matthieu and Soulier, Laure and Piwowarski, Benjamin},
  booktitle={CVPRw},
  year={2024}
}

@article{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv:2211.09110},
  year={2022}
}

@article{ramesh2022hierarchical,
  title={Hierarchical text-conditional image generation with clip latents},
  author={Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  journal={arXiv:2204.06125},
  year={2022}
}

@inproceedings{sung2022vl,
  title={Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks},
  author={Sung, Yi-Lin and Cho, Jaemin and Bansal, Mohit},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{upadhyay2023probvlm,
  title={Probvlm: Probabilistic adapter for frozen vison-language models},
  author={Upadhyay, Uddeshya and Karthik, Shyamgopal and Mancini, Massimiliano and Akata, Zeynep},
  booktitle={ICCV},
  year={2023}
}

@article{zhang2021tip,
  title={Tip-adapter: Training-free clip-adapter for better vision-language modeling},
  author={Zhang, Renrui and Fang, Rongyao and Zhang, Wei and Gao, Peng and Li, Kunchang and Dai, Jifeng and Qiao, Yu and Li, Hongsheng},
  journal={arXiv:2111.03930},
  year={2021}
}

@article{zhang2024llama,
  title={Llama-adapter: Efficient fine-tuning of language models with zero-init attention},
  author={Zhang, Renrui and Han, Jiaming and Liu, Chris and Gao, Peng and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Qiao, Yu},
  journal={ICLR},
  year={2024}
}

@article{wang2022image,
  title={Image as a foreign language: Beit pretraining for all vision and vision-language tasks},
  author={Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and others},
  journal={arXiv:2208.10442},
  year={2022}
}

@inproceedings{yu2024boosting,
  title={Boosting continual learning of vision-language models via mixture-of-experts adapters},
  author={Yu, Jiazuo and Zhuge, Yunzhi and Zhang, Lu and Hu, Ping and Wang, Dong and Lu, Huchuan and He, You},
  booktitle={CVPR},
  year={2024}
}

@article{bao2022vlmo,
  title={Vlmo: Unified vision-language pre-training with mixture-of-modality-experts},
  author={Bao, Hangbo and Wang, Wenhui and Dong, Li and Liu, Qiang and Mohammed, Owais Khan and Aggarwal, Kriti and Som, Subhojit and Piao, Songhao and Wei, Furu},
  journal={NeurIPS},
  year={2022}
}

@article{lin2024moe,
  title={Moe-llava: Mixture of experts for large vision-language models},
  author={Lin, Bin and Tang, Zhenyu and Ye, Yang and Cui, Jiaxi and Zhu, Bin and Jin, Peng and Zhang, Junwu and Ning, Munan and Yuan, Li},
  journal={arXiv:2401.15947},
  year={2024}
}

@article{jacobs1991adaptive,
  title={Adaptive mixtures of local experts},
  author={Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E},
  journal={Neural computation},
  year={1991},
}

@article{bengio2013estimating,
  title={Estimating or propagating gradients through stochastic neurons for conditional computation},
  author={Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
  journal={arXiv:1308.3432},
  year={2013}
}

@inproceedings{mistretta2024improving,
  title={Improving Zero-shot Generalization of Learned Prompts via Unsupervised Knowledge Distillation},
  author={Mistretta, Marco and Baldrati, Alberto and Bertini, Marco and Bagdanov, Andrew D},
  booktitle={ECCV},
  year={2024}
}

@inproceedings{liu2024mmbench,
  title={Mmbench: Is your multi-modal model an all-around player?},
  author={Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
  booktitle={ECCV},
  year={2024}
}

@article{flavell1999cognitive,
  title={Cognitive development: Children's knowledge about the mind},
  author={Flavell, John H},
  journal={Annual review of psychology},
  year={1999}
}

@article{flavell1998social,
  title={Social cognition.},
  author={Flavell, John H and Miller, Patricia H},
  journal={Handbook of Child Psychology},
  year={1998}
}

@article{woodward2009infants,
  title={Infants' grasp of others' intentions},
  author={Woodward, Amanda L},
  journal={Current directions in psychological science},
  year={2009}
}

@inproceedings{li2025llama,
  title={Llama-vid: An image is worth 2 tokens in large language models},
  author={Li, Yanwei and Wang, Chengyao and Jia, Jiaya},
  booktitle={European Conference on Computer Vision},
  year={2024},
}

@article{li2024mini,
  title={Mini-gemini: Mining the potential of multi-modality vision language models},
  author={Li, Yanwei and Zhang, Yuechen and Wang, Chengyao and Zhong, Zhisheng and Chen, Yixin and Chu, Ruihang and Liu, Shaoteng and Jia, Jiaya},
  journal={arXiv:2403.18814},
  year={2024}
}

@article{wu2024longvideobench,
  title={Longvideobench: A benchmark for long-context interleaved video-language understanding},
  author={Wu, Haoning and Li, Dongxu and Chen, Bei and Li, Junnan},
  journal={NeurIPS},
  year={2024}
}

@article{al2024unibench,
  title={Unibench: Visual reasoning requires rethinking vision-language beyond scaling},
  author={Al-Tahan, Haider and Garrido, Quentin and Balestriero, Randall and Bouchacourt, Diane and Hazirbas, Caner and Ibrahim, Mark},
  journal={arXiv:2408.04810},
  year={2024}
}

@article{yang2022learning,
  title={Learning to answer visual questions from web videos},
  author={Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
  journal={IEEE TPAMI},
  year={2022}
}

@inproceedings{yang2022tubedetr,
  title={Tubedetr: Spatio-temporal video grounding with transformers},
  author={Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{sinha2024every,
  title={Every Shot Counts: Using Exemplars for Repetition Counting in Videos},
  author={Sinha, Saptarshi and Stergiou, Alexandros and Damen, Dima},
  booktitle={ACCV},
  year={2024}
}



@article{lee2016making,
  title={Making stochastic neural networks from deterministic ones},
  author={Lee, Kimin and Kim, Jaehyung and Chong, Song and Shin, Jinwoo},
  year={2016}
}

@inproceedings{sermanet2017unsupervised,
  title={Unsupervised perceptual rewards for imitation learning},
  author={Sermanet, Pierre and Xu, Kelvin and Levine, Sergey},
  booktitle={ICLRw},
  year={2017}
}

@inproceedings{bansal2022my,
  title={My view is the best view: Procedure learning from egocentric videos},
  author={Bansal, Siddhant and Arora, Chetan and Jawahar, CV},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{chaabane2020looking,
  title={Looking ahead: Anticipating pedestrians crossing with future frames prediction},
  author={Chaabane, Mohamed and Trabelsi, Ameni and Blanchard, Nathaniel and Beveridge, Ross},
  booktitle={WACV},
  year={2020}
}

@inproceedings{jin2020exploring,
  title={Exploring spatial-temporal multi-frequency analysis for high-fidelity and temporal-consistency video prediction},
  author={Jin, Beibei and Hu, Yu and Tang, Qiankun and Niu, Jingyu and Shi, Zhiping and Han, Yinhe and Li, Xiaowei},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{wang2020probabilistic,
  title={Probabilistic video prediction from noisy data with a posterior confidence},
  author={Wang, Yunbo and Wu, Jiajun and Long, Mingsheng and Tenenbaum, Joshua B},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{chang2021mau,
  title={Mau: A motion-aware unit for video prediction and beyond},
  author={Chang, Zheng and Zhang, Xinfeng and Wang, Shanshe and Ma, Siwei and Ye, Yan and Xinguang, Xiang and Gao, Wen},
  booktitle={NeurIPS},
  year={2021}
}

@article{su2020convolutional,
  title={Convolutional tensor-train LSTM for spatio-temporal learning},
  author={Su, Jiahao and Byeon, Wonmin and Kossaifi, Jean and Huang, Furong and Kautz, Jan and Anandkumar, Anima},
  journal={NeurIPS},
  year={2020}
}

@inproceedings{wang2018predrnn++,
  title={Predrnn++: Towards a resolution of the deep-in-time dilemma in spatiotemporal predictive learning},
  author={Wang, Yunbo and Gao, Zhifeng and Long, Mingsheng and Wang, Jianmin and Philip, S Yu},
  booktitle={ICML},
  year={2018},
}

@inproceedings{wang2023learning,
  title={Learning from semantic alignment between unpaired multiviews for egocentric video recognition},
  author={Wang, Qitong and Zhao, Long and Yuan, Liangzhe and Liu, Ting and Peng, Xi},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{souvcek2022look,
  title={Look for the change: Learning object states and state-modifying actions from untrimmed web videos},
  author={Sou{\v{c}}ek, Tom{\'a}{\v{s}} and Alayrac, Jean-Baptiste and Miech, Antoine and Laptev, Ivan and Sivic, Josef},
  booktitle={CVPR},
  year={2022}
}


@inproceedings{purushwalkam2019task,
  title={Task-driven modular networks for zero-shot compositional learning},
  author={Purushwalkam, Senthil and Nickel, Maximilian and Gupta, Abhinav and Ranzato, Marc'Aurelio},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{doughty2018s,
  title={Who's better? who's best? pairwise deep ranking for skill determination},
  author={Doughty, Hazel and Damen, Dima and Mayol-Cuevas, Walterio},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{doughty2020action,
  title={Action modifiers: Learning from adverbs in instructional videos},
  author={Doughty, Hazel and Laptev, Ivan and Mayol-Cuevas, Walterio and Damen, Dima},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{moltisanti2023learning,
  title={Learning action changes by measuring verb-adverb textual relationships},
  author={Moltisanti, Davide and Keller, Frank and Bilen, Hakan and Sevilla-Lara, Laura},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{martin2019drive,
  title={Drive\&act: A multi-modal dataset for fine-grained driver behavior recognition in autonomous vehicles},
  author={Martin, Manuel and Roitberg, Alina and Haurilet, Monica and Horne, Matthias and Rei{\ss}, Simon and Voit, Michael and Stiefelhagen, Rainer},
  booktitle={ICCV},
  year={2019}
}

@article{becattini2020done,
  title={Am I done? Predicting action progress in videos},
  author={Becattini, Federico and Uricchio, Tiberio and Seidenari, Lorenzo and Ballan, Lamberto and Bimbo, Alberto Del},
  journal={TOMM},
  year={2020},
}

@inproceedings{price2022unweavenet,
  title={Unweavenet: Unweaving activity stories},
  author={Price, Will and Vondrick, Carl and Damen, Dima},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{tang2020uncertainty,
  title={Uncertainty-aware score distribution learning for action quality assessment},
  author={Tang, Yansong and Ni, Zanlin and Zhou, Jiahuan and Zhang, Danyang and Lu, Jiwen and Wu, Ying and Zhou, Jie},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{parmar2019and,
  title={What and how well you performed? a multitask learning approach to action quality assessment},
  author={Parmar, Paritosh and Morris, Brendan Tran},
  booktitle={CVPR},
  year={2019}
}


@inproceedings{damen2024genhowto,
  title={GenHowTo: Learning to Generate Actions and State Transformations from Instructional Videos},
  author={Sou{\v{c}}ek, Tom{\'a}{\v{s}} and Damen, Dima and Wray, Michael and Laptev, Ivan and Sivic, Josef and others},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{ragusa2021meccano,
  title={The meccano dataset: Understanding human-object interactions from egocentric videos in an industrial-like domain},
  author={Ragusa, Francesco and Furnari, Antonino and Livatino, Salvatore and Farinella, Giovanni Maria},
  booktitle={WACV},
  year={2021}
}

@inproceedings{fathi2013modeling,
  title={Modeling actions through state changes},
  author={Fathi, Alireza and Rehg, James M},
  booktitle={CVPR},
  year={2013}
}

@inproceedings{damen2014you,
  title={You-Do, I-Learn: Discovering Task Relevant Objects and their Modes of Interaction from Multi-User Egocentric Video.},
  author={Damen, Dima and Leelasawassuk, Teesid and Haines, Osian and Calway, Andrew and Mayol-Cuevas, Walterio W},
  booktitle={BMVC},
  year={2014}
}

@inproceedings{donahue2024learning,
  title={Learning to predict activity progress by self-supervised video alignment},
  author={Donahue, Gerard and Elhamifar, Ehsan},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{shen2024progress,
  title={Progress-aware online action segmentation for egocentric procedural task videos},
  author={Shen, Yuhan and Elhamifar, Ehsan},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{haresh2021learning,
  title={Learning by aligning videos in time},
  author={Haresh, Sanjay and Kumar, Sateesh and Coskun, Huseyin and Syed, Shahram N and Konin, Andrey and Zia, Zeeshan and Tran, Quoc-Huy},
  booktitle={CVPR},
  year={2021}
}

@article{dvornik2021drop,
  title={Drop-dtw: Aligning common signal between sequences while dropping outliers},
  author={Dvornik, Mikita and Hadji, Isma and Derpanis, Konstantinos G and Garg, Animesh and Jepson, Allan},
  journal={NeurIPS},
  year={2021}
}

@inproceedings{liu2022learning,
  title={Learning to align sequential actions in the wild},
  author={Liu, Weizhe and Tekin, Bugra and Coskun, Huseyin and Vineet, Vibhav and Fua, Pascal and Pollefeys, Marc},
  booktitle={CVPR},
  year={2022}
}

@article{xiong2017pursuit,
  title={A pursuit of temporal accuracy in general activity detection},
  author={Xiong, Yuanjun and Zhao, Yue and Wang, Limin and Lin, Dahua and Tang, Xiaoou},
  journal={arXiv:1703.02716},
  year={2017}
}

@inproceedings{kim2021hotr,
  title={Hotr: End-to-end human-object interaction detection with transformers},
  author={Kim, Bumsoo and Lee, Junhyun and Kang, Jaewoo and Kim, Eun-Sol and Kim, Hyunwoo J},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{ortega2020dmd,
  title={Dmd: A large-scale multi-modal driver monitoring dataset for attention and alertness analysis},
  author={Ortega, Juan Diego and Kose, Neslihan and Ca{\~n}as, Paola and Chao, Min-An and Unnervik, Alexander and Nieto, Marcos and Otaegui, Oihana and Salgado, Luis},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{heidarivincheh2016beyond,
  title={Beyond action recognition: Action completion in rgb-d data},
  author={Heidarivincheh, Farnoosh and Mirmehdi, Majid and Damen, Dima},
  booktitle={BMVC},
  year={2016}
}

@inproceedings{heidarivincheh2018action,
  title={Action completion: A temporal model for moment detection},
  author={Heidarivincheh, Farnoosh and Mirmehdi, Majid and Damen, Dima},
  booktitle={BMVC},
  year={2018}
}

@inproceedings{epstein2020oops,
  title={Oops! predicting unintentional action in video},
  author={Epstein, Dave and Chen, Boyuan and Vondrick, Carl},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{sener2022assembly101,
  title={Assembly101: A large-scale multi-view video dataset for understanding procedural activities},
  author={Sener, Fadime and Chatterjee, Dibyadip and Shelepov, Daniel and He, Kun and Singhania, Dipika and Wang, Robert and Yao, Angela},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{xue2024learning,
  title={Learning object state changes in videos: An open-world perspective},
  author={Xue, Zihui and Ashutosh, Kumar and Grauman, Kristen},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{yang2024active,
  title={Active Object Detection with Knowledge Aggregation and Distillation from Large Models},
  author={Yang, Dejie and Liu, Yang},
  booktitle={CVPR},
  year={2024}
}

@article{dunnhofer2023visual,
  title={Visual object tracking in first person vision},
  author={Dunnhofer, Matteo and Furnari, Antonino and Farinella, Giovanni Maria and Micheloni, Christian},
  journal={IJCV},
  year={2023},
}

@inproceedings{fu2021sequential,
  title={Sequential Decision-Making for Active Object Detection from Hand},
  author={Fu, Qichen and Liu, Xingyu and Kitani, Kris M},
  booktitle={CVPR},
  year={2022}
}
@inproceedings{mittal2024can,
  title={Can't make an Omelette without Breaking some Eggs: Plausible Action Anticipation using Large Video-Language Models},
  author={Mittal, Himangi and Agarwal, Nakul and Lo, Shao-Yuan and Lee, Kwonjoon},
  booktitle={CVPR},
  year={2024}
}

@article{gouidis2023leveraging,
  title={Leveraging knowledge graphs for zero-shot object-agnostic state classification},
  author={Gouidis, Filipos and Patkos, Theodore and Argyros, Antonis and Plexousakis, Dimitris},
  journal={arXiv:2307.12179},
  year={2023}
}

@inproceedings{misra2017red,
  title={From red wine to red tomato: Composition with context},
  author={Misra, Ishan and Gupta, Abhinav and Hebert, Martial},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{nagarajan2018attributes,
  title={Attributes as operators: factorizing unseen attribute-object compositions},
  author={Nagarajan, Tushar and Grauman, Kristen},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{saini2023chop,
  title={Chop \& learn: Recognizing and generating object-state compositions},
  author={Saini, Nirat and Wang, Hanyu and Swaminathan, Archana and Jayasundara, Vinoj and He, Bo and Gupta, Kamal and Shrivastava, Abhinav},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{yu2023video,
  title={Video state-changing object segmentation},
  author={Yu, Jiangwei and Li, Xiang and Zhao, Xinran and Zhang, Hongming and Wang, Yu-Xiong},
  booktitle={ICCV},
  year={2023}
}

@article{alayrac2024multi,
  title={Multi-Task Learning of Object States and State-Modifying Actions from Web Videos},
  author={Alayrac, Jean-Baptiste and Miech, Antoine and Laptev, Ivan and Sivic, Josef and others},
  journal={IEEE TPAMI},
  year={2024},
}

@article{mangalam2023egoschema,
  title={Egoschema: A diagnostic benchmark for very long-form video language understanding},
  author={Mangalam, Karttikeya and Akshulakov, Raiymbek and Malik, Jitendra},
  journal={NeurIPS},
  year={2023}
}

@article{vaina1991object,
  title={Object structure and action requirements: A compatibility model for functional recognition},
  author={Vaina, Lucia M and Jaulent, Marie-Christine},
  journal={IJIS},
  year={1991},
}

@inproceedings{chen2020rethinking,
  title={Rethinking the bottom-up framework for query-based video localization},
  author={Chen, Long and Lu, Chujie and Tang, Siliang and Xiao, Jun and Zhang, Dong and Tan, Chilie and Li, Xiaolin},
  booktitle={AAAI},
  year={2020}
}

@article{hao2022query,
  title={Query-aware video encoder for video moment retrieval},
  author={Hao, Jiachang and Sun, Haifeng and Ren, Pengfei and Wang, Jingyu and Qi, Qi and Liao, Jianxin},
  journal={Neurocomputing},
  year={2022}
}

@inproceedings{liu2022memory,
  title={Memory-guided semantic learning network for temporal sentence grounding},
  author={Liu, Daizong and Qu, Xiaoye and Di, Xing and Cheng, Yu and Xu, Zichuan and Zhou, Pan},
  booktitle={AAAI},
  year={2022}
}

@inproceedings{nan2021interventional,
  title={Interventional video grounding with dual contrastive learning},
  author={Nan, Guoshun and Qiao, Rui and Xiao, Yao and Liu, Jun and Leng, Sicong and Zhang, Hao and Lu, Wei},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{yuan2019find,
  title={To find where you talk: Temporal sentence localization in video with attention based location regression},
  author={Yuan, Yitian and Mei, Tao and Zhu, Wenwu},
  booktitle={AAAI},
  year={2019}
}

@article{zhang2021natural,
  title={Natural language video localization: A revisit in span-based question answering framework},
  author={Zhang, Hao and Sun, Aixin and Jing, Wei and Zhen, Liangli and Zhou, Joey Tianyi and Goh, Rick Siow Mong},
  journal={IEEE TPAMI},
  year={2021}
}

@inproceedings{rohrbach2016grounding,
  title={Grounding of textual phrases in images by reconstruction},
  author={Rohrbach, Anna and Rohrbach, Marcus and Hu, Ronghang and Darrell, Trevor and Schiele, Bernt},
  booktitle={ECCV},
  year={2016}
}

@inproceedings{zhang2023adding,
  title={Adding conditional control to text-to-image diffusion models},
  author={Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{goroshin2015unsupervised,
  title={Unsupervised learning of spatiotemporally coherent metrics},
  author={Goroshin, Ross and Bruna, Joan and Tompson, Jonathan and Eigen, David and LeCun, Yann},
  booktitle={ICCV},
  year={2015}
}

@inproceedings{chen2018temporally,
  title={Temporally grounding natural sentence in video},
  author={Chen, Jingyuan and Chen, Xinpeng and Ma, Lin and Jie, Zequn and Chua, Tat-Seng},
  booktitle={EMNLP},
  year={2018}
}

@inproceedings{ge2019mac,
  title={Mac: Mining activity concepts for language-based temporal localization},
  author={Ge, Runzhou and Gao, Jiyang and Chen, Kan and Nevatia, Ram},
  booktitle={WACV},
  year={2019},
}

@inproceedings{mun2020local,
  title={Local-global video-text interactions for temporal grounding},
  author={Mun, Jonghwan and Cho, Minsu and Han, Bohyung},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{zhang2019man,
  title={Man: Moment alignment network for natural language moment retrieval via iterative graph adjustment},
  author={Zhang, Da and Dai, Xiyang and Wang, Xin and Wang, Yuan-Fang and Davis, Larry S},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{huang2023weakly,
  title={Weakly supervised temporal sentence grounding with uncertainty-guided self-training},
  author={Huang, Yifei and Yang, Lijin and Sato, Yoichi},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{jiang2019cross,
  title={Cross-modal video moment retrieval with spatial and language-temporal attention},
  author={Jiang, Bin and Huang, Xin and Yang, Chao and Yuan, Junsong},
  booktitle={ICMR},
  year={2019}
}

@inproceedings{liu2021context,
  title={Context-aware biaffine localizing network for temporal sentence grounding},
  author={Liu, Daizong and Qu, Xiaoye and Dong, Jianfeng and Zhou, Pan and Cheng, Yu and Wei, Wei and Xu, Zichuan and Xie, Yulai},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{liu2018cross,
  title={Cross-modal moment localization in videos},
  author={Liu, Meng and Wang, Xiang and Nie, Liqiang and Tian, Qi and Chen, Baoquan and Chua, Tat-Seng},
  booktitle={MM},
  year={2018}
}

@inproceedings{qu2020fine,
  title={Fine-grained iterative attention network for temporal language localization in videos},
  author={Qu, Xiaoye and Tang, Pengwei and Zou, Zhikang and Cheng, Yu and Dong, Jianfeng and Zhou, Pan and Xu, Zichuan},
  booktitle={MM},
  year={2020}
}

@inproceedings{wang2020temporally,
  title={Temporally grounding language queries in videos by contextual boundary-aware prediction},
  author={Wang, Jingwen and Ma, Lin and Jiang, Wenhao},
  booktitle={AAAI},
  year={2020}
}

@inproceedings{xu2019multilevel,
  title={Multilevel language and vision integration for text-to-clip retrieval},
  author={Xu, Huijuan and He, Kun and Plummer, Bryan A and Sigal, Leonid and Sclaroff, Stan and Saenko, Kate},
  booktitle={AAAI},
  year={2019}
}

@inproceedings{fernando2017self,
  title={Self-supervised video representation learning with odd-one-out networks},
  author={Fernando, Basura and Bilen, Hakan and Gavves, Efstratios and Gould, Stephen},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{andrew2013deep,
  title={Deep canonical correlation analysis},
  author={Andrew, Galen and Arora, Raman and Bilmes, Jeff and Livescu, Karen},
  booktitle={ICML},
  year={2013},
}

@article{sakoe1978dynamic,
  title={Dynamic programming algorithm optimization for spoken word recognition},
  author={Sakoe, Hiroaki and Chiba, Seibi},
  journal={IEEE TASSP},
  year={1978},
}

@inproceedings{chang2019d3tw,
  title={D3tw: Discriminative differentiable dynamic time warping for weakly supervised action alignment and segmentation},
  author={Chang, Chien-Yi and Huang, De-An and Sui, Yanan and Fei-Fei, Li and Niebles, Juan Carlos},
  booktitle={CVPR},
  year={2019}
}

@article{liu2023video,
  title={Video timeline modeling for news story understanding},
  author={Liu, Meng and Zhang, Mingda and Liu, Jialu and Dai, Hanjun and Yang, Ming-Hsuan and Ji, Shuiwang and Feng, Zheyun and Gong, Boqing},
  journal={NeurIPS},
  year={2023}
}

@inproceedings{park2022exposing,
  title={Exposing the limits of video-text models through contrast sets},
  author={Park, Jae Sung and Shen, Sheng and Farhadi, Ali and Darrell, Trevor and Choi, Yejin and Rohrbach, Anna},
  booktitle={NAACL},
  year={2022}
}

@inproceedings{hakeem2004ontology,
  title={Ontology and taxonomy collaborated framework for meeting classification},
  author={Hakeem, Asaad and Shah, Mubarak},
  booktitle={ICPR},
  year={2004},
}

@article{albanese2010pads,
  title={Pads: A probabilistic activity detection framework for video data},
  author={Albanese, Massimiliano and Chellappa, Rama and Cuntoor, Naresh and Moscato, Vincenzo and Picariello, Antonio and Subrahmanian, VS and Udrea, Octavian},
  journal={IEEE TPAMI},
  year={2010},
}

@article{zhu2023personality,
  title={Personality-aware Human-centric Multimodal Reasoning: A New Task, Dataset and Baselines},
  author={Zhu, Yaochen and Shen, Xiangqing and Xia, Rui},
  journal={arXiv:2304.02313},
  year={2023}
}

@inproceedings{cuturi2017soft,
  title={Soft-dtw: a differentiable loss function for time-series},
  author={Cuturi, Marco and Blondel, Mathieu},
  booktitle={ICML},
  year={2017},
}

@inproceedings{song2024moviechat,
  title={Moviechat: From dense token to sparse memory for long video understanding},
  author={Song, Enxin and Chai, Wenhao and Wang, Guanhong and Zhang, Yucheng and Zhou, Haoyang and Wu, Feiyang and Chi, Haozhe and Guo, Xun and Ye, Tian and Zhang, Yanting and others},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{monfort2021spoken,
  title={Spoken moments: Learning joint audio-visual representations from video descriptions},
  author={Monfort, Mathew and Jin, SouYoung and Liu, Alexander and Harwath, David and Feris, Rogerio and Glass, James and Oliva, Aude},
  booktitle={CVPR},
  year={2021}
}


%% ----- EAP ------

@inproceedings{cao2013recognize,
  title={Recognize human activities from partially observed videos},
  author={Cao, Yu and Barrett, Daniel and Barbu, Andrei and Narayanaswamy, Siddharth and Yu, Haonan and Michaux, Aaron and Lin, Yuewei and Dickinson, Sven and Mark Siskind, Jeffrey and Wang, Song},
  booktitle={CVPR},
  year={2013}
}


@inproceedings{lan2014hierarchical,
  title={A hierarchical representation for future action prediction},
  author={Lan, Tian and Chen, Tsung-Chuan and Savarese, Silvio},
  booktitle={ECCV},
  year={2014}
}

@article{li2014prediction,
  title={Prediction of human activity by discovering temporal sequence patterns},
  author={Li, Kang and Fu, Yun},
  journal={IEEE TPAMI},
  year={2014}
}

@inproceedings{li2012modeling,
  title={Modeling complex temporal composition of actionlets for activity prediction},
  author={Li, Kang and Hu, Jie and Fu, Yun},
  booktitle={ECCV},
  year={2012}
}

@inproceedings{xu2015activity,
  title={Activity auto-completion: Predicting human activities from partial videos},
  author={Xu, Zhen and Qing, Laiyun and Miao, Jun},
  booktitle={CVPR},
  year={2015}
}

@inproceedings{zhou2015temporal,
  title={Temporal perception and prediction in ego-centric video},
  author={Zhou, Yipin and Berg, Tamara L},
  booktitle={ICCV},
  year={2015}
}

@inproceedings{ryoo2011human,
  title={Human activity prediction: Early recognition of ongoing activities from streaming videos},
  author={Ryoo, Michael S},
  booktitle={ICCV},
  year={2011}
}

@article{hoai2014max,
  title={Max-margin early event detectors},
  author={Hoai, Minh and De la Torre, Fernando},
  journal={IJCV},
  year={2014}
}


@inproceedings{kong2014discriminative,
  title={A discriminative model with multiple temporal scales for action prediction},
  author={Kong, Yu and Kit, Dmitry and Fu, Yun},
  booktitle={ECCV},
  year={2014}
}


@inproceedings{pickup2014seeing,
  title={Seeing the arrow of time},
  author={Pickup, Lyndsey C and Pan, Zheng and Wei, Donglai and Shih, YiChang and Zhang, Changshui and Zisserman, Andrew and Scholkopf, Bernhard and Freeman, William T},
  booktitle={CVPR},
  year={2014}
}

@inproceedings{misra2016shuffle,
  title={Shuffle and learn: unsupervised learning using temporal order verification},
  author={Misra, Ishan and Zitnick, C Lawrence and Hebert, Martial},
  booktitle={ECCV},
  year={2016}
}



@inproceedings{hwang2019adversarial,
  title={Adversarial structure matching for structured prediction tasks},
  author={Hwang, Jyh-Jing and Ke, Tsung-Wei and Shi, Jianbo and Yu, Stella X},
  booktitle={CVPR},
  year={2019}
}


@inproceedings{chang2022strpm,
  title={Strpm: A spatiotemporal residual predictive model for high-resolution video prediction},
  author={Chang, Zheng and Zhang, Xinfeng and Wang, Shanshe and Ma, Siwei and Gao, Wen},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{park2021vid,
  title={Vid-ode: Continuous-time video generation with neural ordinary differential equation},
  author={Park, Sunghyun and Kim, Kangyeol and Lee, Junsoo and Choo, Jaegul and Lee, Joonseok and Kim, Sookyung and Choi, Edward},
  booktitle={AAAI},
  year={2021}
}

@inproceedings{gao2022simvp,
  title={Simvp: Simpler yet better video prediction},
  author={Gao, Zhangyang and Tan, Cheng and Wu, Lirong and Li, Stan Z},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{zhong2023mmvp,
  title={Mmvp: Motion-matrix-based video prediction},
  author={Zhong, Yiqi and Liang, Luming and Zharkov, Ilya and Neumann, Ulrich},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{tang2024vmrnn,
  title={Vmrnn: Integrating vision mamba and lstm for efficient and accurate spatiotemporal forecasting},
  author={Tang, Yujin and Dong, Peijie and Tang, Zhenheng and Chu, Xiaowen and Liang, Junwei},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{gu2023seer,
  title={Seer: Language instructed video prediction with latent diffusion models},
  author={Gu, Xianfan and Wen, Chuan and Ye, Weirui and Song, Jiaming and Gao, Yang},
  booktitle={ICLR},
  year={2024}
}

@inproceedings{ning2023mimo,
  title={MIMO is all you need: A strong multi-in-multi-out baseline for video prediction},
  author={Ning, Shuliang and Lan, Mengcheng and Li, Yanran and Chen, Chaofeng and Chen, Qian and Chen, Xunlai and Han, Xiaoguang and Cui, Shuguang},
  booktitle={AAAI},
  year={2023}
}

@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={NeurIPS},
  year={2020}
}

@article{dhariwal2021diffusion,
  title={Diffusion models beat gans on image synthesis},
  author={Dhariwal, Prafulla and Nichol, Alexander},
  journal={NeurIPS},
  year={2021}
}

@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{lipman2022flow,
  title={Flow matching for generative modeling},
  author={Lipman, Yaron and Chen, Ricky TQ and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt},
  booktitle={ICLR},
  year={2023}
}

@inproceedings{liu2017video,
  title={Video frame synthesis using deep voxel flow},
  author={Liu, Ziwei and Yeh, Raymond A and Tang, Xiaoou and Liu, Yiming and Agarwala, Aseem},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{tan2023temporal,
  title={Temporal attention unit: Towards efficient spatiotemporal predictive learning},
  author={Tan, Cheng and Gao, Zhangyang and Wu, Lirong and Xu, Yongjie and Xia, Jun and Li, Siyuan and Li, Stan Z},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{hu2023dynamic,
  title={A dynamic multi-scale voxel flow network for video prediction},
  author={Hu, Xiaotao and Huang, Zhewei and Huang, Ailin and Xu, Jun and Zhou, Shuchang},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{nie2024triplet,
  title={Triplet attention transformer for spatiotemporal predictive learning},
  author={Nie, Xuesong and Chen, Xi and Jin, Haoyuan and Zhu, Zhihang and Yan, Yunfeng and Qi, Donglian},
  booktitle={WACV},
  year={2024}
}

@inproceedings{davtyan2023efficient,
  title={Efficient video prediction via sparsely conditioned flow matching},
  author={Davtyan, Aram and Sameni, Sepehr and Favaro, Paolo},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{shrivastava2024video,
  title={Video Prediction by Modeling Videos as Continuous Multi-Dimensional Processes},
  author={Shrivastava, Gaurav and Shrivastava, Abhinav},
  booktitle={CVPR},
  year={2024}
}

@article{harvey2022flexible,
  title={Flexible diffusion modeling of long videos},
  author={Harvey, William and Naderiparizi, Saeid and Masrani, Vaden and Weilbach, Christian and Wood, Frank},
  journal={NeurIPS},
  year={2022}
}

@article{liang2022nuwa,
  title={Nuwa-infinity: Autoregressive over autoregressive generation for infinite visual synthesis},
  author={Liang, Jian and Wu, Chenfei and Hu, Xiaowei and Gan, Zhe and Wang, Jianfeng and Wang, Lijuan and Liu, Zicheng and Fang, Yuejian and Duan, Nan},
  journal={NeurIPS},
  year={2022}
}

@inproceedings{long2024videodrafter,
  title={Videodrafter: Content-consistent multi-scene video generation with llm},
  author={Long, Fuchen and Qiu, Zhaofan and Yao, Ting and Mei, Tao},
  booktitle={ECCV},
  year={2024}
}

@inproceedings{guen2020disentangling,
  title={Disentangling physical dynamics from unknown factors for unsupervised video prediction},
  author={Guen, Vincent Le and Thome, Nicolas},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{zhang2023modeling,
  title={Modeling video as stochastic processes for fine-grained video representation learning},
  author={Zhang, Heng and Liu, Daqing and Zheng, Qi and Su, Bing},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{wu2021motionrnn,
  title={MotionRNN: A flexible model for video prediction with spacetime-varying motions},
  author={Wu, Haixu and Yao, Zhiyu and Wang, Jianmin and Long, Mingsheng},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{oshima2024ssm,
  title={Ssm meets video diffusion models: Efficient video generation with structured state spaces},
  author={Oshima, Yuta and Taniguchi, Shohei and Suzuki, Masahiro and Matsuo, Yutaka},
  booktitle={ICLRw},
  year={2024}
}

@article{yin2023nuwa,
  title={Nuwa-xl: Diffusion over diffusion for extremely long video generation},
  author={Yin, Shengming and Wu, Chenfei and Yang, Huan and Wang, Jianfeng and Wang, Xiaodong and Ni, Minheng and Yang, Zhengyuan and Li, Linjie and Liu, Shuguang and Yang, Fan and others},
  journal={arXiv:2303.12346},
  year={2023}
}

@article{du2023stable,
  title={Stable diffusion is unstable},
  author={Du, Chengbin and Li, Yanxi and Qiu, Zhongwei and Xu, Chang},
  journal={Advances in Neural Information Processing Systems},
  year={2023}
}

@inproceedings{ye2022vptr,
  title={Vptr: Efficient transformers for video prediction},
  author={Ye, Xi and Bilodeau, Guillaume-Alexandre},
  booktitle={ICPR},
  year={2022},
}

@inproceedings{ge2022long,
  title={Long video generation with time-agnostic vqgan and time-sensitive transformer},
  author={Ge, Songwei and Hayes, Thomas and Yang, Harry and Yin, Xi and Pang, Guan and Jacobs, David and Huang, Jia-Bin and Parikh, Devi},
  booktitle={ECCV},
  year={2022},
}

@article{clark2019adversarial,
  title={Adversarial video generation on complex datasets},
  author={Clark, Aidan and Donahue, Jeff and Simonyan, Karen},
  journal={arXiv:1907.06571},
  year={2019}
}

@article{luc2020transformation,
  title={Transformation-based adversarial video prediction on large-scale data},
  author={Luc, Pauline and Clark, Aidan and Dieleman, Sander and Casas, Diego de Las and Doron, Yotam and Cassirer, Albin and Simonyan, Karen},
  journal={arXiv:2003.04035},
  year={2020}
}

@article{vondrick2016generating,
  title={Generating videos with scene dynamics},
  author={Vondrick, Carl and Pirsiavash, Hamed and Torralba, Antonio},
  journal={NeurIPS},
  year={2016}
}

@inproceedings{yu2022generating,
  title={Generating Videos with Dynamics-aware Implicit Generative Adversarial Networks},
  author={Yu, Sihyun and Tack, Jihoon and Mo, Sangwoo and Kim, Hyunsu and Kim, Junho and Ha, Jung-Woo and Shin, Jinwoo},
  booktitle={ICLR},
  year={2022}
}

@inproceedings{denton2018stochastic,
  title={Stochastic video generation with a learned prior},
  author={Denton, Emily and Fergus, Rob},
  booktitle={ICML},
  year={2018},
}

@article{smith2024convolutional,
  title={Convolutional state space models for long-range spatiotemporal modeling},
  author={Smith, Jimmy and De Mello, Shalini and Kautz, Jan and Linderman, Scott and Byeon, Wonmin},
  journal={NeurIPS},
  year={2024}
}

@inproceedings{chi2023adamsformer,
  title={Adamsformer for spatial action localization in the future},
  author={Chi, Hyung-gun and Lee, Kwonjoon and Agarwal, Nakul and Xu, Yi and Ramani, Karthik and Choi, Chiho},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{agarwal2020unsupervised,
  title={Unsupervised Domain Adaptation for Spatio-Temporal Action Localization},
  author={Agarwal, Nakul and Chen, Yi Ting and Dariush, Behzad and Yang, Ming Hsuan},
  booktitle={BMVC},
  year={2020}
}

@inproceedings{peng2016multi,
  title={Multi-region two-stream R-CNN for action detection},
  author={Peng, Xiaojiang and Schmid, Cordelia},
  booktitle={ECCV},
  year={2016},
}

@inproceedings{singh2017online,
  title={Online real-time multiple spatiotemporal action localisation and prediction},
  author={Singh, Gurkirt and Saha, Suman and Sapienza, Michael and Torr, Philip HS and Cuzzolin, Fabio},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{weinzaepfel2015learning,
  title={Learning to track for spatio-temporal action localization},
  author={Weinzaepfel, Philippe and Harchaoui, Zaid and Schmid, Cordelia},
  booktitle={ICCV},
  year={2015}
}

@inproceedings{zhang2019structured,
  title={A structured model for action detection},
  author={Zhang, Yubo and Tokmakov, Pavel and Hebert, Martial and Schmid, Cordelia},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{ulutan2020actor,
  title={Actor conditioned attention maps for video action detection},
  author={Ulutan, Oytun and Rallapalli, Swati and Srivatsa, Mudhakar and Torres, Carlos and Manjunath, BS},
  booktitle={WACV},
  year={2020}
}

@inproceedings{sun2018actor,
  title={Actor-centric relation network},
  author={Sun, Chen and Shrivastava, Abhinav and Vondrick, Carl and Murphy, Kevin and Sukthankar, Rahul and Schmid, Cordelia},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{feng2021relation,
  title={Relation modeling in spatio-temporal action localization},
  author={Feng, Yutong and Jiang, Jianwen and Huang, Ziyuan and Qing, Zhiwu and Wang, Xiang and Zhang, Shiwei and Tang, Mingqian and Gao, Yue},
  booktitle={CVPRw},
  year={2021}
}

@inproceedings{wang2018videos,
  title={Videos as space-time region graphs},
  author={Wang, Xiaolong and Gupta, Abhinav},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{pan2021actor,
  title={Actor-context-actor relation network for spatio-temporal action localization},
  author={Pan, Junting and Chen, Siyu and Shou, Mike Zheng and Liu, Yu and Shao, Jing and Li, Hongsheng},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{tang2020asynchronous,
  title={Asynchronous interaction aggregation for action detection},
  author={Tang, Jiajun and Xia, Jin and Mu, Xinzhi and Pang, Bo and Lu, Cewu},
  booktitle={ECCV},
  year={2020},
}

@inproceedings{li2018recurrent,
  title={Recurrent tubelet proposal and recognition networks for action detection},
  author={Li, Dong and Qiu, Zhaofan and Dai, Qi and Yao, Ting and Mei, Tao},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{li2020actions,
  title={Actions as moving points},
  author={Li, Yixuan and Wang, Zixu and Wang, Limin and Wu, Gangshan},
  booktitle={ECCV},
  year={2020},
}

@inproceedings{zhao2019dance,
  title={Dance with flow: Two-in-one stream action detection},
  author={Zhao, Jiaojiao and Snoek, Cees G. M.},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{song2019tacnet,
  title={Tacnet: Transition-aware context network for spatio-temporal action detection},
  author={Song, Lin and Zhang, Shiwei and Yu, Gang and Sun, Hongbin},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{jain2014action,
  title={Action localization with tubelets from motion},
  author={Jain, Mihir and Van Gemert, Jan and J{\'e}gou, Herv{\'e} and Bouthemy, Patrick and Snoek, Cees G. M.},
  booktitle={CVPR},
  year={2014}
}

@inproceedings{thoker2023tubelet,
    author={Thoker, Fida Mohammad and Doughty, Hazel and Snoek, Cees G. M.},
    title={Tubelet-Contrastive Self-Supervision for Video-Efficient Generalization},
    booktitle={ICCV},
    year={2023},
}

@inproceedings{hou2017tube,
  title={Tube convolutional neural network (t-cnn) for action detection in videos},
  author={Hou, Rui and Chen, Chen and Shah, Mubarak},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{yang2019step,
  title={Step: Spatio-temporal progressive learning for video action detection},
  author={Yang, Xitong and Yang, Xiaodong and Liu, Ming-Yu and Xiao, Fanyi and Davis, Larry S and Kautz, Jan},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{zhao2022tuber,
  title={Tuber: Tubelet transformer for video action detection},
  author={Zhao, Jiaojiao and Zhang, Yanyi and Li, Xinyu and Chen, Hao and Shuai, Bing and Xu, Mingze and Liu, Chunhui and Kundu, Kaustav and Xiong, Yuanjun and Modolo, Davide and others},
  booktitle={CVPR},
  year={2022}
}

@article{gao2017red,
  title={Red: Reinforced encoder-decoder networks for action anticipation},
  author={Gao, Jiyang and Yang, Zhenheng and Nevatia, Ram},
  journal={arXiv:1707.04818},
  year={2017}
}

@inproceedings{huang2014action,
  title={Action-reaction: Forecasting the dynamics of human interaction},
  author={Huang, De-An and Kitani, Kris M},
  booktitle={ECCV},
  year={2014}
}

@inproceedings{kuehne2014language,
  title={The language of actions: Recovering the syntax and semantics of goal-directed human activities},
  author={Kuehne, Hilde and Arslan, Ali and Serre, Thomas},
  booktitle={CVPR},
  year={2014}
}

@inproceedings{blattmann2023align,
  title={Align your latents: High-resolution video synthesis with latent diffusion models},
  author={Blattmann, Andreas and Rombach, Robin and Ling, Huan and Dockhorn, Tim and Kim, Seung Wook and Fidler, Sanja and Kreis, Karsten},
  booktitle={CVPR},
  year={2023}
}

@article{wang2022omnivl,
  title={Omnivl: One foundation model for image-language and video-language tasks},
  author={Wang, Junke and Chen, Dongdong and Wu, Zuxuan and Luo, Chong and Zhou, Luowei and Zhao, Yucheng and Xie, Yujia and Liu, Ce and Jiang, Yu-Gang and Yuan, Lu},
  journal={NeurIPS},
  year={2022}
}

@inproceedings{nikankin2023sinfusion,
  title={SinFusion: training diffusion models on a single image or video},
  author={Nikankin, Yaniv and Haim, Niv and Irani, Michal},
  booktitle={ICML},
  year={2023}
}

@inproceedings{yang2023video,
  title={Video diffusion models with local-global context guidance},
  author={Yang, Siyuan and Zhang, Lu and Liu, Yu and Jiang, Zhizhuo and He, You},
  booktitle={IJCAI},
  year={2023}
}

@article{yang2023basictad,
  title={Basictad: an astounding rgb-only baseline for temporal action detection},
  author={Yang, Min and Chen, Guo and Zheng, Yin-Dong and Lu, Tong and Wang, Limin},
  journal={CVIU},
  year={2023},
}

@inproceedings{bai2020boundary,
  title={Boundary content graph neural network for temporal action proposal generation},
  author={Bai, Yueran and Wang, Yingying and Tong, Yunhai and Yang, Yang and Liu, Qiyue and Liu, Junhui},
  booktitle={ECCV},
  year={2020},
}

@inproceedings{tan2021relaxed,
  title={Relaxed transformer decoders for direct action proposal generation},
  author={Tan, Jing and Tang, Jiaqi and Wang, Limin and Wu, Gangshan},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{liu2022empirical,
  title={An empirical study of end-to-end temporal action detection},
  author={Liu, Xiaolong and Bai, Song and Bai, Xiang},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{cheng2022stochastic,
  title={Stochastic backpropagation: A memory efficient strategy for training video models},
  author={Cheng, Feng and Xu, Mingze and Xiong, Yuanjun and Chen, Hao and Li, Xinyu and Li, Wei and Xia, Wei},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{ho2022video,
  title={Video diffusion models},
  author={Ho, Jonathan and Salimans, Tim and Gritsenko, Alexey and Chan, William and Norouzi, Mohammad and Fleet, David J},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{wang2023manipulate,
  title={Manipulate by seeing: Creating manipulation controllers from pre-trained representations},
  author={Wang, Jianren and Dasari, Sudeep and Srirama, Mohan Kumar and Tulsiani, Shubham and Gupta, Abhinav},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{kununi2024uni,
  title={Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization},
  author={Kun, LEI and He, Zhengmao and Lu, Chenhao and Hu, Kaizhe and Gao, Yang and Xu, Huazhe},
  booktitle={The Twelfth International Conference on Learning Representations},
year={2024}
}

@inproceedings{salimans2017pixelcnn++,
  title={Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications},
  author={Salimans, Tim and Karpathy, Andrej and Chen, Xi and Kingma, Diederik P},
  booktitle={ICLR},
  year={2017}
}

@article{song2019generative,
  title={Generative modeling by estimating gradients of the data distribution},
  author={Song, Yang and Ermon, Stefano},
  journal={NeurIPS},
  volume={32},
  year={2019}
}

@inproceedings{esser2021taming,
  title={Taming transformers for high-resolution image synthesis},
  author={Esser, Patrick and Rombach, Robin and Ommer, Bjorn},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{sohl2015deep,
  title={Deep unsupervised learning using nonequilibrium thermodynamics},
  author={Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle={ICLR},
  year={2015}
}

@article{voleti2022mcvd,
  title={Mcvd-masked conditional video diffusion for prediction, generation, and interpolation},
  author={Voleti, Vikram and Jolicoeur-Martineau, Alexia and Pal, Chris},
  journal={NeurIPS},
  year={2022}
}

@article{hoppe2024diffusion,
  title={Diffusion models for video prediction and infilling},
  author={H{\"o}ppe, Tobias and Mehrjou, Arash and Bauer, Stefan and Nielsen, Didrik and Dittadi, Andrea},
  journal={IEEE TMLR},
  year={2024}
}

@article{he2022latent,
  title={Latent video diffusion models for high-fidelity video generation with arbitrary lengths},
  author={He, Yingqing and Yang, Tianyu and Zhang, Yong and Shan, Ying and Chen, Qifeng},
  journal={arXiv:2211.13221},
  year={2022}
}


@inproceedings{zhang2024mart,
  title={Mart: Masked affective representation learning via masked temporal distribution distillation},
  author={Zhang, Zhicheng and Zhao, Pancheng and Park, Eunil and Yang, Jufeng},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{singer2023make,
  title={Make-a-video: Text-to-video generation without text-video data},
  author={Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and others},
  booktitle={ICLR},
  year={2023}
}

@inproceedings{sermanet2018time,
  title={Time-contrastive networks: Self-supervised learning from video},
  author={Sermanet, Pierre and Lynch, Corey and Chebotar, Yevgen and Hsu, Jasmine and Jang, Eric and Schaal, Stefan and Levine, Sergey and Brain, Google},
  booktitle={ICRA},
  year={2018},
}

@inproceedings{wu2023tune,
  title={Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation},
  author={Wu, Jay Zhangjie and Ge, Yixiao and Wang, Xintao and Lei, Stan Weixian and Gu, Yuchao and Shi, Yufei and Hsu, Wynne and Shan, Ying and Qie, Xiaohu and Shou, Mike Zheng},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{wu2020not,
  title={Not only look, but also listen: Learning multimodal violence detection under weak supervision},
  author={Wu, Peng and Liu, Jing and Shi, Yujia and Sun, Yujia and Shao, Fangtao and Wu, Zhaoyang and Yang, Zhiwei},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{liu2018future,
  title={Future frame prediction for anomaly detection--a new baseline},
  author={Liu, Wen and Luo, Weixin and Lian, Dongze and Gao, Shenghua},
  booktitle={CVPR},
  year={2018}
}


@inproceedings{lu2013abnormal,
  title={Abnormal event detection at 150 fps in matlab},
  author={Lu, Cewu and Shi, Jianping and Jia, Jiaya},
  booktitle={ICCV},
  year={2013}
}

@inproceedings{ghodrati2021frameexit,
  title={Frameexit: Conditional early exiting for efficient video recognition},
  author={Ghodrati, Amir and Bejnordi, Babak Ehteshami and Habibian, Amirhossein},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{radevski2023multimodal,
  title={Multimodal distillation for egocentric action recognition},
  author={Radevski, Gorjan and Grujicic, Dusan and Blaschko, Matthew and Moens, Marie-Francine and Tuytelaars, Tinne},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{yang2023vid2seq,
  title={Vid2seq: Large-scale pretraining of a visual language model for dense video captioning},
  author={Yang, Antoine and Nagrani, Arsha and Seo, Paul Hongsuck and Miech, Antoine and Pont-Tuset, Jordi and Laptev, Ivan and Sivic, Josef and Schmid, Cordelia},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{xu2021videoclip,
  title={Videoclip: Contrastive pre-training for zero-shot video-text understanding},
  author={Xu, Hu and Ghosh, Gargi and Huang, Po-Yao and Okhonko, Dmytro and Aghajanyan, Armen and Metze, Florian and Zettlemoyer, Luke and Feichtenhofer, Christoph},
  booktitle={EMNLP},
  year={2021}
}

@article{li2023videochat,
  title={Videochat: Chat-centric video understanding},
  author={Li, KunChang and He, Yinan and Wang, Yi and Li, Yizhuo and Wang, Wenhai and Luo, Ping and Wang, Yali and Wang, Limin and Qiao, Yu},
  journal={arXiv:2305.06355},
  year={2023}
}

@article{jiang2023motiongpt,
  title={Motiongpt: Human motion as a foreign language},
  author={Jiang, Biao and Chen, Xin and Liu, Wen and Yu, Jingyi and Yu, Gang and Chen, Tao},
  journal={NeurIPS},
  year={2023}
}


@article{yang2024vidchapters,
  title={Vidchapters-7m: Video chapters at scale},
  author={Yang, Antoine and Nagrani, Arsha and Laptev, Ivan and Sivic, Josef and Schmid, Cordelia},
  journal={NeurIPS},
  year={2024}
}

@article{bengio2013representation,
  title={Representation learning: A review and new perspectives},
  author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={IEEE TPAMI},
  year={2013},
}

@article{larochelle2009exploring,
  title={Exploring strategies for training deep neural networks.},
  author={Larochelle, Hugo and Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Lamblin, Pascal},
  journal={JMLR},
  year={2009}
}

@article{janocha2017loss,
  title={On loss functions for deep neural networks in classification},
  author={Janocha, Katarzyna and Czarnecki, Wojciech Marian},
  journal={TFML},
  year={2017}
}

@inproceedings{chen2022frame,
  title={Frame-wise action representations for long videos via sequence contrastive learning},
  author={Chen, Minghao and Wei, Fangyun and Li, Chong and Cai, Deng},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{wang2024omnivid,
  title={Omnivid: A generative framework for universal video understanding},
  author={Wang, Junke and Chen, Dongdong and Luo, Chong and He, Bo and Yuan, Lu and Wu, Zuxuan and Jiang, Yu-Gang},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{bansal2024videocon,
  title={Videocon: Robust video-language alignment via contrast captions},
  author={Bansal, Hritik and Bitton, Yonatan and Szpektor, Idan and Chang, Kai-Wei and Grover, Aditya},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{zhou2024streaming,
  title={Streaming dense video captioning},
  author={Zhou, Xingyi and Arnab, Anurag and Buch, Shyamal and Yan, Shen and Myers, Austin and Xiong, Xuehan and Nagrani, Arsha and Schmid, Cordelia},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{han2023autoad,
  title={AutoAD: Movie description in context},
  author={Han, Tengda and Bain, Max and Nagrani, Arsha and Varol, G{\"u}l and Xie, Weidi and Zisserman, Andrew},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{han2023autoadii,
  title={Autoad ii: The sequel-who, when, and what in movie audio description},
  author={Han, Tengda and Bain, Max and Nagrani, Arsha and Varol, Gul and Xie, Weidi and Zisserman, Andrew},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{han2024autoadiii,
  title={AutoAD III: The Prequel-Back to the Pixels},
  author={Han, Tengda and Bain, Max and Nagrani, Arsha and Varol, G{\"u}l and Xie, Weidi and Zisserman, Andrew},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{chen2024panda,
  title={Panda-70m: Captioning 70m videos with multiple cross-modality teachers},
  author={Chen, Tsai-Shien and Siarohin, Aliaksandr and Menapace, Willi and Deyneka, Ekaterina and Chao, Hsiang-wei and Jeon, Byung Eun and Fang, Yuwei and Lee, Hsin-Ying and Ren, Jian and Yang, Ming-Hsuan and others},
  booktitle={CVPR},
  year={2024}
}


@inproceedings{ren2024timechat,
  title={Timechat: A time-sensitive multimodal large language model for long video understanding},
  author={Ren, Shuhuai and Yao, Linli and Li, Shicheng and Sun, Xu and Hou, Lu},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{yan2023unloc,
  title={Unloc: A unified framework for video localization tasks},
  author={Yan, Shen and Xiong, Xuehan and Nagrani, Arsha and Arnab, Anurag and Wang, Zhonghao and Ge, Weina and Ross, David and Schmid, Cordelia},
  booktitle={ICCV},
  year={2023}
}

@article{tan2023egodistill,
  title={Egodistill: Egocentric head motion distillation for efficient video understanding},
  author={Tan, Shuhan and Nagarajan, Tushar and Grauman, Kristen},
  journal={NeurIPS},
  year={2023}
}

@inproceedings{wu2019adaframe,
  title={Adaframe: Adaptive frame selection for fast video recognition},
  author={Wu, Zuxuan and Xiong, Caiming and Ma, Chih-Yao and Socher, Richard and Davis, Larry S},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{wang2022adafocus,
  title={Adafocus v2: End-to-end training of spatial dynamic networks for video recognition},
  author={Wang, Yulin and Yue, Yang and Lin, Yuanze and Jiang, Haojun and Lai, Zihang and Kulikov, Victor and Orlov, Nikita and Shi, Humphrey and Huang, Gao},
  booktitle={CVPR},
  year={2022},
}

@inproceedings{xia2022nsnet,
  title={Nsnet: Non-saliency suppression sampler for efficient video recognition},
  author={Xia, Boyang and Wu, Wenhao and Wang, Haoran and Su, Rui and He, Dongliang and Yang, Haosen and Fan, Xiaoran and Ouyang, Wanli},
  booktitle={ECCV},
  year={2022},
}

@inproceedings{lin2022ocsampler,
  title={Ocsampler: Compressing videos to one clip with single-step sampling},
  author={Lin, Jintao and Duan, Haodong and Chen, Kai and Lin, Dahua and Wang, Limin},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{xia2022temporal,
  title={Temporal saliency query network for efficient video recognition},
  author={Xia, Boyang and Wang, Zhihao and Wu, Wenhao and Wang, Haoran and Han, Jungong},
  booktitle={ECCV},
  year={2022},
}

@inproceedings{nugroho2023audio,
  title={Audio-visual glance network for efficient video recognition},
  author={Nugroho, Muhammad Adi and Woo, Sangmin and Lee, Sumin and Kim, Changick},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{gowda2021smart,
  title={Smart frame selection for action recognition},
  author={Gowda, Shreyank N and Rohrbach, Marcus and Sevilla-Lara, Laura},
  booktitle={AAAI},
  year={2021}
}

@inproceedings{srivastava2024omnivec,
  title={Omnivec: Learning robust representations with cross modal sharing},
  author={Srivastava, Siddharth and Sharma, Gaurav},
  booktitle={WACV},
  year={2024}
}

@inproceedings{lei2021less,
  title={Less is more: Clipbert for video-and-language learning via sparse sampling},
  author={Lei, Jie and Li, Linjie and Zhou, Luowei and Gan, Zhe and Berg, Tamara L and Bansal, Mohit and Liu, Jingjing},
  booktitle={CVPR},
  year={2021}
}

@article{wu2019liteeval,
  title={Liteeval: A coarse-to-fine framework for resource efficient video recognition},
  author={Wu, Zuxuan and Xiong, Caiming and Jiang, Yu-Gang and Davis, Larry S},
  journal={NeurIPS},
  year={2019}
}

@inproceedings{gao2020listen,
  title={Listen to look: Action recognition by previewing audio},
  author={Gao, Ruohan and Oh, Tae-Hyun and Grauman, Kristen and Torresani, Lorenzo},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{korbar2019scsampler,
  title={Scsampler: Sampling salient clips from video for efficient action recognition},
  author={Korbar, Bruno and Tran, Du and Torresani, Lorenzo},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{meng2020ar,
  title={Ar-net: Adaptive frame resolution for efficient action recognition},
  author={Meng, Yue and Lin, Chung-Ching and Panda, Rameswar and Sattigeri, Prasanna and Karlinsky, Leonid and Oliva, Aude and Saenko, Kate and Feris, Rogerio},
  booktitle={ECCV},
  year={2020},
}

@inproceedings{wu2022nuwa,
  title={N{\"u}wa: Visual synthesis pre-training for neural visual world creation},
  author={Wu, Chenfei and Liang, Jian and Ji, Lei and Yang, Fan and Fang, Yuejian and Jiang, Daxin and Duan, Nan},
  booktitle={ECCV},
  year={2022},
}

@inproceedings{villegas2022phenaki,
  title={Phenaki: Variable length video generation from open domain textual descriptions},
  author={Villegas, Ruben and Babaeizadeh, Mohammad and Kindermans, Pieter-Jan and Moraldo, Hernan and Zhang, Han and Saffar, Mohammad Taghi and Castro, Santiago and Kunze, Julius and Erhan, Dumitru},
  booktitle={ICLR},
  year={2022}
}

@inproceedings{chen2017learning,
  title={Learning object-centric transformation for video prediction},
  author={Chen, Xiongtao and Wang, Wenmin and Wang, Jinzhuo and Li, Weimian},
  booktitle={MM},
  year={2017}
}

@inproceedings{jin2017video,
  title={Video scene parsing with predictive feature learning},
  author={Jin, Xiaojie and Li, Xin and Xiao, Huaxin and Shen, Xiaohui and Lin, Zhe and Yang, Jimei and Chen, Yunpeng and Dong, Jian and Liu, Luoqi and Jie, Zequn and others},
  booktitle={ICCV},
  year={2017}
}


@inproceedings{liang2017dual,
  title={Dual motion GAN for future-flow embedded video prediction},
  author={Liang, Xiaodan and Lee, Lisa and Dai, Wei and Xing, Eric P},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{cai2019action,
  title={Action knowledge transfer for action prediction with partial videos},
  author={Cai, Yijun and Li, Haoxin and Hu, Jian-Fang and Zheng, Wei-Shi},
  booktitle={AAAI},
  year={2019}
}

@inproceedings{fernando2021anticipating,
  title={Anticipating human actions by correlating past with the future with jaccard similarity measures},
  author={Fernando, Basura and Herath, Samitha},
  booktitle={CVPR},
  year={2021}
}

@article{hou2020confidence,
  title={Confidence-guided self refinement for action prediction in untrimmed videos},
  author={Hou, Jingyi and Wu, Xinxiao and Wang, Ruiqi and Luo, Jiebo and Jia, Yunde},
  journal={IEEE T-IP},
  year={2020}
}


@article{hu2018early,
  title={Early action prediction by soft regression},
  author={Hu, Jian-Fang and Zheng, Wei-Shi and Ma, Lianyang and Wang, Gang and Lai, Jianhuang and Zhang, Jianguo},
  journal={IEEE TPAMI},
  year={2018}
}

@inproceedings{kong2018action,
  title={Action prediction from videos via memorizing hard-to-predict samples},
  author={Kong, Yu and Gao, Shangqian and Sun, Bin and Fu, Yun},
  booktitle={AAAI},
  year={2018}
}

@inproceedings{stergiou2023wisdom,
  title={The wisdom of crowds: Temporal progressive attention for early action prediction},
  author={Stergiou, Alexandros and Damen, Dima},
  booktitle={CVPR},
  year={2023}
}


@inproceedings{xu2019prediction,
  title={Prediction-cgan: Human action prediction with conditional generative adversarial networks},
  author={Xu, Wanru and Yu, Jian and Miao, Zhenjiang and Wan, Lili and Ji, Qiang},
  booktitle={MM},
  year={2019}
}

@inproceedings{zhao2019spatiotemporal,
  title={Spatiotemporal feature residual propagation for action prediction},
  author={Zhao, He and Wildes, Richard P},
  booktitle={ICCV},
  year={2019}
}


@inproceedings{wang2019progressive,
  title={Progressive teacher-student learning for early action prediction},
  author={Wang, Xionghui and Hu, Jian-Fang and Lai, Jian-Huang and Zhang, Jianguo and Zheng, Wei-Shi},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{foo2022era,
  title={Era: Expert retrieval and assembly for early action prediction},
  author={Foo, Lin Geng and Li, Tianjiao and Rahmani, Hossein and Ke, Qiuhong and Liu, Jun},
  booktitle={ECCV},
  year={2022},
}

@article{wang2023magi,
  title={Magi-net: Meta negative network for early activity prediction},
  author={Wang, Wenqian and Chang, Faliang and Zhang, Junhao and Yan, Rui and Liu, Chunsheng and Wang, Bin and Shou, Mike Zheng},
  journal={IEEE T-IP},
  year={2023}
}

@article{xu2023dynamic,
  title={Dynamic Context Removal: A General Training Strategy for Robust Models on Video Action Predictive Tasks},
  author={Xu, Xinyu and Li, Yong-Lu and Lu, Cewu},
  journal={IJCV},
  year={2023},
}

@article{wu2021spatial,
  title={Spatial--temporal relation reasoning for action prediction in videos},
  author={Wu, Xinxiao and Wang, Ruiqi and Hou, Jingyi and Lin, Hanxi and Luo, Jiebo},
  journal={IJCV},
  year={2021},
}

@inproceedings{wu2021anticipating,
  title={Anticipating future relations via graph growing for action prediction},
  author={Wu, Xinxiao and Zhao, Jianwei and Wang, Ruiqi},
  booktitle={AAAI},
  year={2021}
}

@article{chen2022ambiguousness,
  title={Ambiguousness-aware state evolution for action prediction},
  author={Chen, Lei and Lu, Jiwen and Song, Zhanjie and Zhou, Jie},
  journal={IEEE TCSVT},
  year={2022},
}

@inproceedings{zhang2024extdm,
  title={Extdm: Distribution extrapolation diffusion model for video prediction},
  author={Zhang, Zhicheng and Hu, Junyao and Cheng, Wentao and Paudel, Danda and Yang, Jufeng},
  booktitle={CVPR},
  year={2024}
}

@article{zheng2023egocentric,
  title={Egocentric early action prediction via adversarial knowledge distillation},
  author={Zheng, Na and Song, Xuemeng and Su, Tianyu and Liu, Weifeng and Yan, Yan and Nie, Liqiang},
  journal={ACM TOMM},
  year={2023}
}


%% ----- Anticipation -----
@inproceedings{villegas2018hierarchical,
  title={Hierarchical long-term video prediction without supervision},
  author={Villegas, Ruben and Erhan, Dumitru and Lee, Honglak and others},
  booktitle={ICML},
  year={2018}
}

@inproceedings{villegas2017learning,
  title={Learning to generate long-term future via hierarchical prediction},
  author={Villegas, Ruben and Yang, Jimei and Zou, Yuliang and Sohn, Sungryull and Lin, Xunyu and Lee, Honglak},
  booktitle={ICML},
  year={2017}
}

@inproceedings{gong2022future,
  title={Future transformer for long-term action anticipation},
  author={Gong, Dayoung and Lee, Joonseok and Kim, Manjin and Ha, Seong Jong and Cho, Minsu},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{lee2019set,
  title={Set transformer: A framework for attention-based permutation-invariant neural networks},
  author={Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam and Choi, Seungjin and Teh, Yee Whye},
  booktitle={ICML},
  year={2019}
}

@inproceedings{abu2018will,
  title={When will you do what?-anticipating temporal occurrences of activities},
  author={Abu Farha, Yazan and Richard, Alexander and Gall, Juergen},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{ye2024stdiff,
  title={STDiff: Spatio-Temporal Diffusion for Continuous Stochastic Video Prediction},
  author={Ye, Xi and Bilodeau, Guillaume-Alexandre},
  booktitle={AAAI},
  year={2024}
}


@inproceedings{furnari2019would,
  title={What would you expect? anticipating egocentric actions with rolling-unrolling lstms and modality attention},
  author={Furnari, Antonino and Farinella, Giovanni Maria},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{vondrick2016anticipating,
  title={Anticipating visual representations from unlabeled video},
  author={Vondrick, Carl and Pirsiavash, Hamed and Torralba, Antonio},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{girdhar2021anticipative,
  title={Anticipative video transformer},
  author={Girdhar, Rohit and Grauman, Kristen},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{zhong2023anticipative,
  title={Anticipative feature fusion transformer for multi-modal action anticipation},
  author={Zhong, Zeyun and Schneider, David and Voit, Michael and Stiefelhagen, Rainer and Beyerer, J{\"u}rgen},
  booktitle={WACV},
  year={2023}
}

@article{wu2020learning,
  title={Learning to anticipate egocentric actions by imagination},
  author={Wu, Yu and Zhu, Linchao and Wang, Xiaohan and Yang, Yi and Wu, Fei},
  journal={IEEE T-IP},
  year={2020}
}

@article{dessalene2021forecasting,
  title={Forecasting action through contact representations from first person video},
  author={Dessalene, Eadom and Devaraj, Chinmaya and Maynord, Michael and Ferm{\"u}ller, Cornelia and Aloimonos, Yiannis},
  journal={IEEE TPAMI},
  year={2021}
}

@inproceedings{wang2022negative,
  title={Negative sample matters: A renaissance of metric learning for temporal grounding},
  author={Wang, Zhenzhi and Wang, Limin and Wu, Tao and Li, Tianhao and Wu, Gangshan},
  booktitle={AAAI},
  year={2022}
}

@inproceedings{pan2020adversarial,
  title={Adversarial cross-domain action recognition with co-attention},
  author={Pan, Boxiao and Cao, Zhangjie and Adeli, Ehsan and Niebles, Juan Carlos},
  booktitle={AAAI},
  year={2020}
}

@inproceedings{gammulle2019predicting,
  title={Predicting the future: A jointly learnt model for action anticipation},
  author={Gammulle, Harshala and Denman, Simon and Sridharan, Sridha and Fookes, Clinton},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{furnari2018leveraging,
  title={Leveraging uncertainty to rethink loss functions and evaluation measures for egocentric action anticipation},
  author={Furnari, Antonino and Battiato, Sebastiano and Maria Farinella, Giovanni},
  booktitle={ECCVw},
  year={2018}
}

@inproceedings{guo2024uncertainty,
  title={Uncertainty-aware Action Decoupling Transformer for Action Anticipation},
  author={Guo, Hongji and Agarwal, Nakul and Lo, Shao-Yuan and Lee, Kwonjoon and Ji, Qiang},
  booktitle={CVPR},
  year={2024}
}


@inproceedings{nawhal2022rethinking,
  title={Rethinking learning approaches for long-term action anticipation},
  author={Nawhal, Megha and Jyothi, Akash Abdu and Mori, Greg},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{kataoka2016recognition,
  title={Recognition of Transitional Action for Short-Term Action Prediction using Discriminative Temporal {CNN} Feature.},
  author={Kataoka, Hirokatsu and Miyashita, Yudai and Hayashi, Masaki and Iwata, Kenji and Satoh, Yutaka},
  booktitle={BMVC},
  year={2016}
}

@inproceedings{baque2017deep,
  title={Deep occlusion reasoning for multi-camera multi-target detection},
  author={Baqu{\'e}, Pierre and Fleuret, Fran{\c{c}}ois and Fua, Pascal},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{joo2015panoptic,
  title={Panoptic studio: A massively multiview system for social motion capture},
  author={Joo, Hanbyul and Liu, Hao and Tan, Lei and Gui, Lin and Nabbe, Bart and Matthews, Iain and Kanade, Takeo and Nobuhara, Shohei and Sheikh, Yaser},
  booktitle={ICCV},
  year={2015}
}

@article{chen2023vast,
  title={Vast: A vision-audio-subtitle-text omni-modality foundation model and dataset},
  author={Chen, Sihan and Li, Handong and Wang, Qunbo and Zhao, Zijia and Sun, Mingzhen and Zhu, Xinxin and Liu, Jing},
  journal={NeurIPS},
  year={2023}
}

@article{lin2023videoxum,
  title={Videoxum: Cross-modal visual and textural summarization of videos},
  author={Lin, Jingyang and Hua, Hang and Chen, Ming and Li, Yikang and Hsiao, Jenhao and Ho, Chiuman and Luo, Jiebo},
  journal={IEEE TM},
  year={2023}
}


@article{li2024videovista,
  title={Videovista: A versatile benchmark for video understanding and reasoning},
  author={Li, Yunxin and Chen, Xinyu and Hu, Baotian and Wang, Longyue and Shi, Haoyuan and Zhang, Min},
  journal={arXiv:2406.11303},
  year={2024}
}

@inproceedings{yang2024lemon,
  title={Lemon: Learning 3d human-object interaction relation from 2d images},
  author={Yang, Yuhang and Zhai, Wei and Luo, Hongchen and Cao, Yang and Zha, Zheng-Jun},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{zhang2025monst3r,
  title={Monst3r: A simple approach for estimating geometry in the presence of motion},
  author={Zhang, Junyi and Herrmann, Charles and Hur, Junhwa and Jampani, Varun and Darrell, Trevor and Cole, Forrester and Sun, Deqing and Yang, Ming-Hsuan},
  booktitle={ICLR},
  year={2025}
}

@article{li2024megasam,
  title={Megasam: Accurate, fast, and robust structure and motion from casual dynamic videos},
  author={Li, Zhengqi and Tucker, Richard and Cole, Forrester and Wang, Qianqian and Jin, Linyi and Ye, Vickie and Kanazawa, Angjoo and Holynski, Aleksander and Snavely, Noah},
  journal={arXiv:2412.04463},
  year={2024}
}

@inproceedings{fan2024hold,
  title={Hold: Category-agnostic 3d reconstruction of interacting hands and objects from video},
  author={Fan, Zicong and Parelli, Maria and Kadoglou, Maria Eleni and Chen, Xu and Kocabas, Muhammed and Black, Michael J and Hilliges, Otmar},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{lu2024align3r,
  title={Align3r: Aligned monocular depth estimation for dynamic videos},
  author={Lu, Jiahao and Huang, Tianyu and Li, Peng and Dou, Zhiyang and Lin, Cheng and Cui, Zhiming and Dong, Zhen and Yeung, Sai-Kit and Wang, Wenping and Liu, Yuan},
  booktitle={CVPR},
  year={2025}
}

@inproceedings{wang2024dust3r,
  title={Dust3r: Geometric 3d vision made easy},
  author={Wang, Shuzhe and Leroy, Vincent and Cabon, Yohann and Chidlovskii, Boris and Revaud, Jerome},
  booktitle={CVPR},
  year={2024}
}

@article{liang2024feed,
  title={Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular Videos},
  author={Liang, Hanxue and Ren, Jiawei and Mirzaei, Ashkan and Torralba, Antonio and Liu, Ziwei and Gilitschenski, Igor and Fidler, Sanja and Oztireli, Cengiz and Ling, Huan and Gojcic, Zan and others},
  journal={arXiv:2412.03526},
  year={2024}
}

@inproceedings{dwivedi2024tokenhmr,
  title={Tokenhmr: Advancing human mesh recovery with a tokenized pose representation},
  author={Dwivedi, Sai Kumar and Sun, Yu and Patel, Priyanka and Feng, Yao and Black, Michael J},
  booktitle={CVPR},
  year={2024}
}

@article{liu2025joint,
  title={Joint Optimization for 4D Human-Scene Reconstruction in the Wild},
  author={Liu, Zhizheng and Lin, Joe and Wu, Wayne and Zhou, Bolei},
  journal={arXiv:2501.02158},
  year={2025}
}

@article{lei2024mosca,
  title={Mosca: Dynamic gaussian fusion from casual videos via 4d motion scaffolds},
  author={Lei, Jiahui and Weng, Yijia and Harley, Adam and Guibas, Leonidas and Daniilidis, Kostas},
  journal={arXiv:2405.17421},
  year={2024}
}


@article{loper2015smpl,
  title={SMPL: A skinned multi-person linear model},
  author={Loper, Matthew and Mahmood, Naureen and Romero, Javier and Pons-Moll, Gerard and Black, Michael J},
  journal={ACM-TOG},
  year={2015}
}

@article{romero2017embodied,
  title={Embodied hands: modeling and capturing hands and bodies together},
  author={Romero, Javier and Tzionas, Dimitrios and Black, Michael J},
  journal={ACM TOG},
  year={2017},
}

@inproceedings{joo2018total,
  title={Total capture: A 3d deformation model for tracking faces, hands, and bodies},
  author={Joo, Hanbyul and Simon, Tomas and Sheikh, Yaser},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{xu2020ghum,
  title={Ghum \& ghuml: Generative 3d human shape and articulated pose models},
  author={Xu, Hongyi and Bazavan, Eduard Gabriel and Zanfir, Andrei and Freeman, William T and Sukthankar, Rahul and Sminchisescu, Cristian},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{kolotouros2019learning,
  title={Learning to reconstruct 3D human pose and shape via model-fitting in the loop},
  author={Kolotouros, Nikos and Pavlakos, Georgios and Black, Michael J and Daniilidis, Kostas},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{hassan2019resolving,
  title={Resolving 3D human pose ambiguities with 3D scene constraints},
  author={Hassan, Mohamed and Choutas, Vasileios and Tzionas, Dimitrios and Black, Michael J},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{omran2018neural,
  title={Neural body fitting: Unifying deep learning and model based human pose and shape estimation},
  author={Omran, Mohamed and Lassner, Christoph and Pons-Moll, Gerard and Gehler, Peter and Schiele, Bernt},
  booktitle={3DV},
  year={2018},
}

@inproceedings{kanazawa2018end,
  title={End-to-end recovery of human shape and pose},
  author={Kanazawa, Angjoo and Black, Michael J and Jacobs, David W and Malik, Jitendra},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{moon2022neuralannot,
  title={Neuralannot: Neural annotator for 3d human mesh training sets},
  author={Moon, Gyeongsik and Choi, Hongsuk and Lee, Kyoung Mu},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{li2022cliff,
  title={Cliff: Carrying location information in full frames into human pose and shape estimation},
  author={Li, Zhihao and Liu, Jianzhuang and Zhang, Zhensong and Xu, Songcen and Yan, Youliang},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{stathopoulos2024score,
  title={Score-guided diffusion for 3d human recovery},
  author={Stathopoulos, Anastasis and Han, Ligong and Metaxas, Dimitris},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{li2024coin,
  title={COIN: Control-Inpainting Diffusion Prior for Human and Camera Motion Estimation},
  author={Li, Jiefeng and Yuan, Ye and Rempe, Davis and Zhang, Haotian and Molchanov, Pavlo and Lu, Cewu and Kautz, Jan and Iqbal, Umar},
  booktitle={ECCV},
  year={2024}
}

@inproceedings{sengupta2023humaniflow,
  title={Humaniflow: Ancestor-conditioned normalising flows on so (3) manifolds for human pose and shape distribution estimation},
  author={Sengupta, Akash and Budvytis, Ignas and Cipolla, Roberto},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{zhang2023probabilistic,
  title={Probabilistic human mesh recovery in 3d scenes from egocentric views},
  author={Zhang, Siwei and Ma, Qianli and Zhang, Yan and Aliakbarian, Sadegh and Cosker, Darren and Tang, Siyu},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{joo2021exemplar,
  title={Exemplar fine-tuning for 3d human model fitting towards in-the-wild 3d human pose estimation},
  author={Joo, Hanbyul and Neverova, Natalia and Vedaldi, Andrea},
  booktitle={3DV},
  year={2021}
}

@inproceedings{pavlakos2019expressive,
  title={Expressive body capture: 3d hands, face, and body from a single image},
  author={Pavlakos, Georgios and Choutas, Vasileios and Ghorbani, Nima and Bolkart, Timo and Osman, Ahmed AA and Tzionas, Dimitrios and Black, Michael J},
  booktitle={CVPR},
  year={2019}
}

@article{allen2003space,
  title={The space of human body shapes: reconstruction and parameterization from range scans},
  author={Allen, Brett and Curless, Brian and Popovi{\'c}, Zoran},
  journal={ACM TOG},
  year={2003},
}

@inproceedings{allen2006learning,
  title={Learning a correlated model of identity and pose-dependent body shape variation for real-time synthesis},
  author={Allen, Brett and Curless, Brian and Popovi{\'c}, Zoran and Hertzmann, Aaron},
  booktitle={SIGGRAPH},
  year={2006}
}

@article{chu2025dreamscene4d,
  title={Dreamscene4d: Dynamic multi-object scene generation from monocular videos},
  author={Chu, Wen-Hsuan and Ke, Lei and Fragkiadaki, Katerina},
  journal={NeurIPS},
  year={2024}
}

@inproceedings{liu2025modgs,
      title={MoDGS: Dynamic Gaussian Splatting from Casually-captured Monocular Videos}, 
      author={Qingming Liu and Yuan Liu and Jiepeng Wang and Xianqiang Lyv and Peng Wang and Wenping Wang and Junhui Hou},
      year={2024},
      booktitle={ICLR}, 
}

@inproceedings{schonberger2016structure,
  title={Structure-from-motion revisited},
  author={Schonberger, Johannes L and Frahm, Jan-Michael},
  booktitle={CVPR},
  year={2016}
}

@article{teed2021droid,
  title={Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras},
  author={Teed, Zachary and Deng, Jia},
  journal={NeurIPS},
  year={2021}
}

@article{mur2017orb,
  title={Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras},
  author={Mur-Artal, Raul and Tard{\'o}s, Juan D},
  journal={IEEE TR},
  year={2017},
}

@inproceedings{gordon2019depth,
  title={Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras},
  author={Gordon, Ariel and Li, Hanhan and Jonschkowski, Rico and Angelova, Anelia},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{godard2019digging,
  title={Digging into self-supervised monocular depth estimation},
  author={Godard, Cl{\'e}ment and Mac Aodha, Oisin and Firman, Michael and Brostow, Gabriel J},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{zhang2022structure,
  title={Structure and motion from casual videos},
  author={Zhang, Zhoutong and Cole, Forrester and Li, Zhengqi and Rubinstein, Michael and Snavely, Noah and Freeman, William T},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{weinzaepfel2023croco,
  title={Croco v2: Improved cross-view completion pre-training for stereo matching and optical flow},
  author={Weinzaepfel, Philippe and Lucas, Thomas and Leroy, Vincent and Cabon, Yohann and Arora, Vaibhav and Br{\'e}gier, Romain and Csurka, Gabriela and Antsfeld, Leonid and Chidlovskii, Boris and Revaud, J{\'e}r{\^o}me},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{kopf2021robust,
  title={Robust consistent video depth estimation},
  author={Kopf, Johannes and Rong, Xuejian and Huang, Jia-Bin},
  booktitle={CVPR},
  year={2021}
}

@article{geiger2013vision,
  title={Vision meets robotics: The kitti dataset},
  author={Geiger, Andreas and Lenz, Philip and Stiller, Christoph and Urtasun, Raquel},
  journal={IJRR},
  year={2013}
}

@inproceedings{engel2014lsd,
  title={LSD-SLAM: Large-scale direct monocular SLAM},
  author={Engel, Jakob and Sch{\"o}ps, Thomas and Cremers, Daniel},
  booktitle={ECCV},
  year={2014}
}

@inproceedings{abdelsalam2023gepsan,
  title={Gepsan: Generative procedure step anticipation in cooking videos},
  author={Abdelsalam, Mohamed A and Rangrej, Samrudhdhi B and Hadji, Isma and Dvornik, Nikita and Derpanis, Konstantinos G and Fazly, Afsaneh},
  booktitle={ICCV},
  year={2023}
}

@article{roy2021action,
  title={Action anticipation using pairwise human-object interactions and transformers},
  author={Roy, Debaditya and Fernando, Basura},
  journal={IEEE T-IP},
  year={2021}
}

@inproceedings{roy2022action,
  title={Action anticipation using latent goal learning},
  author={Roy, Debaditya and Fernando, Basura},
  booktitle={WACV},
  year={2022}
}

@inproceedings{epstein2021learning,
  title={Learning temporal dynamics from cycles in narrated video},
  author={Epstein, Dave and Wu, Jiajun and Schmid, Cordelia and Sun, Chen},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{shen2018egocentric,
  title={Egocentric activity prediction via event modulated attention},
  author={Shen, Yang and Ni, Bingbing and Li, Zefan and Zhuang, Ning},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{piergiovanni2020adversarial,
  title={Adversarial generative grammars for human activity prediction},
  author={Piergiovanni, AJ and Angelova, Anelia and Toshev, Alexander and Ryoo, Michael S},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{zhao2020diverse,
  title={On diverse asynchronous activity anticipation},
  author={Zhao, He and Wildes, Richard P},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{sun2019relational,
  title={Relational action forecasting},
  author={Sun, Chen and Shrivastava, Abhinav and Vondrick, Carl and Sukthankar, Rahul and Murphy, Kevin and Schmid, Cordelia},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{liu2022hybrid,
  title={A hybrid egocentric activity anticipation framework via memory-augmented recurrent and one-shot representation forecasting},
  author={Liu, Tianshan and Lam, Kin-Man},
  booktitle={CVPR},
  year={2022}
}


@inproceedings{mascaro2023intention,
  title={Intention-conditioned long-term human egocentric action anticipation},
  author={Mascar{\'o}, Esteve Valls and Ahn, Hyemin and Lee, Dongheui},
  booktitle={WACV},
  year={2023}
}

@inproceedings{huang2018makes,
  title={What makes a video a video: Analyzing temporal information in video understanding models and datasets},
  author={Huang, De-An and Ramanathan, Vignesh and Mahajan, Dhruv and Torresani, Lorenzo and Paluri, Manohar and Fei-Fei, Li and Niebles, Juan Carlos},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{roy2024interaction,
  title={Interaction region visual transformer for egocentric action anticipation},
  author={Roy, Debaditya and Rajendiran, Ramanathan and Fernando, Basura},
  booktitle={WACV},
  year={2024}
}


%% ----- Pose and Tracking papers -----

@article{ormoneit2000learning,
  title={Learning and tracking cyclic human motion},
  author={Ormoneit, Dirk and Sidenbladh, Hedvig and Black, Michael and Hastie, Trevor},
  journal={NeurIPS},
  year={2000}
}

@inproceedings{stergiou2024holistic,
  title={Holistic representation learning for multitask trajectory anomaly detection},
  author={Stergiou, Alexandros and De Weerdt, Brent and Deligiannis, Nikos},
  booktitle={WACV},
  year={2024}
}

%% ----- END of Pose and TRacking papers -----


%% ----- Multimodal (more than 2 modalities) ----- 

@inproceedings{girdhar2022omnivore,
  title={Omnivore: A single model for many visual modalities},
  author={Girdhar, Rohit and Singh, Mannat and Ravi, Nikhila and Van Der Maaten, Laurens and Joulin, Armand and Misra, Ishan},
  booktitle={CVPR},
  year={2022}
}

@article{liang2022mind,
  title={Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning},
  author={Liang, Victor Weixin and Zhang, Yuhui and Kwon, Yongchan and Yeung, Serena and Zou, James Y},
  journal={NeurIPS},
  year={2022}
}

@article{liang2024foundations,
  title={Foundations \& trends in multimodal machine learning: Principles, challenges, and open questions},
  author={Liang, Paul Pu and Zadeh, Amir and Morency, Louis-Philippe},
  journal={ACM Computing Surveys},
  year={2024}
}



%%% Cognitive neuroscience

@article{jeannerod1994representing,
  title={The representing brain: Neural correlates of motor intention and imagery},
  author={Jeannerod, Marc},
  journal={BBS},
  year={1994},
}

@article{gallese1996action,
  title={Action recognition in the premotor cortex},
  author={Gallese, Vittorio and Fadiga, Luciano and Fogassi, Leonardo and Rizzolatti, Giacomo},
  journal={Brain},
  year={1996}
}

@article{calvo2005action,
  title={Action observation and acquired motor skills: an FMRI study with expert dancers},
  author={Calvo-Merino, Beatriz and Glaser, Daniel E and Gr{\`e}zes, Julie and Passingham, Richard E and Haggard, Patrick},
  journal={Cerebral cortex},
  year={2005}
}

@inproceedings{patsch2024long,
  title={Long-Term Action Anticipation Based on Contextual Alignment},
  author={Patsch, Constantin and Zhang, Jinghan and Wu, Yuankai and Zakour, Marsil and Salihu, Driton and Steinbach, Eckehard},
  booktitle={ICASSP},
  year={2024}
}

@article{kohler2002hearing,
  title={Hearing sounds, understanding actions: action representation in mirror neurons},
  author={Kohler, Evelyne and Keysers, Christian and Umilta, M Alessandra and Fogassi, Leonardo and Gallese, Vittorio and Rizzolatti, Giacomo},
  journal={Science},
  year={2002}
}

@article{li2024efficient,
  title={Efficient Action Counting with Dynamic Queries},
  author={Li, Zishi and Ma, Xiaoxuan and Shang, Qiuyan and Zhu, Wentao and Ci, Hai and Qiao, Yu and Wang, Yizhou},
  journal={arXiv:2403.01543},
  year={2024}
}

@article{zhao2024skim,
  title={Skim then Focus: Integrating Contextual and Fine-grained Views for Repetitive Action Counting},
  author={Zhao, Zhengqi and Huang, Xiaohu and Zhou, Hao and Yao, Kun and Ding, Errui and Wang, Jingdong and Wang, Xinggang and Liu, Wenyu and Feng, Bin},
  journal={arXiv:2406.08814},
  year={2024}
}

@inproceedings{wu2023stmixer,
  title={Stmixer: A one-stage sparse action detector},
  author={Wu, Tao and Cao, Mengqi and Gao, Ziteng and Wu, Gangshan and Wang, Limin},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{kumar2022end,
  title={End-to-end semi-supervised learning for video action detection},
  author={Kumar, Akash and Rawat, Yogesh Singh},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{sun2022dancetrack,
  title={Dancetrack: Multi-object tracking in uniform appearance and diverse motion},
  author={Sun, Peize and Cao, Jinkun and Jiang, Yi and Yuan, Zehuan and Bai, Song and Kitani, Kris and Luo, Ping},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{cui2023sportsmot,
  title={Sportsmot: A large multi-object tracking dataset in multiple sports scenes},
  author={Cui, Yutao and Zeng, Chenkai and Zhao, Xiaoyu and Yang, Yichun and Wu, Gangshan and Wang, Limin},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{ntinou2024multiscale,
  title={Multiscale vision transformers meet bipartite matching for efficient single-stage action localization},
  author={Ntinou, Ioanna and Sanchez, Enrique and Tzimiropoulos, Georgios},
  booktitle={CVPR},
  year={2024}
}


@inproceedings{li2022scale,
  title={Scale-aware spatio-temporal relation learning for video anomaly detection},
  author={Li, Guoqiu and Cai, Guanxiong and Zeng, Xingyu and Zhao, Rui},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{dave2022gabriellav2,
  title={Gabriellav2: Towards better generalization in surveillance videos for action detection},
  author={Dave, Ishan and Scheffer, Zacchaeus and Kumar, Akash and Shiraz, Sarah and Rawat, Yogesh Singh and Shah, Mubarak},
  booktitle={WACV},
  year={2022}
}

@inproceedings{xue2023egocentric,
  title={Egocentric video task translation},
  author={Xue, Zihui and Song, Yale and Grauman, Kristen and Torresani, Lorenzo},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{aakur2019perceptual,
  title={A perceptual prediction framework for self supervised event segmentation},
  author={Aakur, Sathyanarayanan N and Sarkar, Sudeep},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{shou2021generic,
  title={Generic event boundary detection: A benchmark for event segmentation},
  author={Shou, Mike Zheng and Lei, Stan Weixian and Wang, Weiyao and Ghadiyaram, Deepti and Feiszli, Matt},
  booktitle={ICCV},
  year={2021}
}

@article{mounir2023streamer,
  title={STREAMER: Streaming representation learning and event segmentation in a hierarchical manner},
  author={Mounir, Ramy and Vijayaraghavan, Sujal and Sarkar, Sudeep},
  journal={NeurIPS},
  year={2023}
}

@inproceedings{kang2022uboco,
  title={Uboco: Unsupervised boundary contrastive learning for generic event boundary detection},
  author={Kang, Hyolim and Kim, Jinwoo and Kim, Taehyun and Kim, Seon Joo},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{wu2023newsnet,
  title={Newsnet: A novel dataset for hierarchical temporal segmentation},
  author={Wu, Haoqian and Chen, Keyu and Liu, Haozhe and Zhuge, Mingchen and Li, Bing and Qiao, Ruizhi and Shu, Xiujun and Gan, Bei and Xu, Liangsheng and Ren, Bo and others},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{hadji2021representation,
  title={Representation learning via global temporal alignment and cycle-consistency},
  author={Hadji, Isma and Derpanis, Konstantinos G and Jepson, Allan D},
  booktitle={CVPR},
  year={2021}
}


@article{ashutosh2023video,
  title={Video-mined task graphs for keystep recognition in instructional videos},
  author={Ashutosh, Kumar and Ramakrishnan, Santhosh Kumar and Afouras, Triantafyllos and Grauman, Kristen},
  journal={NeurIPS},
  year={2023}
}

@inproceedings{zhou2023procedure,
  title={Procedure-aware pretraining for instructional video understanding},
  author={Zhou, Honglu and Mart{\'\i}n-Mart{\'\i}n, Roberto and Kapadia, Mubbasir and Savarese, Silvio and Niebles, Juan Carlos},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{ong2023chaotic,
  title={Chaotic World: A Large and Challenging Benchmark for Human Behavior Understanding in Chaotic Events},
  author={Ong, Kian Eng and Ng, Xun Long and Li, Yanchao and Ai, Wenjie and Zhao, Kuangyi and Yeo, Si Yong and Liu, Jun},
  booktitle={ICCV},
  year={2023}
}

@article{rizzolatti2001neurophysiological,
  title={Neurophysiological mechanisms underlying the understanding and imitation of action},
  author={Rizzolatti, Giacomo and Fogassi, Leonardo and Gallese, Vittorio},
  journal={Nature Reviews Neuroscience},
  year={2001}
}

@article{spunt2011identifying,
  title={Identifying the what, why, and how of an observed action: an fMRI study of mentalizing and mechanizing during action observation},
  author={Spunt, Robert P and Satpute, Ajay B and Lieberman, Matthew D},
  journal={JCN},
  year={2011},
}

@article{fogassi2005parietal,
  title={Parietal lobe: from action organization to intention understanding},
  author={Fogassi, Leonardo and Ferrari, Pier Francesco and Gesierich, Benno and Rozzi, Stefano and Chersi, Fabian and Rizzolatti, Giacomo},
  journal={Science},
  year={2005},
}

@article{kilner2011more,
  title={More than one pathway to action understanding},
  author={Kilner, James M},
  journal={Trends in cognitive sciences},
  year={2011},
}

@article{uithol2011understanding,
  title={Understanding motor resonance},
  author={Uithol, Sebo and van Rooij, Iris and Bekkering, Harold and Haselager, Pim},
  journal={Social neuroscience},
  year={2011}
}

@article{thompson2019conceptualizing,
  title={Conceptualizing and testing action understanding},
  author={Thompson, Emma L and Bird, Geoffrey and Catmur, Caroline},
  journal={NBR},
  year={2019}
}

@inproceedings{huang2020inset,
  title={INSET: Sentence Infilling with INter-SEntential Transformer},
  author={Huang, Yichen and Zhang, Yizhe and Elachqar, Oussama and Cheng, Yu},
  booktitle={ACL},
  year={2020}
}

@inproceedings{ippolito2019unsupervised,
  title={Unsupervised hierarchical story infilling},
  author={Ippolito, Daphne and Grangier, David and Callison-Burch, Chris and Eck, Douglas},
  booktitle={WNU},
  year={2019}
}

@inproceedings{saini2022disentangling,
  title={Disentangling visual embeddings for attributes and objects},
  author={Saini, Nirat and Pham, Khoi and Shrivastava, Abhinav},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{nagarajan2019grounded,
  title={Grounded human-object interaction hotspots from video},
  author={Nagarajan, Tushar and Feichtenhofer, Christoph and Grauman, Kristen},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{zhuo2019explainable,
  title={Explainable video action reasoning via prior knowledge and state transitions},
  author={Zhuo, Tao and Cheng, Zhiyong and Zhang, Peng and Wong, Yongkang and Kankanhalli, Mohan},
  booktitle={MM},
  year={2019}
}

@inproceedings{wang2021adaptive,
  title={Adaptive focus for efficient video recognition},
  author={Wang, Yulin and Chen, Zhaoxi and Jiang, Haojun and Song, Shiji and Han, Yizeng and Huang, Gao},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{wang2022adafocusv3,
  title={Adafocusv3: On unified spatial-temporal dynamic video recognition},
  author={Wang, Yulin and Yue, Yang and Xu, Xinhong and Hassani, Ali and Kulikov, Victor and Orlov, Nikita and Song, Shiji and Shi, Humphrey and Huang, Gao},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{hong2021transformation,
  title={Transformation driven visual reasoning},
  author={Hong, Xin and Lan, Yanyan and Pang, Liang and Guo, Jiafeng and Cheng, Xueqi},
  booktitle={CVPR},
  year={2021}
}

@article{thakur2024anticipating,
  title={Anticipating next active objects for egocentric videos},
  author={Thakur, Sanket and Beyan, Cigdem and Morerio, Pietro and Murino, Vittorio and Del Bue, Alessio},
  journal={IEEE Access},
  year={2024}
}


@inproceedings{acsintoae2022ubnormal,
  title={Ubnormal: New benchmark for supervised open-set video anomaly detection},
  author={Acsintoae, Andra and Florescu, Andrei and Georgescu, Mariana-Iuliana and Mare, Tudor and Sumedrea, Paul and Ionescu, Radu Tudor and Khan, Fahad Shahbaz and Shah, Mubarak},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{micorek2024mulde,
  title={MULDE: Multiscale Log-Density Estimation via Denoising Score Matching for Video Anomaly Detection},
  author={Micorek, Jakub and Possegger, Horst and Narnhofer, Dominik and Bischof, Horst and Kozinski, Mateusz},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{chen2023mgfn,
  title={Mgfn: Magnitude-contrastive glance-and-focus network for weakly-supervised video anomaly detection},
  author={Chen, Yingxian and Liu, Zhengzhe and Zhang, Baoheng and Fok, Wilton and Qi, Xiaojuan and Wu, Yik-Chung},
  booktitle={AAAI},
  year={2023}
}

@article{pu2023learning,
  title={Learning prompt-enhanced context features for weakly-supervised video anomaly detection},
  author={Pu, Yujiang and Wu, Xiaoyu and Yang, Lulu and Wang, Shengjin},
  journal={IEEE T-IP},
  year={2024},
}

@inproceedings{sultani2018real,
  title={Real-world anomaly detection in surveillance videos},
  author={Sultani, Waqas and Chen, Chen and Shah, Mubarak},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{dubey20193d,
  title={3d resnet with ranking loss function for abnormal activity detection in videos},
  author={Dubey, Shikha and Boragule, Abhijeet and Jeon, Moongu},
  booktitle={ICCAIS},
  year={2019}
}

@inproceedings{zhong2019graph,
  title={Graph convolutional label noise cleaner: Train a plug-and-play action classifier for anomaly detection},
  author={Zhong, Jia-Xing and Li, Nannan and Kong, Weijie and Liu, Shan and Li, Thomas H and Li, Ge},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{zhang2019temporal,
  title={Temporal convolutional network with complementary inner bag loss for weakly supervised anomaly detection},
  author={Zhang, Jiangong and Qing, Laiyun and Miao, Jun},
  booktitle={ICIP},
  year={2019},
}


@inproceedings{zhu2019motion,
  title={Motion-aware feature for improved video anomaly detection},
  author={Zhu, Yi and Newsam, Shawn},
  booktitle={BMVC},
  year={2019}
}

@inproceedings{wang2019gods,
  title={Gods: Generalized one-class discriminative subspaces for anomaly detection},
  author={Wang, Jue and Cherian, Anoop},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{markovitz2020graph,
  title={Graph embedded pose clustering for anomaly detection},
  author={Markovitz, Amir and Sharir, Gilad and Friedman, Itamar and Zelnik-Manor, Lihi and Avidan, Shai},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{morais2019learning,
  title={Learning regularity in skeleton trajectories for anomaly detection in videos},
  author={Morais, Romero and Le, Vuong and Tran, Truyen and Saha, Budhaditya and Mansour, Moussa and Venkatesh, Svetha},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{flaborea2023multimodal,
  title={Multimodal motion conditioned diffusion model for skeleton-based video anomaly detection},
  author={Flaborea, Alessandro and Collorone, Luca and Di Melendugno, Guido Maria D'Amely and D'Arrigo, Stefano and Prenkaj, Bardh and Galasso, Fabio},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{georgescu2021anomaly,
  title={Anomaly detection in video via self-supervised and multi-task learning},
  author={Georgescu, Mariana-Iuliana and Barbalau, Antonio and Ionescu, Radu Tudor and Khan, Fahad Shahbaz and Popescu, Marius and Shah, Mubarak},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{feng2021mist,
  title={Mist: Multiple instance self-training framework for video anomaly detection},
  author={Feng, Jia-Chang and Hong, Fa-Ting and Zheng, Wei-Shi},
  booktitle={CVPR},
  year={2021}
}


@inproceedings{purwanto2021dance,
  title={Dance with self-attention: A new look of conditional random fields on anomaly detection in videos},
  author={Purwanto, Didik and Chen, Yie-Tarng and Fang, Wen-Hsien},
  booktitle={ICCV},
  year={2021}
}


@inproceedings{tian2021weakly,
  title={Weakly-supervised video anomaly detection with robust temporal feature magnitude learning},
  author={Tian, Yu and Pang, Guansong and Chen, Yuanhong and Singh, Rajvinder and Verjans, Johan W and Carneiro, Gustavo},
  booktitle={ICCV},
  year={2021}
}

@article{zaheer2020self,
  title={A self-reasoning framework for anomaly detection using video-level labels},
  author={Zaheer, Muhammad Zaigham and Mahmood, Arif and Shin, Hochul and Lee, Seung-Ik},
  journal={IEEE SPL},
  year={2020},
}

@inproceedings{zaheer2020claws,
  title={Claws: Clustering assisted weakly supervised learning with normalcy suppression for anomalous event detection},
  author={Zaheer, Muhammad Zaigham and Mahmood, Arif and Astrid, Marcella and Lee, Seung-Ik},
  booktitle={ECCV},
  year={2020},
}

@inproceedings{li2021deep,
  title={Deep unsupervised anomaly detection},
  author={Li, Tangqing and Wang, Zheng and Liu, Siying and Lin, Wen-Yan},
  booktitle={WACV},
  year={2021}
}

@article{lee2019bman,
  title={BMAN: Bidirectional multi-scale aggregation networks for abnormal event detection},
  author={Lee, Sangmin and Kim, Hak Gu and Ro, Yong Man},
  journal={IEEE T-IP},
  year={2019},
}


@article{wang2021robust,
  title={Robust unsupervised video anomaly detection by multipath frame prediction},
  author={Wang, Xuanzhao and Che, Zhengping and Jiang, Bo and Xiao, Ning and Yang, Ke and Tang, Jian and Ye, Jieping and Wang, Jingyu and Qi, Qi},
  journal={IEEE TNNLS},
  year={2021},
}


@inproceedings{astrid2021learning,
  title={Learning not to reconstruct anomalies},
  author={Astrid, Marcella and Zaheer, Muhammad Zaigham and Lee, Jae-Yeong and Lee, Seung-Ik},
  booktitle={BMVC},
  year={2021}
}

@article{cho2022unsupervised,
  title={Unsupervised video anomaly detection via normalizing flows with implicit latent features},
  author={Cho, MyeongAh and Kim, Taeoh and Kim, Woo Jin and Cho, Suhwan and Lee, Sangyoun},
  journal={PR},
  year={2022},
}

@inproceedings{astrid2021synthetic,
  title={Synthetic temporal anomaly guided end-to-end video anomaly detection},
  author={Astrid, Marcella and Zaheer, Muhammad Zaigham and Lee, Seung-Ik},
  booktitle={ICCVw},
  year={2021}
}

@inproceedings{nguyen2019anomaly,
  title={Anomaly detection in video sequence with appearance-motion correspondence},
  author={Nguyen, Trong-Nguyen and Meunier, Jean},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{park2020learning,
  title={Learning memory-guided normality for anomaly detection},
  author={Park, Hyunjong and Noh, Jongyoun and Ham, Bumsub},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{luo2017revisit,
  title={A revisit of sparse coding based anomaly detection in stacked rnn framework},
  author={Luo, Weixin and Liu, Wen and Gao, Shenghua},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{hasan2016learning,
  title={Learning temporal regularity in video sequences},
  author={Hasan, Mahmudul and Choi, Jonghyun and Neumann, Jan and Roy-Chowdhury, Amit K and Davis, Larry S},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{almarri2024multi,
  title={A Multi-Head Approach with Shuffled Segments for Weakly-Supervised Video Anomaly Detection},
  author={AlMarri, Salem and Zaheer, Muhammad Zaigham and Nandakumar, Karthik},
  booktitle={WACVw},
  year={2024}
}

@inproceedings{fioresi2023ted,
  title={Ted-spad: Temporal distinctiveness for self-supervised privacy-preservation for video anomaly detection},
  author={Fioresi, Joseph and Dave, Ishan Rajendrakumar and Shah, Mubarak},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{yang2024text,
  title={Text Prompt with Normality Guidance for Weakly Supervised Video Anomaly Detection},
  author={Yang, Zhiwei and Liu, Jing and Wu, Peng},
  booktitle={CVPR},
  year={2024}
}

@article{wang2023gen,
  title={Gen-l-video: Multi-text to long video generation via temporal co-denoising},
  author={Wang, Fu-Yun and Chen, Wenshuo and Song, Guanglu and Ye, Han-Jia and Liu, Yu and Li, Hongsheng},
  journal={arXiv:2305.18264},
  year={2023}
}

@inproceedings{zanella2024harnessing,
  title={Harnessing Large Language Models for Training-free Video Anomaly Detection},
  author={Zanella, Luca and Menapace, Willi and Mancini, Massimiliano and Wang, Yiming and Ricci, Elisa},
  booktitle={CVPR},
  year={2024}
}

@article{zhao2024can,
  title={Can Vision Language Models Learn from Visual Demonstrations of Ambiguous Spatial Reasoning?},
  author={Zhao, Bowen and Dirac, Leo Parker and Varshavskaya, Paulina},
  journal={arXiv:2409.17080},
  year={2024}
}

@inproceedings{skorokhodov2022stylegan,
  title={Stylegan-v: A continuous video generator with the price, image quality and perks of stylegan2},
  author={Skorokhodov, Ivan and Tulyakov, Sergey and Elhoseiny, Mohamed},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{kim2024fifo,
  title={FIFO-Diffusion: Generating Infinite Videos from Text without Training},
  author={Kim, Jihwan and Kang, Junoh and Choi, Jinyoung and Han, Bohyung},
  booktitle={NeurIPS},
  year={2024}
}

@misc{videoworldsimulators2024,
  title={Video generation models as world simulators},
  author={Tim Brooks and Bill Peebles and Connor Holmes and Will DePue and Yufei Guo and Li Jing and David Schnurr and Joe Taylor and Troy Luhman and Eric Luhman and Clarence Ng and Ricky Wang and Aditya Ramesh},
  year={2024},
  url={https://openai.com/research/video-generation-models-as-world-simulators},
}

@article{renconsisti2v,
  title={ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation},
  author={Ren, Weiming and Yang, Huan and Zhang, Ge and Wei, Cong and Du, Xinrun and Huang, Wenhao and Chen, Wenhu},
  journal={TMLR},
  year={2024}
}



@article{dietterich1997solving,
  title={Solving the multiple instance problem with axis-parallel rectangles},
  author={Dietterich, Thomas G and Lathrop, Richard H and Lozano-P{\'e}rez, Tom{\'a}s},
  journal={Artificial intelligence},
  year={1997},
}

@article{brooks2022generating,
  title={Generating long videos of dynamic scenes},
  author={Brooks, Tim and Hellsten, Janne and Aittala, Miika and Wang, Ting-Chun and Aila, Timo and Lehtinen, Jaakko and Liu, Ming-Yu and Efros, Alexei and Karras, Tero},
  journal={NeurIPS},
  year={2022}
}

@inproceedings{shen2023mostgan,
  title={Mostgan-v: Video generation with temporal motion styles},
  author={Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  booktitle={CVPR},
  year={2023}
}

@article{liu2024sora,
  title={Sora: A review on background, technology, limitations, and opportunities of large vision models},
  author={Liu, Yixin and Zhang, Kai and Li, Yuan and Yan, Zhiling and Gao, Chujie and Chen, Ruoxi and Yuan, Zhengqing and Huang, Yue and Sun, Hanchi and Gao, Jianfeng and others},
  journal={arXiv:2402.17177},
  year={2024}
}


@article{hong2022cogvideo,
  title={Cogvideo: Large-scale pretraining for text-to-video generation via transformers},
  author={Hong, Wenyi and Ding, Ming and Zheng, Wendi and Liu, Xinghan and Tang, Jie},
  journal={arXiv:2205.15868},
  year={2022}
}


@inproceedings{yu2023avideo,
  title={Video probabilistic diffusion models in projected latent space},
  author={Yu, Sihyun and Sohn, Kihyuk and Kim, Subin and Shin, Jinwoo},
  booktitle={CVPR},
  year={2023}
}


@inproceedings{zeng2024make,
  title={Make pixels dance: High-dynamic video generation},
  author={Zeng, Yan and Wei, Guoqiang and Zheng, Jiani and Zou, Jiaxin and Wei, Yang and Zhang, Yuchen and Li, Hang},
  booktitle={CVPR},
  year={2024}
}

@article{gupta2023photorealistic,
  title={Photorealistic video generation with diffusion models},
  author={Gupta, Agrim and Yu, Lijun and Sohn, Kihyuk and Gu, Xiuye and Hahn, Meera and Fei-Fei, Li and Essa, Irfan and Jiang, Lu and Lezama, Jos{\'e}},
  journal={arXiv:2312.06662},
  year={2023}
}

@inproceedings{zhuang2024vlogger,
  title={Vlogger: Make your dream a vlog},
  author={Zhuang, Shaobin and Li, Kunchang and Chen, Xinyuan and Wang, Yaohui and Liu, Ziwei and Qiao, Yu and Wang, Yali},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{fu2023tell,
  title={Tell me what happened: Unifying text-guided video completion via multimodal masked video generation},
  author={Fu, Tsu-Jui and Yu, Licheng and Zhang, Ning and Fu, Cheng-Yang and Su, Jong-Chyi and Wang, William Yang and Bell, Sean},
  booktitle={CVPR},
  year={2023}
}

@article{wang2023videocomposer,
  title={Videocomposer: Compositional video synthesis with motion controllability},
  author={Wang, Xiang and Yuan, Hangjie and Zhang, Shiwei and Chen, Dayou and Wang, Jiuniu and Zhang, Yingya and Shen, Yujun and Zhao, Deli and Zhou, Jingren},
  journal={NeurIPS},
  year={2023}
}



@article{ho2022imagen,
  title={Imagen video: High definition video generation with diffusion models},
  author={Ho, Jonathan and Chan, William and Saharia, Chitwan and Whang, Jay and Gao, Ruiqi and Gritsenko, Alexey and Kingma, Diederik P and Poole, Ben and Norouzi, Mohammad and Fleet, David J and others},
  journal={arXiv:2210.02303},
  year={2022}
}

@inproceedings{moltisanti2019action,
  title={Action recognition from single timestamp supervision in untrimmed videos},
  author={Moltisanti, Davide and Fidler, Sanja and Damen, Dima},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{rahmanzadehgervi2024vision,
  title={Vision language models are blind},
  author={Rahmanzadehgervi, Pooyan and Bolton, Logan and Taesiri, Mohammad Reza and Nguyen, Anh Totti},
  booktitle={ACCV},
  year={2024}
}

@inproceedings{jang2023unifying,
  title={Unifying vision-language representation space with single-tower transformer},
  author={Jang, Jiho and Kong, Chaerin and Jeon, Donghyeon and Kim, Seonhoon and Kwak, Nojun},
  booktitle={AAAI},
  year={2023}
}

@inproceedings{jin2024integration,
  title={Integration of Global and Local Representations for Fine-grained Cross-modal Alignment},
  author={Jin, Seungwan and Choi, Hoyoung and Noh, Taehyung and Han, Kyungsik},
  booktitle={ECCV},
  year={2024}
}

@article{chen2021intriguing,
  title={Intriguing properties of contrastive losses},
  author={Chen, Ting and Luo, Calvin and Li, Lala},
  journal={NeurIPS},
  year={2021}
}

@inproceedings{cole2022does,
  title={When does contrastive visual representation learning work?},
  author={Cole, Elijah and Yang, Xuan and Wilber, Kimberly and Mac Aodha, Oisin and Belongie, Serge},
  booktitle={CVPR},
  year={2022}
}


@inproceedings{yang2020localizing,
  title={Localizing the common action among a few videos},
  author={Yang, Pengwan and Hu, Vincent Tao and Mettes, Pascal and Snoek, Cees G. M.},
  booktitle={ECCV},
  year={2020},
}

@inproceedings{han2022show,
  title={Show me what and tell me how: Video synthesis via multimodal conditioning},
  author={Han, Ligong and Ren, Jian and Lee, Hsin-Ying and Barbieri, Francesco and Olszewski, Kyle and Minaee, Shervin and Metaxas, Dimitris and Tulyakov, Sergey},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{hu2022make,
  title={Make it move: controllable image-to-video generation with text descriptions},
  author={Hu, Yaosi and Luo, Chong and Chen, Zhenzhong},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{babaeizadeh2018stochastic,
  title={Stochastic variational video prediction},
  author={Babaeizadeh, Mohammad and Finn, Chelsea and Erhan, Dumitru and Campbell, Roy H and Levine, Sergey},
  booktitle={ICLR},
  year={2018}
}

@inproceedings{castrejon2019improved,
  title={Improved conditional vrnns for video prediction},
  author={Castrejon, Lluis and Ballas, Nicolas and Courville, Aaron},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{dorkenwald2021stochastic,
  title={Stochastic image-to-video synthesis using cinns},
  author={Dorkenwald, Michael and Milbich, Timo and Blattmann, Andreas and Rombach, Robin and Derpanis, Konstantinos G and Ommer, Bjorn},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{zhang2020dtvnet,
  title={Dtvnet: Dynamic time-lapse video generation via single still image},
  author={Zhang, Jiangning and Xu, Chao and Liu, Liang and Wang, Mengmeng and Wu, Xia and Liu, Yong and Jiang, Yunliang},
  booktitle={ECCV},
  year={2020},
}

@inproceedings{menick2019generating,
  title={Generating high fidelity images with subscale pixel networks and multidimensional upscaling},
  author={Menick, Jacob and Kalchbrenner, Nal},
  booktitle={ICLR},
  year={2019}
}

@inproceedings{weissenborn2020scaling,
  title={Scaling autoregressive video models},
  author={Weissenborn, Dirk and T{\"a}ckstr{\"o}m, Oscar and Uszkoreit, Jakob},
  booktitle={ICLR},
  year={2020}
}

@article{le2021ccvs,
  title={Ccvs: Context-aware controllable video synthesis},
  author={Le Moing, Guillaume and Ponce, Jean and Schmid, Cordelia},
  journal={NeurIPS},
  year={2021}
}

@inproceedings{yan2018mt,
  title={Mt-vae: Learning motion transformations to generate multimodal human dynamics},
  author={Yan, Xinchen and Rastogi, Akash and Villegas, Ruben and Sunkavalli, Kalyan and Shechtman, Eli and Hadap, Sunil and Yumer, Ersin and Lee, Honglak},
  booktitle={ECCV},
  year={2018}
}

@article{saxena2021clockwork,
  title={Clockwork variational autoencoders},
  author={Saxena, Vaibhav and Ba, Jimmy and Hafner, Danijar},
  journal={NeurIPS},
  year={2021}
}

@inproceedings{yan2023temporally,
  title={Temporally consistent transformers for video generation},
  author={Yan, Wilson and Hafner, Danijar and James, Stephen and Abbeel, Pieter},
  booktitle={ICML},
  year={2023}
}

@article{finn2016unsupervised,
  title={Unsupervised learning for physical interaction through video prediction},
  author={Finn, Chelsea and Goodfellow, Ian and Levine, Sergey},
  journal={NeurIPS},
  year={2016}
}

@inproceedings{chatterjee2021hierarchical,
  title={A hierarchical variational neural uncertainty model for stochastic video prediction},
  author={Chatterjee, Moitreya and Ahuja, Narendra and Cherian, Anoop},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{kumar2020videoflow,
  title={Videoflow: A conditional flow-based model for stochastic video generation},
  author={Kumar, Manoj and Babaeizadeh, Mohammad and Erhan, Dumitru and Finn, Chelsea and Levine, Sergey and Dinh, Laurent and Kingma, Durk},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{franceschi2020stochastic,
  title={Stochastic latent residual video prediction},
  author={Franceschi, Jean-Yves and Delasalles, Edouard and Chen, Micka{\"e}l and Lamprier, Sylvain and Gallinari, Patrick},
  booktitle={ICML},
  year={2020}
}

@article{fragkiadaki2017motion,
  title={Motion prediction under multimodality with conditional stochastic networks},
  author={Fragkiadaki, Katerina and Huang, Jonathan and Alemi, Alex and Vijayanarasimhan, Sudheendra and Ricco, Susanna and Sukthankar, Rahul},
  journal={arXiv:1705.02082},
  year={2017}
}

@inproceedings{walker2016uncertain,
  title={An uncertain future: Forecasting from static images using variational autoencoders},
  author={Walker, Jacob and Doersch, Carl and Gupta, Abhinav and Hebert, Martial},
  booktitle={ECCV},
  year={2016},
}
@article{lee2018stochastic,
  title={Stochastic adversarial video prediction},
  author={Lee, Alex X and Zhang, Richard and Ebert, Frederik and Abbeel, Pieter and Finn, Chelsea and Levine, Sergey},
  journal={rXiv:1804.01523},
  year={2018}
}

@article{wu2021godiva,
  title={Godiva: Generating open-domain videos from natural descriptions},
  author={Wu, Chenfei and Huang, Lun and Zhang, Qianxi and Li, Binyang and Ji, Lei and Yang, Fan and Sapiro, Guillermo and Duan, Nan},
  journal={rXiv:2104.14806},
  year={2021}
}

@inproceedings{zhao2011online,
  title={Online detection of unusual events in videos via dynamic sparse coding},
  author={Zhao, Bin and Fei-Fei, Li and Xing, Eric P},
  booktitle={CVPR},
  year={2011},
}

@inproceedings{kitani2012activity,
  title={Activity forecasting},
  author={Kitani, Kris M and Ziebart, Brian D and Bagnell, James Andrew and Hebert, Martial},
  booktitle={ECCV},
  year={2012},
}

@article{koppula2015anticipating,
  title={Anticipating human activities using object affordances for reactive robotic response},
  author={Koppula, Hema S and Saxena, Ashutosh},
  journal={IEEE TPAMI},
  year={2015}
}

@inproceedings{liu2020forecasting,
  title={Forecasting human-object interaction: joint prediction of motor attention and actions in first person video},
  author={Liu, Miao and Tang, Siyu and Li, Yin and Rehg, James M},
  booktitle={ECCV},
  year={2020}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv:2106.09685},
  year={2021}
}

@inproceedings{luc2018predicting,
  title={Predicting future instance segmentation by forecasting convolutional features},
  author={Luc, Pauline and Couprie, Camille and Lecun, Yann and Verbeek, Jakob},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{miech2019leveraging,
  title={Leveraging the present to anticipate the future in videos},
  author={Miech, Antoine and Laptev, Ivan and Sivic, Josef and Wang, Heng and Torresani, Lorenzo and Tran, Du},
  booktitle={CVPRw},
  year={2019}
}

@inproceedings{zatsarynna2021multi,
  title={Multi-modal temporal convolutional network for anticipating actions in egocentric videos},
  author={Zatsarynna, Olga and Abu Farha, Yazan and Gall, Juergen},
  booktitle={CVPRw},
  pages={2249--2258},
  year={2021}
}

@misc{randal2009movie,
 author = {Munroe Randall},
 title = {Movie Narrative Charts},
 url = {https://xkcd.com/657/},
 year = {2009}
}
@inproceedings{ashutosh2023hiervl,
  title={Hiervl: Learning hierarchical video-language embeddings},
  author={Ashutosh, Kumar and Girdhar, Rohit and Torresani, Lorenzo and Grauman, Kristen},
  booktitle={CVPR},
  year={2023}
}

@article{yan2021videogpt,
  title={Videogpt: Video generation using vq-vae and transformers},
  author={Yan, Wilson and Zhang, Yunzhi and Abbeel, Pieter and Srinivas, Aravind},
  journal={arXiv:2104.10157},
  year={2021}
}

@inproceedings{menapace2021playable,
  title={Playable video generation},
  author={Menapace, Willi and Lathuiliere, Stephane and Tulyakov, Sergey and Siarohin, Aliaksandr and Ricci, Elisa},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{yu2024efficient,
  title={Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition},
  author={Yu, Sihyun and Nie, Weili and Huang, De-An and Li, Boyi and Shin, Jinwoo and Anandkumar, Anima},
  booktitle={ICLR},
  year={2024}
}

@inproceedings{arandjelovic2018objects,
  title={Objects that sound},
  author={Arandjelovic, Relja and Zisserman, Andrew},
  booktitle={ECCV},
  year={2018}
}

@article{kaiser2017one,
  title={One model to learn them all},
  author={Kaiser, Lukasz and Gomez, Aidan N and Shazeer, Noam and Vaswani, Ashish and Parmar, Niki and Jones, Llion and Uszkoreit, Jakob},
  journal={arXiv:1706.05137},
  year={2017}
}

@article{recasens2023zorro,
  title={Zorro: the masked multimodal transformer},
  author={Recasens, Adri{\`a} and Lin, Jason and Carreira, Jo{\=a}o and Jaegle, Drew and Wang, Luyu and Alayrac, Jean-baptiste and Luc, Pauline and Miech, Antoine and Smaira, Lucas and Hemsley, Ross and others},
  journal={arXiv:2301.09595},
  year={2023}
}

@article{xiao2020audiovisual,
  title={Audiovisual slowfast networks for video recognition},
  author={Xiao, Fanyi and Lee, Yong Jae and Grauman, Kristen and Malik, Jitendra and Feichtenhofer, Christoph},
  journal={arXiv:2001.08740},
  year={2020}
}

@inproceedings{saito2017temporal,
  title={Temporal generative adversarial nets with singular value clipping},
  author={Saito, Masaki and Matsumoto, Eiichi and Saito, Shunta},
  booktitle={ICCV},
  year={2017}
}



@inproceedings{ke2019time,
  title={Time-conditioned action anticipation in one shot},
  author={Ke, Qiuhong and Fritz, Mario and Schiele, Bernt},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{munro2020multi,
  title={Multi-modal domain adaptation for fine-grained action recognition},
  author={Munro, Jonathan and Damen, Dima},
  booktitle={CVPR},
  year={2020}
}

@article{jiang2021predicting,
  title={Predicting short-term next-active-object through visual attention and hand position},
  author={Jiang, Jingjing and Nan, Zhixiong and Chen, Hui and Chen, Shitao and Zheng, Nanning},
  journal={Neurocomputing},
  year={2021}
}

@inproceedings{liu2022joint,
  title={Joint hand motion and interaction hotspots prediction from egocentric videos},
  author={Liu, Shaowei and Tripathi, Subarna and Majumdar, Somdeb and Wang, Xiaolong},
  booktitle={CVPR},
  year={2022}
}


@article{mendonca2023structured,
  title={Structured world models from human videos},
  author={Mendonca, Russell and Bahl, Shikhar and Pathak, Deepak},
  journal={arXiv:2308.10901},
  year={2023}
}

@inproceedings{cheng2024egothink,
  title={EgoThink: Evaluating First-Person Perspective Thinking Capability of Vision-Language Models},
  author={Cheng, Sijie and Guo, Zhicheng and Wu, Jingwen and Fang, Kechen and Li, Peng and Liu, Huaping and Liu, Yang},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{diko2024semantically,
  title={Semantically Guided Representation Learning For Action Anticipation},
  author={Diko, Anxhelo and Avola, Danilo and Prenkaj, Bardh and Fontana, Federico and Cinque, Luigi},
  booktitle={ECCV},
  year={2024}
}

@article{chang2024look,
  title={Look ma, no hands! agent-environment factorization of egocentric videos},
  author={Chang, Matthew and Prakash, Aditya and Gupta, Saurabh},
  journal={NeurIPS},
  year={2024}
}

@article{mur2024aff,
  title={AFF-ttention! Affordances and Attention models for Short-Term Object Interaction Anticipation},
  author={Mur-Labadia, Lorenzo and Martinez-Cantin, Ruben and Guerrero, Josechu and Farinella, Giovanni Maria and Furnari, Antonino},
  journal={arXiv:2406.01194},
  year={2024}
}

@article{fan2023aigcbench,
  title={AIGCBench: Comprehensive evaluation of image-to-video content generated by AI},
  author={Fan, Fanda and Luo, Chunjie and Gao, Wanling and Zhan, Jianfeng},
  journal={BenchCouncil Transactions on Benchmarks, Standards and Evaluations},
  year={2023}
}

@inproceedings{liu2024evalcrafter,
  title={Evalcrafter: Benchmarking and evaluating large video generation models},
  author={Liu, Yaofang and Cun, Xiaodong and Liu, Xuebo and Wang, Xintao and Zhang, Yong and Chen, Haoxin and Liu, Yang and Zeng, Tieyong and Chan, Raymond and Shan, Ying},
  booktitle={CVPR},
  year={2024}
}

@article{liu2023fetv,
  title={Fetv: A benchmark for fine-grained evaluation of open-domain text-to-video generation},
  author={Liu, Yuanxin and Li, Lei and Ren, Shuhuai and Gao, Rundong and Li, Shicheng and Chen, Sishuo and Sun, Xu and Hou, Lu},
  journal={NeurIPS},
  year={2023}
}

@article{unterthiner2019fvd,
  title={FVD: A new metric for video generation},
  author={Unterthiner, Thomas and van Steenkiste, Sjoerd and Kurach, Karol and Marinier, Rapha{\"e}l and Michalski, Marcin and Gelly, Sylvain},
  journal={ICLR},
  year={2019}
}

@article{valevski2024diffusion,
  title={Diffusion models are real-time game engines},
  author={Valevski, Dani and Leviathan, Yaniv and Arar, Moab and Fruchter, Shlomi},
  journal={arXiv:2408.14837},
  year={2024}
}

@article{alonso2024diffusion,
  title={Diffusion for World Modeling: Visual Details Matter in Atari},
  author={Alonso, Eloi and Jelley, Adam and Micheli, Vincent and Kanervisto, Anssi and Storkey, Amos and Pearce, Tim and Fleuret, Fran{\c{c}}ois},
  journal={arXiv:2405.12399},
  year={2024}
}

@article{yang2023learning,
  title={Learning interactive real-world simulators},
  author={Yang, Mengjiao and Du, Yilun and Ghasemipour, Kamyar and Tompson, Jonathan and Schuurmans, Dale and Abbeel, Pieter},
  journal={arXiv:2310.06114},
  year={2023}
}

@article{saito2020train,
  title={Train sparsely, generate densely: Memory-efficient unsupervised training of high-resolution temporal gan},
  author={Saito, Masaki and Saito, Shunta and Koyama, Masanori and Kobayashi, Sosuke},
  journal={IJCV},
  year={2020},
}

@inproceedings{perrett2019ddlstm,
  title={DDLSTM: dual-domain LSTM for cross-dataset action recognition},
  author={Perrett, Toby and Damen, Dima},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{zatsarynna2024gated,
  title={Gated Temporal Diffusion for Stochastic Long-Term Dense Anticipation},
  author={Zatsarynna, Olga and Bahrami, Emad and Farha, Yazan Abu and Francesca, Gianpiero and Gall, Juergen},
  booktitle={ECCV},
  year={2024}
}

@inproceedings{bokhari2017long,
  title={Long-term activity forecasting using first-person vision},
  author={Bokhari, Syed Zahir and Kitani, Kris M},
  booktitle={ACCV},
  year={2017}
}

@inproceedings{abu2021long,
  title={Long-term anticipation of activities with cycle consistency},
  author={Abu, Yazan and Ke, Qiuhong and Schiele, Bernt and Gall, Juergen},
  booktitle={DAGM GCPR},
  year={2021},
}

@article{lee2006efficient,
  title={Efficient sparse coding algorithms},
  author={Lee, Honglak and Battle, Alexis and Raina, Rajat and Ng, Andrew},
  journal={NeurIPS},
  year={2006}
}

@article{tu2018multistream,
  title={Multi-Stream {CNN}: {L}earning Representations Based on Human-Related Regions for Action Recognition},
  author={Tu, Zhigang and Xie, Wei and Qin, Qianqing and Poppe, Ronald and Veltkamp, Remco C and Li, Baoxin and Yuan, Junsong},
  journal={PR},
  year={2018}
}

@article{lu2024improving,
  title={Improving the Generalization of ViTs for Action
Understanding with VLM Pre-Training},
  author={Lu, Hui and Poppe, Ronald and Salah, Albert A},
  journal={arXiv:2403.16128},
  year={2024}
}

@inproceedings{mettes2020searching,
  title={Searching for Actions on the Hyperbole},
  author={Long, Teng and Mettes, Pascal and Shen, Heng Tao and Snoek, Cees G. M.},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{liu2024physgen,
  title={PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation},
  author={Liu, Shaowei and Ren, Zhongzheng and Gupta, Saurabh and Wang, Shenlong},
  booktitle={ECCV},
  year={2024}
}

@inproceedings{kowal2024understanding,
  title={Understanding Video Transformers via Universal Concept Discovery},
  author={Kowal, Matthew and Dave, Achal and Ambrus, Rares and Gaidon, Adrien and Derpanis, Konstantinos G and Tokmakov, Pavel},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{park2023self,
  title={What Do Self-Supervised Vision Transformers Learn?},
  author={Park, Namuk and Kim, Wonjae and Heo, Byeongho and Kim, Taekyung and Yun, Sangdoo},
  booktitle={ICLR},
  year={2023}
}

@inproceedings{walmer2023teaching,
  title={Teaching matters: Investigating the role of supervision in vision transformers},
  author={Walmer, Matthew and Suri, Saksham and Gupta, Kamal and Shrivastava, Abhinav},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{kowal2024visual,
  title={Visual Concept Connectome (VCC): Open World Concept Discovery and their Interlayer Connections in Deep Models},
  author={Kowal, Matthew and Wildes, Richard P and Derpanis, Konstantinos G},
  booktitle={CVPR},
  year={2024}
}



@article{minh2022explainable,
  title={Explainable artificial intelligence: A comprehensive review},
  author={Minh, Dang and Wang, H. Xiang and Li, Y. Fen and Nguyen, Tan N.},
  journal={Artificial Intelligence Review},
  year={2022}
}

@inproceedings{iofinova2023bias,
  title={Bias in Pruned Vision Models: In-Depth Analysis and Countermeasures},
  author={Iofinova, Eugenia and Peste, Alexandra and Alistarh, Dan},
  booktitle={CVPR},
  year={2023}
}

@article{kaufmann2023survey,
  title={A Survey of Reinforcement Learning from Human Feedback},
  author={Kaufmann, Timo and Weng, Paul and Bengs, Viktor and H{\"u}llermeier, Eyke},
  journal={arXiv:2312.14925},
  year={2023}
}

@inproceedings{bugarin2024unveiling,
  title={Unveiling the Anomalies in an Ever-Changing World: A Benchmark for Pixel-Level Anomaly Detection in Continual Learning},
  author={Bugarin, Nikola and Bugaric, Jovana and Barusco, Manuel and Pezze, Davide Dalle and Susto, Gian Antonio},
  booktitle={CVPRw},
  year={2024}
}

@inproceedings{eyzaguirre2024streaming,
  title={Streaming Detection of Queried Event Start},
  author={Eyzaguirre, Cristobal and Tang, Eric and Buch, Shyamal and Gaidon, Adrien and Wu, Jiajun and Niebles, Juan Carlos},
  booktitle={NeurIPS},
  year={2024}
}

@article{majumder2024viewpoint,
  title={Which Viewpoint Shows it Best? Language for Weakly Supervising View Selection in Multi-view Videos},
  author={Majumder, Sagnik and Nagarajan, Tushar and Al-Halah, Ziad and Pradhan, Reina and Grauman, Kristen},
  journal={arXiv:2411.08753},
  year={2024}
}

@article{chen2024efficient,
      title={Efficient Transfer Learning for Video-language Foundation Models}, 
      author={Haoxing Chen and Zizheng Huang and Yan Hong and Yanshuo Wang and Zhongcai Lyu and Zhuoer Xu and Jun Lan and Zhangxuan Gu},
      year={2024},
      journal={arXiv:2411.11223}
}

@inproceedings{liui2024kea,
  title={IKEA Manuals at Work: 4D Grounding of Assembly Instructions on Internet Videos},
  author={Liu, Yunong and Eyzaguirre, Cristobal and Li, Manling and Khanna, Shubh and Niebles, Juan Carlos and Ravi, Vineeth and Mishra, Saumitra and Liu, Weiyu and Wu, Jiajun},
  booktitle={NeurIPS},
  year={2024}
}

@inproceedings{van2024moving,
  title={Moving Off-the-Grid: Scene-Grounded Video Representations},
  author={van Steenkiste, Sjoerd and Zoran, Daniel and Yang, Yi and Rubanova, Yulia and Kabra, Rishabh and Doersch, Carl and Gokay, Dilara and Heyward, Joseph and Pot, Etienne and Greff, Klaus and others},
  booktitle={NeurIPS},
  year={2024}
}

@inproceedings{chandrasegaran2024hourvideo,
  title={HourVideo: 1-Hour Video-Language Understanding},
  author={Chandrasegaran, Keshigeyan and Gupta, Agrim and Hadzic, Lea M and Kota, Taran and He, Jimming and Eyzaguirre, Crist{\'o}bal and Durante, Zane and Li, Manling and Wu, Jiajun and Fei-Fei, Li},
  booktitle={NeurIPS},
  year={2024}
}

@inproceedings{weinland2010makingaction,
  title={Making Action Recognition Robust to Occlusions and Viewpoint Changes},
  author={Weinland, Daniel and {\"O}zuysal, Mustafa and Fua, Pascal},
  booktitle={ECCV},
  year={2010}
}

@inproceedings{rehg2013decoding,
  title={Decoding children’s social behavior},
  author={Rehg, James M. and Abowd, Gregory D. and Rozga, Agata and Romero, Mario and Clements, Mark A. and Sclaroff, Stan and Essa, Irfan and Ousley, Opal Y. and Li, Yin and Kim, Chanho and Rao, Hrishikesh and Kim, Jonathan C. and Presti, Liliana Lo and Zhang, Jianming and Lantsman, Denis and Bidwell, Jonathan and Ye, Zhefan},
  booktitle={CVPR},
  year={2013}
}

@inproceedings{jain2015whatdo,
  title={What do 15,000 Object Categories Tell Us About Classifying and Localizing Actions},
  author={Jain, Mihir and van Gemert, Jan C and Snoek, Cees G. M.},
  booktitle={CVPR},
  year={2015}
}

@inproceedings{choi2012unified,
  title={A Unified Framework for Multi-target Tracking and Collective Activity Recognition},
  author={Choi, Wongun and Savarese, Silvio},
  booktitle={ECCV},
  year={2012}
}

@article{wang2013dense,
  title={Dense Trajectories and Motion Boundary Descriptors for Action Recognition},
  author={Wang, Heng and Kl{\"a}ser, Alexander and Schmid, Cordelia and Liu, Cheng-Lin},
  journal={IJCV},
  year={2013}
}

@inproceedings{benfold2011stable,
  title={Stable multi-target tracking in real-time surveillance video},
  author={Benfold, Ben and Reid, Ian},
  booktitle={CVPR},
  year={2011}
}

@inproceedings{oreifej2013hon4d,
  title={HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences},
  author={Oreifej, Omar and Liu, Zicheng},
  booktitle={CVPR},
  year={2013}
}

@article{wang2014learning,
  title={Learning Actionlet Ensemble for 3D Human Action Recognition},
  author={Wang, Jiang and Liu, Zicheng and Wu, Ying and Yuan, Junsong},
  journal={IEEE TPAMI},
  year={2014}
}

@inproceedings{hoai2011joint,
  title={Joint segmentation and classification of human actions in video},
  author={Hoai, Minh and Lan, Zhen-Zhong and {De la Torre}, Fernando},
  booktitle={CVPR},
  year={2011}
}

@article{gaidon2013temporal,
  title={Temporal Localization of Actions with Actoms},
  author={Gaidon, Adrien and Harchaoui, Zaid and Schmid, Cordelia},
  journal={IEEE TPAMI},
  year={2013}
}

@article{zhou2013hierarchical,
  title={Hierarchical Aligned Cluster Analysis for Temporal Clustering of Human Motion},
  author={Zhou, Feng and {De la Torre}, Fernando and Hodgins, Jessica K.},
  journal={IEEE TPAMI},
  year={2013}
}

@inproceedings{rohrbach2011translating,
  title={Translating Video Content to Natural Language Descriptions},
  author={Rohrbach, Marcus and Qiu, Wei and Titov, Ivan and Thater, Stefan and Pinkal, Manfred and Schiele, Bernt},
  booktitle={ICCV},
  year={2011}
}

@article{takano2015statistical,
  title={Statistical mutual conversion between whole body motion primitives and linguistic sentences for human motions},
  author={Takano, Wataru and Nakamura, Yoshihiko},
  journal={IJRR},
  year={2015}
}

@article{vinciarelli2012bridging,
  title={Bridging the Gap between Social Animal and Unsocial Machine: A Survey of Social Signal Processing},
  author={Vinciarelli, Alessandro and Pantic, Maja and Heylen, Dirk and Pelachaud, Catherine and Poggi, Isabella and D'Errico, Francesca and Schroeder, Marc},
  journal={IEEE TAFFC},
  year={2012}
}

@inproceedings{song2011multiple,
  title={Multiple feature hashing for real-time large scale near-duplicate video retrieval},
  author={Song, Jingkuan and Yang, Yi and Huang, Zi and Shen, Heng Tao and Hong, Richang},
  booktitle={MM},
  year={2011}
}


@inproceedings{li2010action,
  title={Action recognition based on a bag of 3d points},
  author={Li, Wanqing and Zhang, Zhengyou and Liu, Zicheng},
  booktitle={CVPRw},
  year={2010}
}

@inproceedings{bogo2017dynamic,
  title={Dynamic FAUST: Registering human bodies in motion},
  author={Bogo, Federica and Romero, Javier and Pons-Moll, Gerard and Black, Michael J},
  booktitle={CVPR},
  year={2017}
}

@article{perrett2025hd,
  title={HD-EPIC: A Highly-Detailed Egocentric Video Dataset},
  author={Perrett, Toby and Darkhalil, Ahmad and Sinha, Saptarshi and Emara, Omar and Pollard, Sam and Parida, Kranti and Liu, Kaiting and Gatti, Prajwal and Bansal, Siddhant and Flanagan, Kevin and others},
  journal={arXiv:2502.04144},
  year={2025}
}

@inproceedings{liu2022hoi4d,
  title={Hoi4d: A 4d egocentric dataset for category-level human-object interaction},
  author={Liu, Yunze and Liu, Yun and Jiang, Che and Lyu, Kangbo and Wan, Weikang and Shen, Hao and Liang, Boqiang and Fu, Zhoujie and Wang, He and Yi, Li},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{wang2023holoassist,
  title={Holoassist: an egocentric human interaction dataset for interactive ai assistants in the real world},
  author={Wang, Xin and Kwon, Taein and Rad, Mahdi and Pan, Bowen and Chakraborty, Ishani and Andrist, Sean and Bohus, Dan and Feniello, Ashley and Tekin, Bugra and Frujeri, Felipe Vieira and others},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{pan2023aria,
  title={Aria digital twin: A new benchmark dataset for egocentric 3d machine perception},
  author={Pan, Xiaqing and Charron, Nicholas and Yang, Yongqian and Peters, Scott and Whelan, Thomas and Kong, Chen and Parkhi, Omkar and Newcombe, Richard and Ren, Yuheng Carl},
  booktitle={ICCV},
  year={2023}
}

@article{lv2024aria,
  title={Aria everyday activities dataset},
  author={Lv, Zhaoyang and Charron, Nicholas and Moulon, Pierre and Gamino, Alexander and Peng, Cheng and Sweeney, Chris and Miller, Edward and Tang, Huixuan and Meissner, Jeff and Dong, Jing and others},
  journal={arXiv:2402.13349},
  year={2024}
}

@article{straub2024efm3d,
  title={EFM3D: A Benchmark for Measuring Progress Towards 3D Egocentric Foundation Models},
  author={Straub, Julian and DeTone, Daniel and Shen, Tianwei and Yang, Nan and Sweeney, Chris and Newcombe, Richard},
  journal={arXiv:2406.10224},
  year={2024}
}

@article{pons2015dyna,
  title={Dyna: A model of dynamic human shape in motion},
  author={Pons-Moll, Gerard and Romero, Javier and Mahmood, Naureen and Black, Michael J},
  journal={ACM TOG},
  year={2015}
}

@inproceedings{mahmood2019amass,
  title={AMASS: Archive of motion capture as surface shapes},
  author={Mahmood, Naureen and Ghorbani, Nima and Troje, Nikolaus F and Pons-Moll, Gerard and Black, Michael J},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{jiang2022neuman,
  title={Neuman: Neural human radiance field from a single video},
  author={Jiang, Wei and Yi, Kwang Moo and Samei, Golnoosh and Tuzel, Oncel and Ranjan, Anurag},
  booktitle={ECCV},
  year={2022}
}


@inproceedings{guo2020action2motion,
  title={Action2motion: Conditioned generation of 3d human motions},
  author={Guo, Chuan and Zuo, Xinxin and Wang, Sen and Zou, Shihao and Sun, Qingyao and Deng, Annan and Gong, Minglun and Cheng, Li},
  booktitle={ACM MM},
  year={2020}
}


@inproceedings{ben20243dinaction,
  title={3dinaction: Understanding human actions in 3d point clouds},
  author={Ben-Shabat, Yizhak and Shrout, Oren and Gould, Stephen},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{fiche2024vq,
  title={Vq-hps: Human pose and shape estimation in a vector-quantized latent space},
  author={Fiche, Gu{\'e}nol{\'e} and Leglaive, Simon and Alameda-Pineda, Xavier and Agudo, Antonio and Moreno-Noguer, Francesc},
  booktitle={ECCV},
  year={2024}
}

@inproceedings{taheri2020grab,
  title={GRAB: A dataset of whole-body human grasping of objects},
  author={Taheri, Omid and Ghorbani, Nima and Black, Michael J and Tzionas, Dimitrios},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{black2023bedlam,
  title={Bedlam: A synthetic dataset of bodies exhibiting detailed lifelike animated motion},
  author={Black, Michael J and Patel, Priyanka and Tesch, Joachim and Yang, Jinlong},
  booktitle={CVPR},
  year={2023}
}

@article{lin2023motion,
  title={Motion-x: A large-scale 3d expressive whole-body human motion dataset},
  author={Lin, Jing and Zeng, Ailing and Lu, Shunlin and Cai, Yuanhao and Zhang, Ruimao and Wang, Haoqian and Zhang, Lei},
  journal={NeurIPS},
  year={2023}
}

@inproceedings{delmas2022posescript,
  title={Posescript: 3d human poses from natural language},
  author={Delmas, Ginger and Weinzaepfel, Philippe and Lucas, Thomas and Moreno-Noguer, Francesc and Rogez, Gr{\'e}gory},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{bhatnagar2022behave,
  title={Behave: Dataset and method for tracking human object interactions},
  author={Bhatnagar, Bharat Lal and Xie, Xianghui and Petrov, Ilya A and Sminchisescu, Cristian and Theobalt, Christian and Pons-Moll, Gerard},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{bronstein2010shrec,
  title={SHREC 2010: robust correspondence benchmark},
  author={Bronstein, Alex and Bronstein, MM and Castellani, Umberto and Dubrovina, Anastasia and Guibas, LJ and Horaud, RP and Kimmel, Ron and Knossow, David and Von Lavante, Etienne and Mateus, Diana and others},
  booktitle={Eurographicsw 3D-OR},
  year={2010}
}

@inproceedings{anguelov2005scape,
  title={Scape: shape completion and animation of people},
  author={Anguelov, Dragomir and Srinivasan, Praveen and Koller, Daphne and Thrun, Sebastian and Rodgers, Jim and Davis, James},
  booktitle={SIGGRAPH},
  year={2005}
}

@inproceedings{bogo2014faust,
  title={FAUST: Dataset and evaluation for 3D mesh registration},
  author={Bogo, Federica and Romero, Javier and Loper, Matthew and Black, Michael J},
  booktitle={CVPR},
  year={2014}
}

@inproceedings{zou20203d,
  title={3D human shape reconstruction from a polarization image},
  author={Zou, Shihao and Zuo, Xinxin and Qian, Yiming and Wang, Sen and Xu, Chi and Gong, Minglun and Cheng, Li},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{zhang2022egobody,
  title={Egobody: Human body shape and motion of interacting people from head-mounted devices},
  author={Zhang, Siwei and Ma, Qianli and Zhang, Yan and Qian, Zhiyin and Kwon, Taein and Pollefeys, Marc and Bogo, Federica and Tang, Siyu},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{cai2022humman,
  title={Humman: Multi-modal 4d human dataset for versatile sensing and modeling},
  author={Cai, Zhongang and Ren, Daxuan and Zeng, Ailing and Lin, Zhengyu and Yu, Tao and Wang, Wenjia and Fan, Xiangyu and Gao, Yang and Yu, Yifan and Pan, Liang and others},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{corona2020ganhand,
  title={Ganhand: Predicting human grasp affordances in multi-object scenes},
  author={Corona, Enric and Pumarola, Albert and Alenya, Guillem and Moreno-Noguer, Francesc and Rogez, Gr{\'e}gory},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{grady2021contactopt,
  title={Contactopt: Optimizing contact to improve grasps},
  author={Grady, Patrick and Tang, Chengcheng and Twigg, Christopher D and Vo, Minh and Brahmbhatt, Samarth and Kemp, Charles C},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{hasson2019learning,
  title={Learning joint reconstruction of hands and manipulated objects},
  author={Hasson, Yana and Varol, Gul and Tzionas, Dimitrios and Kalevatykh, Igor and Black, Michael J and Laptev, Ivan and Schmid, Cordelia},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{hasson2020leveraging,
  title={Leveraging photometric consistency over time for sparsely supervised hand-object reconstruction},
  author={Hasson, Yana and Tekin, Bugra and Bogo, Federica and Laptev, Ivan and Pollefeys, Marc and Schmid, Cordelia},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{hasson2021towards,
  title={Towards unconstrained joint hand-object reconstruction from rgb videos},
  author={Hasson, Yana and Varol, G{\"u}l and Schmid, Cordelia and Laptev, Ivan},
  booktitle={3DV},
  year={2021},
}

@inproceedings{ng2021body2hands,
  title={Body2hands: Learning to infer 3d hands from conversational gesture body dynamics},
  author={Ng, Evonne and Ginosar, Shiry and Darrell, Trevor and Joo, Hanbyul},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{liu2021semi,
  title={Semi-supervised 3d hand-object poses estimation with interactions in time},
  author={Liu, Shaowei and Jiang, Hanwen and Xu, Jiarui and Liu, Sifei and Wang, Xiaolong},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{tekin2019h+,
  title={H+ o: Unified egocentric recognition of 3d hand-object poses and interactions},
  author={Tekin, Bugra and Bogo, Federica and Pollefeys, Marc},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{tse2022collaborative,
  title={Collaborative learning for hand and object reconstruction with attention-guided graph convolution},
  author={Tse, Tze Ho Elden and Kim, Kwang In and Leonardis, Ales and Chang, Hyung Jin},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{pavlakos2024reconstructing,
  title={Reconstructing hands in 3d with transformers},
  author={Pavlakos, Georgios and Shan, Dandan and Radosavovic, Ilija and Kanazawa, Angjoo and Fouhey, David and Malik, Jitendra},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{xu2023h2onet,
  title={H2onet: Hand-occlusion-and-orientation-aware network for real-time 3d hand mesh reconstruction},
  author={Xu, Hao and Wang, Tianyu and Tang, Xiao and Fu, Chi-Wing},
  booktitle={CVPR},
  year={2023}
}

@article{dong2024hamba,
  title={Hamba: Single-view 3d hand reconstruction with graph-guided bi-scanning mamba},
  author={Dong, Haoye and Chharia, Aviral and Gou, Wenbo and Vicente Carrasco, Francisco and De la Torre, Fernando D},
  journal={NeurIPS},
  year={2024}
}

@inproceedings{leng2023dynamic,
  title={Dynamic hyperbolic attention network for fine hand-object reconstruction},
  author={Leng, Zhiying and Wu, Shun-Cheng and Saleh, Mahdi and Montanaro, Antonio and Yu, Hao and Wang, Yin and Navab, Nassir and Liang, Xiaohui and Tombari, Federico},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{zhang2024graspxl,
  title={Graspxl: Generating grasping motions for diverse objects at scale},
  author={Zhang, Hui and Christen, Sammy and Fan, Zicong and Hilliges, Otmar and Song, Jie},
  booktitle={ECCV},
  year={2024}
}

@inproceedings{prakash20243d,
  title={3D Reconstruction of Objects in Hands Without Real World 3D Supervision},
  author={Prakash, Aditya and Chang, Matthew and Jin, Matthew and Tu, Ruisen and Gupta, Saurabh},
  booktitle={ECCV},
  year={2024}
}

@article{zhu2023get,
  title={Get a grip: Reconstructing hand-object stable grasps in egocentric videos},
  author={Zhu, Zhifan and Damen, Dima},
  journal={arXiv:2312.15719},
  year={2023}
}

@inproceedings{goletto2024amego,
  title={AMEGO: Active Memory from long EGOcentric videos},
  author={Goletto, Gabriele and Nagarajan, Tushar and Averta, Giuseppe and Damen, Dima},
  booktitle={ECCV},
  year={2024},
}

@article{huang2024intercap,
  title={InterCap: joint markerless 3D tracking of humans and objects in interaction from multi-view RGB-D images},
  author={Huang, Yinghao and Taheri, Omid and Black, Michael J and Tzionas, Dimitrios},
  journal={IJCV},
  year={2024}
}

@inproceedings{xie2022chore,
  title={Chore: Contact, human and object reconstruction from a single rgb image},
  author={Xie, Xianghui and Bhatnagar, Bharat Lal and Pons-Moll, Gerard},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{xu2023interdiff,
  title={Interdiff: Generating 3d human-object interactions with physics-informed diffusion},
  author={Xu, Sirui and Li, Zhengyuan and Wang, Yu-Xiong and Gui, Liang-Yan},
  booktitle={CVPR},
  year={2023}
}

@article{xu2025interdreamer,
  title={Interdreamer: Zero-shot text to 3d dynamic human-object interaction},
  author={Xu, Sirui and Wang, Yu-Xiong and Gui, Liangyan and others},
  journal={NeurIPS},
  year={2025}
}

@inproceedings{nam2024joint,
  title={Joint reconstruction of 3d human and object via contact-based refinement transformer},
  author={Nam, Hyeongjin and Jung, Daniel Sungho and Moon, Gyeongsik and Lee, Kyoung Mu},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{jiang2023full,
  title={Full-body articulated human-object interaction},
  author={Jiang, Nan and Liu, Tengyu and Cao, Zhexuan and Cui, Jieming and Zhang, Zhiyuan and Chen, Yixin and Wang, He and Zhu, Yixin and Huang, Siyuan},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{liu2023contactgen,
  title={Contactgen: Generative contact modeling for grasp generation},
  author={Liu, Shaowei and Zhou, Yang and Yang, Jimei and Gupta, Saurabh and Wang, Shenlong},
  booktitle={ICCV},
  year={2023}
}


@inproceedings{fieraru2020three,
  title={Three-dimensional reconstruction of human interactions},
  author={Fieraru, Mihai and Zanfir, Mihai and Oneata, Elisabeta and Popa, Alin-Ionut and Olaru, Vlad and Sminchisescu, Cristian},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{huang2022capturing,
  title={Capturing and inferring dense full-body human-scene contact},
  author={Huang, Chun-Hao P and Yi, Hongwei and H{\"o}schle, Markus and Safroshkin, Matvey and Alexiadis, Tsvetelina and Polikovsky, Senya and Scharstein, Daniel and Black, Michael J},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{yin2023hi4d,
  title={Hi4d: 4d instance segmentation of close human interaction},
  author={Yin, Yifei and Guo, Chen and Kaufmann, Manuel and Zarate, Juan Jose and Song, Jie and Hilliges, Otmar},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{huang2024closely,
  title={Closely interactive human reconstruction with proxemics and physics-guided adaption},
  author={Huang, Buzhen and Li, Chen and Xu, Chongyang and Pan, Liang and Wang, Yangang and Lee, Gim Hee},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{fieraru2021learning,
  title={Learning complex 3D human self-contact},
  author={Fieraru, Mihai and Zanfir, Mihai and Oneata, Elisabeta and Popa, Alin-Ionut and Olaru, Vlad and Sminchisescu, Cristian},
  booktitle={AAAI},
  year={2021}
}

@inproceedings{muller2021self,
  title={On self-contact and human pose},
  author={Muller, Lea and Osman, Ahmed AA and Tang, Siyu and Huang, Chun-Hao P and Black, Michael J},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{zhang2020perceiving,
  title={Perceiving 3d human-object spatial arrangements from a single image in the wild},
  author={Zhang, Jason Y and Pepose, Sam and Joo, Hanbyul and Ramanan, Deva and Malik, Jitendra and Kanazawa, Angjoo},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{mo2021where2act,
  title={Where2act: From pixels to actions for articulated 3d objects},
  author={Mo, Kaichun and Guibas, Leonidas J and Mukadam, Mustafa and Gupta, Abhinav and Tulsiani, Shubham},
  booktitle={ICCV},
  year={2021}
}

@article{zhai2024background,
  title={Background activation suppression for weakly supervised object localization and semantic segmentation},
  author={Zhai, Wei and Wu, Pingyu and Zhu, Kai and Cao, Yang and Wu, Feng and Zha, Zheng-Jun},
  journal={IJCV},
  year={2024}
}

@inproceedings{wray2019learning,
  title={Learning Visual Actions Using Multiple Verb-Only Labels},
  author={Wray, Michael and Damen, Dima},
  booktitle={BMVC},
  year={2019}
}

@inproceedings{kim2022action,
  title={An Action Is Worth Multiple Words: Handling Ambiguity in Action Recognition},
  author={Kim, Kiyoon and Moltisanti, Davide and Mac Aodha, Oisin and Sevilla-Lara, Laura},
  booktitle={BMVC},
  year={2022}
}

@inproceedings{moltisanti2017trespassing,
  title={Trespassing the boundaries: Labeling temporal bounds for object interactions in egocentric video},
  author={Moltisanti, Davide and Wray, Michael and Mayol-Cuevas, Walterio and Damen, Dima},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{sigurdsson2017actions,
  title={What actions are needed for understanding human actions in videos?},
  author={Sigurdsson, Gunnar A and Russakovsky, Olga and Gupta, Abhinav},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{khamis2015walking,
  title={Walking and talking: A bilinear approach to multi-label action recognition},
  author={Khamis, Sameh and Davis, Larry S},
  booktitle={CVPRws},
  year={2015}
}

@article{zhu2024video,
  title={Video Editing for Video Retrieval},
  author={Zhu, Bin and Flanagan, Kevin and Fragomeni, Adriano and Wray, Michael and Damen, Dima},
  journal={arXiv:2402.02335},
  year={2024}
}

@inproceedings{rahaman2022generalized,
  title={A generalized and robust framework for timestamp supervision in temporal action segmentation},
  author={Rahaman, Rahul and Singhania, Dipika and Thiery, Alexandre and Yao, Angela},
  booktitle={ECCV},
  year={2022}
}

@article{itti2002model,
  title={A model of saliency-based visual attention for rapid scene analysis},
  author={Itti, Laurent and Koch, Christof and Niebur, Ernst},
  journal={IEEE TPAMI},
  year={2002},
}

@article{borji2012state,
  title={State-of-the-art in visual attention modeling},
  author={Borji, Ali and Itti, Laurent},
  journal={IEEE TPAMI},
  year={2012},
}

@article{hou2011image,
  title={Image signature: Highlighting sparse salient regions},
  author={Hou, Xiaodi and Harel, Jonathan and Koch, Christof},
  journal={IEEE TPAMI},
  year={2011},
}

@article{harel2006graph,
  title={Graph-based visual saliency},
  author={Harel, Jonathan and Koch, Christof and Perona, Pietro},
  journal={NeurIPS},
  year={2006}
}

@inproceedings{judd2009learning,
  title={Learning to predict where humans look},
  author={Judd, Tilke and Ehinger, Krista and Durand, Fr{\'e}do and Torralba, Antonio},
  booktitle={ICCV},
  year={2009},
}

@article{torralba2006contextual,
  title={Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search.},
  author={Torralba, Antonio and Oliva, Aude and Castelhano, Monica S and Henderson, John M},
  journal={Psychological review},
  year={2006},
}

@inproceedings{borji2012probabilistic,
  title={Probabilistic learning of task-specific visual attention},
  author={Borji, Ali and Sihite, Dicky N and Itti, Laurent},
  booktitle={CVPR},
  year={2012},
}

@inproceedings{fathi2012learning,
  title={Learning to recognize daily actions using gaze},
  author={Fathi, Alireza and Li, Yin and Rehg, James M},
  booktitle={ECCV},
  year={2012},
}


@article{land2004coordination,
  title={The coordination of rotations of the eyes, head and trunk in saccadic turns produced in natural situations},
  author={Land, Michael F},
  journal={Experimental brain research},
  year={2004},
}

@article{land2001ways,
  title={In what ways do eye movements contribute to everyday activities?},
  author={Land, Michael F and Hayhoe, Mary},
  journal={Vision research},
  year={2001},
}

@article{nystrom2010adaptive,
  title={An adaptive algorithm for fixation, saccade, and glissade detection in eyetracking data},
  author={Nystr{\"o}m, Marcus and Holmqvist, Kenneth},
  journal={Behavior research methods},
  year={2010},
}

@article{pelz2001coordination,
  title={The coordination of eye, head, and hand movements in a natural task},
  author={Pelz, Jeff and Hayhoe, Mary and Loeber, Russ},
  journal={Experimental brain research},
  year={2001},
}

@inproceedings{xu2018gaze,
  title={Gaze prediction in dynamic 360 immersive videos},
  author={Xu, Yanyu and Dong, Yanbing and Wu, Junru and Sun, Zhengzhong and Shi, Zhiru and Yu, Jingyi and Gao, Shenghua},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{zhang2017deep,
  title={Deep future gaze: Gaze anticipation on egocentric videos using adversarial networks},
  author={Zhang, Mengmi and Teck Ma, Keng and Hwee Lim, Joo and Zhao, Qi and Feng, Jiashi},
  booktitle={CVPR},
  year={2017}
}

@article{huang2020mutual,
  title={Mutual context network for jointly estimating egocentric gaze and action},
  author={Huang, Yifei and Cai, Minjie and Li, Zhenqiang and Lu, Feng and Sato, Yoichi},
  journal={IEEE TIP},
  year={2020}
}

@inproceedings{tavakoli2019digging,
  title={Digging deeper into egocentric gaze prediction},
  author={Tavakoli, Hamed Rezazadegan and Rahtu, Esa and Kannala, Juho and Borji, Ali},
  booktitle={WACV},
  year={2019},
}

@inproceedings{huang2024egoexolearn,
  title={Egoexolearn: A dataset for bridging asynchronous ego-and exo-centric view of procedural activities in real world},
  author={Huang, Yifei and Chen, Guo and Xu, Jilan and Zhang, Mingfang and Yang, Lijin and Pei, Baoqi and Zhang, Hongjie and Dong, Lu and Wang, Yali and Wang, Limin and others},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{liu2021goal,
  title={Goal-oriented gaze estimation for zero-shot learning},
  author={Liu, Yang and Zhou, Lei and Bai, Xiao and Huang, Yifei and Gu, Lin and Zhou, Jun and Harada, Tatsuya},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{li2018eye,
  title={In the eye of beholder: Joint learning of gaze and actions in first person video},
  author={Li, Yin and Liu, Miao and Rehg, James M},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{min2021integrating,
  title={Integrating human gaze into attention for egocentric activity recognition},
  author={Min, Kyle and Corso, Jason J},
  booktitle={WACV},
  year={2021}
}

@article{marinjimenez2021laeonet,
  title={LAEO-Net++: Revisiting People Looking at Each Other in Videos},
  author={Marín-Jim{\'e}nez, Manuel J and Kalogeiton, Vicky and Medina-Suárez, Pablo and Zisserman, Andrew},
  journal={IEEE TPAMI},
  year={2021}
}

@article{damen2024youdo,
  title={You-Do, I-Learn: Egocentric unsupervised discovery of objects and their modes of interaction towards video-based guidance},
  author={Damen, Dima and Leelasawassuk, Teesid and Mayol-Cuevas,  
Walterio},
  journal={CVIU},
  year={2016}
}

@inproceedings{huang2018predicting,
  title={Predicting gaze in egocentric video by learning task-dependent attention transition},
  author={Huang, Yifei and Cai, Minjie and Li, Zhenqiang and Sato, Yoichi},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{lai2024listen,
  title={Listen to Look Into the Future: Audio-Visual Egocentric Gaze Anticipation},
  author={Lai, Bolin and Ryan, Fiona and Jia, Wenqi and Liu, Miao and Rehg, James M},
  booktitle={ECCV},
  year={2024}
}

@article{lai2024intheeye,
  title={In the Eye of Transformer: Global–Local Correlation for Egocentric Gaze Estimation and Beyond},
  author={Lai, Bolin and Liu, Miao and Ryan, Fiona and Rehg, James M},
  journal={IJCV},
  year={2024}
}

@inproceedings{xu2015gazeenabled,
  title={Gaze-enabled egocentric video summarization via constrained submodular maximization},
  author={Xu, Jia and Mukherjee, Lopamudra and Li, Yin and Warner, Jamieson and Rehg, James M and Singh, Vikas},
  booktitle={CVPR},
  year={2015}
}

@inproceedings{zhang2017deepfuture,
  title={Deep Future Gaze: Gaze Anticipation on Egocentric Videos Using Adversarial Networks},
  author={Zhang, Mengmiand Ma, Keng Teck and Lim, Joo Hwee and Zhao, Qi and Feng, Jiashi},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{tafasca2024sharingan,
  title={Sharingan: A Transformer Architecture for Multi-Person Gaze Following},
  author={Tafasca, Samy and Gupta, Anshul and Odobez, Jean-Marc},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{chong2020detecting,
  title={Detecting Attended Visual Targets in Video},
  author={Chong, Eunji and Wang, Yongxin and Ruiz, Nataniel and Rehg, James M},
  booktitle={CVPR},
  year={2020}
}

@article{vickersadvances,
  title={Advances in coupling perception and action: The quiet eye as a bidirectional link between gaze, attention, and action},
  author={Vickers, Joan N},
  journal={Progress in Brain Research},
  year={2009}
}

@inproceedings{mazzamuto2025gazing,
  title={Gazing Into Missteps: Leveraging Eye-Gaze for Unsupervised Mistake Detection in Egocentric Videos of Skilled Human Activities},
  author={Mazzamuto, Michele and Furnari, Antonino and Sato, Yoichi and Farinella, Giovanni Maria},
  booktitle={CVPR},
  year={2025}
}

@article{lu2019deepattention,
  title={Deep Attention Network for Egocentric Action Recognition},
  author={Lu, Minlong and Li, Ze-Nian and Wang, Yueming and Pan, Gang},
  journal={IEEE TIP},
  year={2019}
}

@inproceedings{recasens2017following,
  title={Following Gaze in Video},
  author={Recasens, Adri\{à} and Vondrick, Carl and Khosla, Aditya and Torralba, Antonio},
  booktitle={CVPR},
  year={2017}
}

@article{chong2020detection,
  title={Detection of eye contact with deep neural networks is as accurate as human experts},
  author={Chong, Eunji and Clark-Whitney, Elysha and Southerland, Audrey and Stubbs, Elizabeth and Miller, Chanel and Ajodan, Eliana L and Silverman, Melanie R and Lord, Catherine and Rozga, Agata and Jones, Rebecca M and Rehg, James M},
  journal={Nature Communications},
  year={2020}
}

@inproceedings{hu2024tcnet,
  title={TCNet: Continuous Sign Language Recognition from Trajectories and Correlated Regions},
  author={Lu, Hui and Poppe, Ronald and Salah, Albert Ali},
  booktitle={ECCV},
  year={2024}
}
