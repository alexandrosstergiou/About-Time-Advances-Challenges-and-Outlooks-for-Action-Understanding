\section{Modeling actions in videos}
\label{sec:modeling}

% section intro
%Videos include changes over time to spatial information such as visual aspects of objects, background information, or the visual context of scenes.
In this section, we define two general groups of approaches for encoding videos without explicitly relating them to tasks. Approaches discussed in~\Cref{sec:modeling::separate} model spatial and temporal information separately, while works overviewed in~\Cref{sec:modeling::joint} use joint spatiotemporal representations. We start with a brief characterization of key challenges.

\subsection{Challenges in action representation}
\label{sec:modeling::challenges}
The diversity of the video input poses several challenges. \textbf{Intra-class variations} in the visual appearance of actions across videos can be due to viewpoint, performance, occlusions, background noise, or lighting conditions. The performances and durations of actions can also significantly deviate. Such variations appear across datasets of varying size \citep{grauman2022ego4d,kay2017kinetics,miech2019howto100m,soomro2012ucf101}. In the learned semantics, training/test set instance distribution variances can significantly impact the performance and overall generalization of methods. Challenging action instances can be traced to feature representations that are further from the training set distribution.

Since action understanding tasks are increasingly semantic, we also face challenges in the diversity and granularity of the target outputs. Interpretation of the visual input, and sometimes the lack of specific information, increasingly requires higher-level understanding. Consequently, the relation between visual input and model output necessarily becomes more complex. \textbf{Vocabulary limitations} present challenges as action categories are often finite. The generalization of models in both \emph{open-set} or \emph{cross-domain} settings primarily depends on the similarity between seen and unseen instances. Limited \textit{inter-class variation} of actions further affects good representation performance of rare coarse-grained concepts of visually similar actions. This issue is more prevalent for tasks requiring more fine-grained semantic granularities.


\subsection{Separating visual and temporal information}
\label{sec:modeling::separate}

\noindent
\textbf{Tracking and template matching}. Early works \citep{bobick2001recognition} have introduced template matching approaches to determine spatial and temporal locations where motions occur. Template-based methods have also been explored for local patches \citep{shechtman2005space}, through correlation filters \citep{rodriguez2008action}, or based on voxel similarity \citep{ke2007spatio}. A different research direction has considered the discovery of temporal patterns by directly tracking visual features over time \citep{cipolla1990dynamic,isard1998condensation,rohr1994towards}. 


\noindent
\textbf{Local descriptors}. Template-based approaches can capture the general essence of actions. However, they provide limited flexibility in representing varying degrees of noise, partial occlusions, and viewpoint variations. Instead, motivated by the observations that actions can be characterized by appearance changes over time, approaches have been designed to associate changes in local features with actions. \citet{mikolajczyk2008action} clustered local features to tree representations and related them to action categories. Similarly, pose-based primitives \citep{thurau2008pose}, temporal bins \citep{nowozin2007discriminative}, pictorial structures \citep{tran2012part}, and graphical structures of the actions \citep{ni2014multiple} have been used with local descriptor features. Other approaches \citep{gupta2009observing,yao2010modeling} cast action recognition as a structural connectivity task by recognizing parts of objects and understanding actions through pose.
Several methods have extended this notion to individual regions \citep{ikizler2010object}, poselet clusters \citep{pishchulin2013strong}, decision trees \citep{rahmani2014real}, and covariance matrices \citep{kviatkovsky2014online}.


% CNN-based action recognition
\noindent
\textbf{Spatial convolutions}. Convolutions can efficiently extract local patterns from visual inputs. An early application of Convolutional Neural Networks (CNNs) to video \citep{karpathy2014large} temporally fused spatial frame embeddings from the first few or final layers. Others explored the factorization of frame embeddings \citep{sun2015human}, frame ranking \citep{fernando2015modeling}, pooling \citep{fernando2016rank}, salient region focus \citep{girdhar2017attentional, zong2021motion}, and relation reasoning between neighboring frames \citep{zhou2018temporal}. \citet{le2011learning} spatially convolved videos over combinations of the spatial and temporal dimensions. Seminal efforts focused on single volumes to represent motion \citep{bilen2016dynamic,chung2016signs,iosifidis2012view} or learned the correlation and exclusion between action classes \citep{hoai2015improving}. \citet{tran2018closer} proposed convolutional blocks based on spatial and temporal kernels to create more efficient video models. To reduce feature redundancies over densely sampled frames, \citet{lin2019tsm} proposed shifting features at subsequent frames to model actions in both online and offline settings with later adaptations also including conditional gates \citep{sudhakaran2020gate}.

% RNNs over image CNNs
\noindent
\textbf{Temporal recursion}. A parallel line of research has focused on extracting motion patterns with recurrent layers \citep{ballas2015delving,dwibedi2018temporal,kapidis2019multitask,perrett2019ddlstm,yue2015beyond,ullah2017action}, from static frame features from spatial CNNs. Several works have jointly encoded frame features and learned changes in appearance over time with LSTMs \citep{donahue2015long,srivastava2015unsupervised}. Similarly, for multi-actor action recognition, \citet{wang2017recurrent} used three individual pathways with LSTMs for person action, group action, and scene recognition.


% Two-stream CNNs
\noindent
\textbf{Two-stream models}. An alternative set of approaches has included a parallel motion-specific stream in spatial CNNs. Two-stream models \citep{simonyan2014two} encode motion explicitly through an optical flow stream and appearance information through an RGB stream over stacks of frames. Extensions \citep{feichtenhofer2016convolutional} also fused flow and spatial streams at intermediate layers. Other approaches used cross-stream connections \citep{feichtenhofer2017spatiotemporal}, multiple appearance streams \citep{tu2018multistream}, recurrent layers \citep{singh2016multi}, or concatenated appearance and motion volumes \citep{jain2015modeep,wang2017spatiotemporal} to share information between motion and appearance streams. \citet{wang2016temporal} used a step-based approach of segmenting videos into individual snippets, processing them in parallel, and fusing class scores from each snippet. Improvements in inference speeds of two-stream models have also been reported with the addition of motion vectors \citep{zhang2016real} or key volume mining \citep{zhu2016key}. Although such approaches have included a new research direction in modeling videos, the representation of motion with precomputed motion features limits the capabilities of learned backbones. \citet{sevilla2019integration} empirically showed that one of the main limitations in optical flow representations is capturing accurate movements near the edges of objects.

\subsection{Jointly encoding space time}
\label{sec:modeling::joint}

% Intro paragraph
\looseness-1 Time and appearance can also be encoded jointly. 

% hand-coded approaches for space-time (part-based)
\noindent
\textbf{Part-based representations}. SpatioTemporal Interest Points (STIPs) \citep{laptev2003space} extended spatial interest point detection methods \citep{forstner1987fast,harris1988combined} to the video domain. STIP features have also been quantized in histograms of codewords \citep{schuldt2004recognizing}. \citet {liu2008learning,oikonomopoulos2005spatiotemporal} explored salient points based on peaks of activity variation. Several approaches have further studied action-relevant temporal locations across viewpoints \citep{yilmaz2006matching} and view-invariant trajectories \citep{sheikh2005exploring}. \citet{dollar2005behavior} proposed modeling periodic motions using sparse distributions of points of interest. This feature extractor prompted subsequent works \citep{niebles2008unsupervised} with actions classified through a codebook of features.

% hand-coded approaches for space-time (holistic)
\noindent
\textbf{Holistic stochastic representations}. Actions have also been modeled based on global information. \citet{efros2003recognizing} created representations for different body parts and regressed towards representations of pre-classified actions. Subsequent works have explored action descriptors focused on object shapes \citep{gorelick2006shape,jia2008human}, movements \citep{sun2009action}, spatiotemporal salient regions \citep{wong2007extracting}, and extended existing approaches over multiple features and temporal scales \citep{amer2012sum,liu2008recognizing,zelnik2001event,yang2020temporal}. Later works \citep{blank2005actions} adapted and generalized holistic descriptors \citep{gorelick2006shape} by concatenating 2D silhouettes to form space-time shapes corresponding to action performances. \citet{sadanand2012action} similarly proposed a bank of volumetrically pooled features containing high-level representations of the actions. 
%which were then classified.


% 3D CNNs
\noindent
\textbf{3D CNNs}. Orthogonal to hand-crafted features, 2D convolutions have been extended in various ways to 3D spatiotemporal kernels to jointly encode space and time \citep{baccouche2011sequential,ji20123d,taylor2010convolutional,tran2015learning}. Subsequent works have demonstrated the potential of adapting image models to video \citep{hara2018can}, explored video-specific architectures with distinct spatiotemporal volumes across channels \citep{chen2018multi}, and tiled 3D kernels \citep{hegde2018morph}. They have also used channel-separated convolutions \citep{jiang2019stm,luo2019grouped,tran2019video}, temporal residual connections \citep{qiu2017learning}, global feature fusion \citep{qiu2019learning}, resolution reduction \citep{chen2019drop,stergiou2021multi}, or related appearance to spatiotemporal embeddings \citep{wang2018appearance,zhou2018mict}. \citet{carreira2017quo} integrated 3D convolutions into two-stream models for motion-implicit appearance representations in the RGB stream and motion-explicit representations in the optical flow stream. Several works have also aimed to improve the efficiency of action recognition architectures \citep{feichtenhofer2020x3d,kondratyuk2021movinets,liu2022convnet}. They have also used visual context from the scenes of actions, by either scene-type objectives \citep{choi2019can}, decoupling scene and motion features \citep{wang2021enhancing}, multi-domain information concatenation \citep{plizzari2023can,kapidis2021multi}, or by fusing motion and scene information \citep{stergiou2021learn}. To better extract temporal information, \citet{feichtenhofer2019slowfast} proposed a dual pathway video model with a slow pathway operating over low frame rates for spatial semantics and a fast pathway with a high frame rate for motion. Similarly, \citet{wang2020self} included a contrastive objective for learning the pace in videos. \citet{xu2019self} explored temporal reasoning by including clip order prediction as an additional task to improve action recognition. The extension of 3D CNNs to longer sequences by segmenting videos with multiple temporal patches has also been attempted \citep{ji2020action,hussein2019timeception,varol2017long}.


% Action recognition with attention
\noindent
\textbf{Spatiotemporal attention}. Attention is an effective approach for learning feature correspondences over space and time. \citet{sharma2015action} used visual attention to localize action regions from CNN features with recurrent layers. \citet{du2017recurrent} attended over spatial features across multiple frames based on their relevance to the action. Similarly, \citet{chen20182} aggregated and propagated global information by attending over convolution features. \citet{wang2018non} introduced non-local operations with bi-directional attention blocks over convolutions. Another early application of attention \citep{girdhar2019video} was based on region proposals and the creation of feature banks \citep{wu2019long} in longer videos. The introduction of vision transformers \citep{dosovitskiy2020image} that encode visual information through region-based tokenization led to video-based adaptations that explored different spatiotemporal attention configurations \citep{arnab2021vivit,bertasius2021space}. Others have explored token selection \citep{bulat2021space,ryoo2021tokenlearner,zha2021shifted}, and the inclusion of contextual information \citep{kim2021relational}. \citet{liu2022video} introduced shifted non-overlapping attention windows to share information across patches. Feature hierarchies and latent resolution reductions have led to more compute- \citep{fan2021multiscale,li2022mvitv2} and memory- \citep{wu2022memvit} efficient architectures. Recent models such as MViT \citep{yan2022multiview}, Hiera \citep{ryali2023hiera}, and UniFormer \citep{li2022uniformer} improved both performance and capacities of video models. SSL has also shown great promise with pre-text tasks based on contrastive learning \citep{chen2020simple} or token masking \citep{he2022masked}. \citet{xing2023svformer} increased the complexity of the contrastive objective with pseudo labels and token mixing from different inputs, while masked autoencoders have also been extended to video data \citep{feichtenhofer2022masked,wei2022masked}. Subsequent works have explored adaptive token masking \citep{bandara2023adamae}, double masking on both the encoder and decoder \citep{wang2023videomae}, fusion of tokens \citep{kim2024token}, and teacher-student masked autoencoders \citep{wang2023masked}.

% VLM
\noindent
\textbf{Video-language models}. Recently, language semantics captured by Large Language Models (LLMs) \citep{brown2020language,touvron2023llama} have been used as a supervisory signal for vision tasks \citep{li2023blip,liu2024visual,radford2021learning}. Initial efforts \citep{zellers2021merlot} matched frame-level encodings to corresponding LLM embeddings of captions. Other approaches have optimized image-based encodings over frames by pooling spatial tokens \citep{yu2022coca}, cross-modal skip connections \citep{xu2023mplug}, cross-attending over modalities \citep{alayrac2022flamingo}, and jointly attending visual and text embeddings \citep{maaz2023video}. As static features provide only an appearance-based view, works have also used spatiotemporal vision-language encoders \citep{piergiovanni2024mirasol3b} or extended the training objective \citep{lu2024enhancing,zhao2024videoprism} to a two-step SSL pre-training with video to text alignment and video masking.