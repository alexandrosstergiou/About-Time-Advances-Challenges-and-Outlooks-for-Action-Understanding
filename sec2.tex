\section{Modeling actions in videos}
\label{sec:modeling}

% section intro
In this section, we define two general groups of approaches for encoding videos without explicitly relating them to tasks. We start with characterizing key challenges in \Cref{sec:modeling::challenges}. Approaches discussed in~\Cref{sec:modeling::separate} model spatial and temporal information separately, while works overviewed in~\Cref{sec:modeling::joint} use joint spatiotemporal representations. 

\subsection{Challenges in action representation}
\label{sec:modeling::challenges}

The diversity of the video input poses several challenges. \textbf{Intra-class variations} in the visual appearance of actions of the same category across videos can be due to viewpoint, occlusions, background noise, or lighting conditions. The performances and durations of actions can also significantly deviate. Such variations appear across datasets  \pcite{grauman2022ego4d,kay2017kinetics,miech2019howto100m,soomro2012ucf101}. Training/test set instance distribution variance can also significantly impact the performance and overall generalization of the learned semantics. Challenging action instances can be traced to feature representations further from the training set distribution in such cases.

Since action understanding tasks are increasingly semantic, we also face challenges in the diversity and granularity of the target outputs. Interpretation of the visual input, and sometimes the lack of observable information, increasingly requires higher-level understanding. Consequently, the relation between visual input and model output becomes more complex. \textbf{Vocabulary limitations} present challenges as action categories are often finite. The generalization of models to \emph{open-set} or \emph{cross-domain} settings primarily depends on the similarity between seen and unseen instances. Limited \textit{inter-class variation} further affects good representation performance of rare coarse-grained concepts of visually similar actions. This issue is more prevalent for tasks that require fine-grained semantic granularities.


\subsection{Separating visual and temporal information}
\label{sec:modeling::separate}
We first discuss approaches that process visual and temporal information independently. 

\noindent
\textbf{Tracking and template matching}. Early works \pcite{bobick2001recognition} have applied template matching to spatially and temporally localize motions. These approaches relied on view-specific representations of movements, in the form of templates, to capture underlying motion similarity across action instances. Templates have been explored through local patches \pcite{shechtman2005space}, correlation filters \pcite{rodriguez2008action}, and voxels \pcite{ke2007spatio}. Another line of research has considered temporal pattern discovery by directly tracking visual features over time \pcite{cipolla1990dynamic,isard1998condensation,rohr1994towards}. Template approaches have relied on assumptions such as static backgrounds, fixed camera views, and linear motions that limit the exploration of intra-class variability.


\noindent
\textbf{Local descriptors}. Motivated by the observation that actions can be characterized through appearance changes over time, a set of approaches aims to associate per-frame changes from local descriptor features to action categories. Pose primitives \pcite{thurau2008pose}, temporal bins \pcite{nowozin2007discriminative}, pictorial structures \pcite{tran2012part}, and graphical structures of the actions \pcite{ni2014multiple} have been explored as descriptors for local action features. \tcite{mikolajczyk2008action} clustered an ensemble of local features to tree representations and related them to action categories. Other approaches \pcite{gupta2009observing,yao2010modeling} cast action recognition as a two-step structural connectivity task by recognizing parts of objects and understanding actions through pose.
Several methods have extended this notion to individual regions \pcite{ikizler2010object}, poselet clusters \pcite{pishchulin2013strong}, decision trees \pcite{rahmani2014real}, and covariance matrices \pcite{kviatkovsky2014online}.

% CNN-based action recognition
\noindent
\textbf{Spatial convolutions}. Convolutions can efficiently extract local patterns from visual inputs. An early application of Convolutional Neural Networks (CNNs) to video \pcite{karpathy2014large} temporally fused spatial frame embeddings over pre-defined sets of layers. Others explored the factorization of frame embeddings \pcite{sun2015human}, frame ranking \pcite{fernando2015modeling}, pooling \pcite{fernando2016rank}, salient region focus \pcite{girdhar2017attentional, zong2021motion}, and relation reasoning between neighboring frames \pcite{zhou2018temporal}. \tcite{le2011learning} spatially convolved videos over combinations of the spatial and temporal dimensions. Seminal efforts focused on single volumes to represent motion \pcite{bilen2016dynamic,chung2016signs,iosifidis2012view} or learned the correlation and exclusion between action classes \pcite{hoai2015improving}. \tcite{tran2018closer} proposed convolutional blocks based on spatial (2D) and temporal (1D) kernels to create more efficient video models. \tcite{lin2019tsm} reduced redundancies by shifting features at subsequent frames, while later adaptations also included conditional gates \pcite{sudhakaran2020gate}.

% RNNs over image CNNs
\noindent
\textbf{Temporal recursion}. A parallel line of research has focused on extracting motion patterns with recurrent layers \pcite{ballas2015delving,dwibedi2018temporal,perrett2019ddlstm,yue2015beyond,ullah2017action}, from the static frame features of spatial CNNs. Several works have jointly encoded frame features and learned changes in appearance over time with Convolutional LSTMs \pcite{donahue2015long,srivastava2015unsupervised}. Similarly, for multi-actor action recognition, \tcite{wang2017recurrent} used three individual pathways with LSTMs for person action, group action, and scene recognition.

% Two-stream CNNs
\noindent
\textbf{Two-stream models}. An alternative group of approaches included a parallel motion-specific stream in spatial CNNs. Two-stream models \pcite{simonyan2014two} encode motion and appearance explicitly with respective optical flow and RGB streams over stacks of frames. Extensions \pcite{feichtenhofer2016convolutional} have fused flow and spatial streams at intermediate layers while other approaches used cross-stream connections \pcite{feichtenhofer2017spatiotemporal}, multiple appearance streams \pcite{tu2018multistream}, recurrent layers \pcite{singh2016multi}, or concatenated appearance and motion volumes \pcite{jain2015modeep,wang2017spatiotemporal} to share information between the streams. \tcite{wang2016temporal} used a step-based approach that segmented videos into individual snippets, processed them in parallel, and fused class scores from each snippet. Improvements in inference speeds of two-stream models have been achieved with the addition of motion vectors \pcite{zhang2016real} or key volume mining \pcite{zhu2016key}. Although such approaches have established a new research direction in modeling videos, the representation of motion with precomputed motion features limits the capabilities of learned backbones \pcite{sevilla2019integration}. 


\subsection{Jointly encoding space and time}
\label{sec:modeling::joint}

% Intro
\looseness-1 Time and appearance can also be encoded jointly. 

% hand-coded approaches for space-time (part-based)
\noindent
\textbf{Part-based representations}. SpatioTemporal Interest Points (STIPs) \pcite{laptev2003space} extended spatial interest point detection methods \pcite{forstner1987fast,harris1988combined} to the video domain. \tcite {liu2008learning,oikonomopoulos2005spatiotemporal} explored salient points based on peaks of activity variation. STIP features have been quantized in histograms of codewords \pcite{schuldt2004recognizing}. Several approaches have studied action-relevant temporal locations across viewpoints \pcite{yilmaz2006matching} and view-invariant trajectories \pcite{sheikh2005exploring}. \tcite{dollar2005behavior} proposed modeling periodic motions using sparse distributions of points of interest. This feature extractor prompted subsequent works \pcite{niebles2008unsupervised} with actions classified through a codebook of features.

% hand-coded approaches for space-time (holistic)
\noindent
\textbf{Holistic stochastic representations}. Actions have also been modeled based on global information. \tcite{efros2003recognizing} created representations for different body parts and regressed towards representations of pre-classified actions. Subsequent works have explored action descriptors focused on object shapes \pcite{gorelick2006shape,jia2008human}, movements \pcite{sun2009action}, and spatiotemporal salient regions \pcite{wong2007extracting}. They have also extended existing approaches to multiple features and temporal scales \pcite{amer2012sum,liu2008recognizing,zelnik2001event,yang2020temporal}. Later works \pcite{blank2005actions} adapted and generalized holistic descriptors \pcite{gorelick2006shape} by concatenating 2D silhouettes to form space-time shapes corresponding to action performances. \tcite{sadanand2012action} similarly proposed a bank of volumetrically pooled features containing high-level representations of the actions. 

% 3D CNNs
\noindent
\textbf{3D CNNs}. Orthogonal to hand-crafted features, 2D convolutions have been extended in various ways to 3D spatiotemporal kernels to jointly encode space and time \pcite{baccouche2011sequential,ji20123d,taylor2010convolutional,tran2015learning}. Subsequent works have demonstrated the potential of adapting image models to video \pcite{hara2018can}, explored video-specific architectures with spatiotemporal volumes across channels \pcite{chen2018multi}, and tiled 3D kernels \pcite{hegde2018morph}. They have also used channel-separated convolutions \pcite{jiang2019stm,luo2019grouped,tran2019video}, temporal residual connections \pcite{qiu2017learning}, global feature fusion \pcite{qiu2019learning}, resolution reduction \pcite{chen2019drop,stergiou2021multi}, and related appearance to spatiotemporal embeddings \pcite{wang2018appearance,zhou2018mict}. \tcite{carreira2017quo} integrated 3D convolutions into two-stream models for motion-implicit appearance representations in the RGB stream and motion-explicit representations in the optical flow stream. Several works have focused on improving the efficiency of action recognition architectures \pcite{feichtenhofer2020x3d,kondratyuk2021movinets,liu2022convnet}. They have used visual context from the scenes of actions, by either scene-type objectives \pcite{choi2019can}, decoupling scene and motion features \pcite{wang2021enhancing}, multi-domain information concatenation \pcite{kapidis2023multi}, or by fusing motion and scene information \pcite{stergiou2021learn}. To better extract temporal information, \tcite{feichtenhofer2019slowfast} proposed a dual pathway video model with a slow pathway operating over low frame rates for spatial semantics and a fast pathway with a high frame rate for motion. Similarly, \tcite{wang2020self} included a contrastive objective for learning the pace in videos. \tcite{xu2019self} explored temporal reasoning by including clip order prediction as an additional task to improve action recognition. The extension of 3D CNNs to longer sequences by segmenting videos with multiple temporal patches has also been attempted \pcite{ji2020action,hussein2019timeception,varol2017long}.

% Action recognition with attention
\noindent
\textbf{Spatiotemporal attention}. Attention is an effective approach for learning feature correspondences over space and time. \tcite{sharma2015action} used visual attention to localize action regions from CNN features with recurrent layers. \tcite{du2017recurrent} attended over spatial features across multiple frames based on their relevance to the action. Similarly, \tcite{chen20182} aggregated and propagated global information by attending over convolution features. \tcite{wang2018non} introduced non-local operations with bi-directional attention blocks over convolutions. Another early application of attention \pcite{girdhar2019video} was based on region proposals and the creation of feature banks \pcite{wu2019long} in longer videos. The introduction of Vision Transformers (ViTs) \pcite{dosovitskiy2020image} that encode visual information through region-based tokenization led to video-based adaptations that explored different spatiotemporal attention configurations \pcite{arnab2021vivit,bertasius2021space}. Others have explored token selection \pcite{bulat2021space,ryoo2021tokenlearner,zha2021shifted}, and the inclusion of contextual information \pcite{kim2021relational}. \tcite{liu2022video} introduced shifted non-overlapping attention windows to share information across patches. \tcite{hu2024tcnet} additionally used attention over motion-aligned input volumes. Feature hierarchies and latent resolution reductions have led to more compute- \pcite{fan2021multiscale,li2022mvitv2} and memory-efficient \pcite{wu2022memvit} architectures. Recent models such as MViT \pcite{yan2022multiview}, Hiera \pcite{ryali2023hiera}, UniFormer \pcite{li2022uniformer}, and MooG \pcite{van2024moving}, improved both the performance and capacity of video models. SSL has also shown great promise with pretext tasks based on contrastive learning \pcite{chen2020simple} or token masking \pcite{he2022masked}. \tcite{xing2023svformer} increased the complexity of the contrastive objective with pseudo labels and token mixing from different inputs. Masked autoencoders have also been extended to video data \pcite{feichtenhofer2022masked,wei2022masked}. Subsequent works have explored adaptive token masking \pcite{bandara2023adamae}, double masking on both the encoder and decoder \pcite{wang2023videomae}, token fusion \pcite{kim2024token}, and teacher-student masked autoencoders \pcite{wang2023masked}.

% VLM
\noindent
\textbf{Video-language models}. Recently, language semantics from Large Language Models (LLMs) \pcite{brown2020language,touvron2023llama} have been used as a supervisory signal for vision tasks \pcite{li2023blip,liu2024visual,radford2021learning}. Initial efforts \pcite{zellers2021merlot} matched frame-level encodings to corresponding LLM embeddings of captions. Other approaches have optimized image-based encodings over frames by pooling spatial tokens \pcite{yu2022coca}, including cross-modal skip connections \pcite{xu2023mplug}, cross-attending modalities \pcite{alayrac2022flamingo}, and jointly attending visual and text embeddings \pcite{maaz2023video}. As static features provide only an appearance-based view, works have also used spatiotemporal Vision-Language models (VLMs) \pcite{piergiovanni2024mirasol3b} and extended the training objective \pcite{lu2024improving,zhao2024videoprism} to a two-step SSL pre-training with video-to-text alignment and video masking.